#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
å¤šæ™ºèƒ½ä½“åä½œç³»ç»Ÿ (Multi-Agent Swarm) v3.0.0
âœ¨ æ–°å¢åŠŸèƒ½ï¼š
- å¤šå±‚å¯¹æŠ—è¾©è®º (Adversarial Debate)
- Meta-Critic å…ƒæ‰¹è¯„æœºåˆ¶
- åŠ¨æ€ Agent å·¥å‚ + ä»»åŠ¡åˆ†è§£
- ä¸»åŠ¨çŸ¥è¯†å›¾è°± + è’¸é¦
- è‡ªé€‚åº”åæ€æ·±åº¦
"""

import yaml
import logging
import os
import glob
import importlib.util
import requests
import random
import time
import threading
import base64
import mimetypes
from bs4 import BeautifulSoup
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import List, Dict, Optional, Tuple
from openai import OpenAI
import json
from datetime import datetime
import chromadb
from chromadb.config import Settings
from sentence_transformers import SentenceTransformer
from duckduckgo_search import DDGS

# ====================== æ—¶é—´ç»Ÿè®¡å·¥å…· ======================
from contextlib import contextmanager


class TimeTracker:
    """æ—¶é—´ç»Ÿè®¡å·¥å…·ç±»"""

    def __init__(self):
        self.start_time = None
        self.checkpoints = {}

    def start(self):
        """å¼€å§‹è®¡æ—¶"""
        self.start_time = time.time()
        return self.start_time

    def checkpoint(self, name: str):
        """è®°å½•æ£€æŸ¥ç‚¹"""
        if self.start_time is None:
            self.start()
        elapsed = time.time() - self.start_time
        self.checkpoints[name] = elapsed
        return elapsed

    def get_elapsed(self) -> float:
        """è·å–æ€»è€—æ—¶"""
        if self.start_time is None:
            return 0
        return time.time() - self.start_time

    def format_time(self, seconds: float) -> str:
        """æ ¼å¼åŒ–æ—¶é—´æ˜¾ç¤º"""
        if seconds < 60:
            return f"{seconds:.2f}ç§’"
        elif seconds < 3600:
            minutes = int(seconds // 60)
            secs = seconds % 60
            return f"{minutes}åˆ†{secs:.1f}ç§’"
        else:
            hours = int(seconds // 3600)
            minutes = int((seconds % 3600) // 60)
            secs = seconds % 60
            return f"{hours}å°æ—¶{minutes}åˆ†{secs:.0f}ç§’"

    def summary(self) -> str:
        """ç”Ÿæˆè€—æ—¶æ‘˜è¦"""
        total = self.get_elapsed()
        lines = [f"\n{'=' * 60}"]
        lines.append(f"â±ï¸  æ€»è€—æ—¶: {self.format_time(total)}")
        lines.append(f"{'â”€' * 60}")

        if self.checkpoints:
            lines.append("ğŸ“Š å„é˜¶æ®µè€—æ—¶:")
            for name, elapsed in self.checkpoints.items():
                percentage = (elapsed / total * 100) if total > 0 else 0
                lines.append(f"   {name}: {self.format_time(elapsed)} ({percentage:.1f}%)")

        lines.append(f"{'=' * 60}")
        return "\n".join(lines)


@contextmanager
def timer(description: str):
    """ä¸Šä¸‹æ–‡ç®¡ç†å™¨ï¼šè‡ªåŠ¨è®¡æ—¶å¹¶æ‰“å°"""
    start = time.time()
    print(f"â±ï¸  å¼€å§‹: {description}", flush=True)
    try:
        yield
    finally:
        elapsed = time.time() - start
        print(f"âœ… å®Œæˆ: {description} | è€—æ—¶: {TimeTracker().format_time(elapsed)}", flush=True)


# ====================== çº¿ç¨‹å®‰å…¨çš„å·¥å…·ç¼“å­˜ ======================
tool_cache = {}
cache_count = 0
cache_lock = threading.Lock()


def clean_cache():
    """è‡ªåŠ¨æ¸…ç†å·¥å…·ç¼“å­˜ï¼ˆçº¿ç¨‹å®‰å…¨ï¼‰"""
    global cache_count
    with cache_lock:
        if cache_count > 50:
            tool_cache.clear()
            cache_count = 0
            logging.info("ğŸ§¹ è‡ªåŠ¨æ¸…ç†å·¥å…·ç¼“å­˜")


# ====================== å·¥å…·å‡½æ•° ======================
def web_search(query: str, num_results: int = 5) -> str:
    """DuckDuckGo ç½‘é¡µæœç´¢ï¼ˆå¸¦ç¼“å­˜ + éšæœºå»¶æ—¶ï¼‰"""
    global cache_count
    clean_cache()

    with cache_lock:
        if query in tool_cache:
            return tool_cache[query]

    time.sleep(random.uniform(0.5, 2.0))

    try:
        with DDGS() as ddgs:
            results = [r for r in ddgs.text(query, max_results=num_results)]

        result = "\n".join([
            f"æ ‡é¢˜: {r['title']}\næ‘˜è¦: {r['body']}\né“¾æ¥: {r['href']}"
            for r in results
        ])

        with cache_lock:
            tool_cache[query] = result
            cache_count += 1

        return result
    except Exception as e:
        logging.error(f"æœç´¢å¤±è´¥: {e}")
        return f"æœç´¢å¤±è´¥: {str(e)}"


def browse_page(url: str) -> str:
    """æµè§ˆç½‘é¡µå¹¶æå–æ–‡æœ¬ï¼ˆå¸¦ç¼“å­˜ï¼‰"""
    global cache_count
    clean_cache()

    with cache_lock:
        if url in tool_cache:
            return tool_cache[url]

    try:
        headers = {"User-Agent": "Mozilla/5.0"}
        resp = requests.get(url, headers=headers, timeout=10)
        resp.raise_for_status()

        soup = BeautifulSoup(resp.text, "html.parser")
        for script in soup(["script", "style"]):
            script.decompose()

        text = soup.get_text()
        lines = (line.strip() for line in text.splitlines())
        chunks = (phrase.strip() for line in lines for phrase in line.split("  "))
        result = "\n".join(chunk for chunk in chunks if chunk)

        with cache_lock:
            tool_cache[url] = result
            cache_count += 1

        return result
    except Exception as e:
        logging.error(f"æµè§ˆå¤±è´¥ {url}: {e}")
        return f"æµè§ˆå¤±è´¥: {str(e)}"


def run_python(code: str) -> str:
    """
    æ²™ç®±æ‰§è¡Œ Python ä»£ç ï¼ˆ10ç§’è¶…æ—¶ï¼‰
    æ³¨æ„ï¼šthreading.Timer æ— æ³•çœŸæ­£ç»ˆæ­¢é˜»å¡ä»£ç ï¼Œä»…ä½œè½¯è¶…æ—¶
    """
    result_container = {"output": None, "done": False}

    def target():
        try:
            restricted_globals = {
                "__builtins__": {
                    "print": print,
                    "range": range,
                    "len": len,
                    "str": str,
                    "int": int,
                    "float": float,
                    "list": list,
                    "dict": dict,
                    "sum": sum,
                    "min": min,
                    "max": max,
                }
            }
            local_vars = {}
            exec(code, restricted_globals, local_vars)
            result_container["output"] = str(local_vars.get("result", "æ‰§è¡ŒæˆåŠŸï¼Œæ— è¿”å›ç»“æœ"))
        except Exception as e:
            result_container["output"] = f"æ‰§è¡Œé”™è¯¯: {str(e)}"
        finally:
            result_container["done"] = True

    thread = threading.Thread(target=target, daemon=True)
    thread.start()
    thread.join(timeout=10.0)

    if not result_container["done"]:
        return "â±ï¸ æ‰§è¡Œè¶…æ—¶ï¼ˆ10ç§’ï¼‰"

    return result_container["output"]


# ====================== å‘é‡è®°å¿† ======================
class VectorMemory:
    """
    åŸºäº ChromaDB å’Œ SentenceTransformer çš„å‘é‡è®°å¿†ç³»ç»Ÿ
    âœ… ä¼˜å…ˆä½¿ç”¨ç¼“å­˜æ¨¡å‹ï¼Œé¿å…é‡å¤ä¸‹è½½
    """

    def __init__(
            self,
            persist_directory: str = "./memory_db",
            model_name: str = "sentence-transformers/all-MiniLM-L6-v2",
            cache_dir: str = "./cached_model/"
    ):
        """
        åˆå§‹åŒ–å‘é‡è®°å¿†ç³»ç»Ÿ

        Args:
            persist_directory: ChromaDB æ•°æ®åº“è·¯å¾„
            model_name: SentenceTransformer æ¨¡å‹åç§°
            cache_dir: æ¨¡å‹ç¼“å­˜ç›®å½•ï¼ˆä¼˜å…ˆä»æ­¤å¤„åŠ è½½ï¼‰
        """
        self.persist_directory = persist_directory
        self.model_name = model_name
        self.cache_dir = os.path.abspath(cache_dir)

        # ç¡®ä¿ç¼“å­˜ç›®å½•å­˜åœ¨
        os.makedirs(self.cache_dir, exist_ok=True)

        # åˆå§‹åŒ–åµŒå…¥æ¨¡å‹ï¼ˆä¼˜å…ˆä½¿ç”¨ç¼“å­˜ï¼‰
        self._init_embedding_model()

        # åˆå§‹åŒ– ChromaDB
        self._init_chromadb()

    def _init_embedding_model(self):
        """åˆå§‹åŒ–åµŒå…¥æ¨¡å‹ï¼ˆä¼˜å…ˆä»ç¼“å­˜åŠ è½½ï¼‰"""
        try:
            # æ£€æŸ¥ç¼“å­˜æ˜¯å¦å­˜åœ¨
            cached_model_path = os.path.join(self.cache_dir, self.model_name.replace('/', '_'))

            if os.path.exists(cached_model_path):
                logging.info(f"ğŸ“¦ ä»ç¼“å­˜åŠ è½½å‘é‡æ¨¡å‹: {cached_model_path}")
                print(f"ğŸ“¦ ä»ç¼“å­˜åŠ è½½å‘é‡æ¨¡å‹: {self.model_name}")
                self.embedding_model = SentenceTransformer(cached_model_path)
            else:
                logging.info(f"â¬‡ï¸  ä¸‹è½½å‘é‡æ¨¡å‹: {self.model_name} â†’ {cached_model_path}")
                print(f"â¬‡ï¸  é¦–æ¬¡ä½¿ç”¨ï¼Œæ­£åœ¨ä¸‹è½½å‘é‡æ¨¡å‹: {self.model_name}")
                print(f"   ä¸‹è½½åå°†ç¼“å­˜åˆ°: {cached_model_path}")

                # ä¸‹è½½æ¨¡å‹
                self.embedding_model = SentenceTransformer(self.model_name)

                # ä¿å­˜åˆ°ç¼“å­˜
                self.embedding_model.save(cached_model_path)
                logging.info(f"âœ… æ¨¡å‹å·²ç¼“å­˜åˆ°: {cached_model_path}")
                print(f"âœ… æ¨¡å‹å·²ç¼“å­˜ï¼Œä¸‹æ¬¡å°†ç›´æ¥ä½¿ç”¨")

        except Exception as e:
            logging.error(f"âŒ å‘é‡æ¨¡å‹åˆå§‹åŒ–å¤±è´¥: {e}")
            raise

    def _init_chromadb(self):
        """åˆå§‹åŒ– ChromaDB å®¢æˆ·ç«¯"""
        try:
            os.makedirs(os.path.dirname(self.persist_directory) if os.path.dirname(
                self.persist_directory) else ".", exist_ok=True)

            self.client = chromadb.PersistentClient(
                path=self.persist_directory,
                settings=Settings(anonymized_telemetry=False)
            )

            # è·å–æˆ–åˆ›å»ºé›†åˆ
            self.collection = self.client.get_or_create_collection(
                name="swarm_memory",
                metadata={"description": "Agent memory storage"}
            )

            logging.info(f"âœ… ChromaDB åˆå§‹åŒ–æˆåŠŸ: {self.persist_directory}")

        except Exception as e:
            logging.error(f"âŒ ChromaDB åˆå§‹åŒ–å¤±è´¥: {e}")
            self.collection = None

    def add(self, text: str, metadata: Optional[Dict] = None):
        """æ·»åŠ è®°å¿†åˆ°å‘é‡æ•°æ®åº“"""
        if not self.collection:
            return

        try:
            # ç”ŸæˆåµŒå…¥å‘é‡
            embedding = self.embedding_model.encode(text).tolist()

            # ç”Ÿæˆ ID
            memory_id = f"mem_{datetime.now().strftime('%Y%m%d_%H%M%S_%f')}"

            # æ·»åŠ åˆ°æ•°æ®åº“
            self.collection.add(
                ids=[memory_id],
                embeddings=[embedding],
                documents=[text],
                metadatas=[metadata or {"timestamp": datetime.now().isoformat()}]
            )

            logging.info(f"âœ… è®°å¿†å·²ä¿å­˜: {memory_id}")

        except Exception as e:
            logging.error(f"âŒ ä¿å­˜è®°å¿†å¤±è´¥: {e}")

    def search(self, query: str, n_results: int = 5) -> str:
        """æœç´¢ç›¸å…³è®°å¿†"""
        if not self.collection:
            return ""

        try:
            # ç”ŸæˆæŸ¥è¯¢åµŒå…¥
            query_embedding = self.embedding_model.encode(query).tolist()

            # æœç´¢
            results = self.collection.query(
                query_embeddings=[query_embedding],
                n_results=n_results
            )

            # æ ¼å¼åŒ–ç»“æœ
            if results and results["documents"]:
                return "\n\n---\n\n".join(results["documents"][0])

        except Exception as e:
            logging.error(f"âŒ æœç´¢è®°å¿†å¤±è´¥: {e}")

        return ""


# ====================== âœ¨ çŸ¥è¯†å›¾è°±ç®¡ç†å™¨ ======================
class KnowledgeGraph:
    """
    è½»é‡çº§çŸ¥è¯†å›¾è°± + è‡ªåŠ¨è’¸é¦
    ç”¨äºè¿½è¸ªå…³é”®æ¦‚å¿µã€å…³ç³»å’Œå‘ç°
    """

    def __init__(self, enable_distillation: bool = True):
        self.graph = {}  # {entity: {"type": str, "relations": [(rel, target)], "evidence": [str]}}
        self.enable_distillation = enable_distillation
        self.distilled_knowledge = []

    def add_entity(self, entity: str, entity_type: str = "concept", evidence: str = ""):
        """æ·»åŠ å®ä½“"""
        if entity not in self.graph:
            self.graph[entity] = {
                "type": entity_type,
                "relations": [],
                "evidence": [evidence] if evidence else []
            }
        elif evidence:
            self.graph[entity]["evidence"].append(evidence)

    def add_relation(self, source: str, relation: str, target: str):
        """æ·»åŠ å…³ç³»"""
        if source in self.graph:
            self.graph[source]["relations"].append((relation, target))
        else:
            self.add_entity(source)
            self.graph[source]["relations"].append((relation, target))

    def distill(self, max_items: int = 10) -> str:
        """
        è’¸é¦çŸ¥è¯†å›¾è°±ï¼ˆæå–æ ¸å¿ƒæ¦‚å¿µå’Œå…³ç³»ï¼‰
        """
        if not self.enable_distillation or not self.graph:
            return ""

        # æŒ‰å…³ç³»æ•°é‡æ’åºï¼ˆæœ€é‡è¦çš„æ¦‚å¿µï¼‰
        sorted_entities = sorted(
            self.graph.items(),
            key=lambda x: len(x[1]["relations"]),
            reverse=True
        )[:max_items]

        distilled = ["ğŸ§  æ ¸å¿ƒçŸ¥è¯†è’¸é¦:"]
        for entity, data in sorted_entities:
            relations_str = ", ".join([f"{rel}â†’{tgt}" for rel, tgt in data["relations"][:3]])
            distilled.append(f"â€¢ {entity} ({data['type']}): {relations_str}")

        result = "\n".join(distilled)
        self.distilled_knowledge.append(result)
        return result

    def get_context(self, entity: str, depth: int = 1) -> str:
        """è·å–å®ä½“çš„ä¸Šä¸‹æ–‡"""
        if entity not in self.graph:
            return ""

        context = [f"ğŸ“Œ {entity} ({self.graph[entity]['type']})"]

        # ä¸€çº§å…³ç³»
        for rel, target in self.graph[entity]["relations"]:
            context.append(f"  â””â”€ {rel} â†’ {target}")

            # äºŒçº§å…³ç³»ï¼ˆå¦‚æœ depth > 1ï¼‰
            if depth > 1 and target in self.graph:
                for sub_rel, sub_target in self.graph[target]["relations"][:2]:
                    context.append(f"     â””â”€ {sub_rel} â†’ {sub_target}")

        return "\n".join(context)


# ====================== Skill åŠ¨æ€åŠ è½½å™¨ ======================
def load_skills(skills_dir: str = "skills"):
    """
    é€’å½’åŠ è½½ skills ç›®å½•ä¸‹çš„æ‰€æœ‰ Python å·¥å…·å’Œ Markdown çŸ¥è¯†æ–‡ä»¶
    æ”¯æŒå­ç›®å½•ç»“æ„
    """
    tool_registry = {}
    shared_knowledge = []

    if not os.path.exists(skills_dir):
        logging.warning(f"âš ï¸ Skills ç›®å½•ä¸å­˜åœ¨: {skills_dir}")
        return tool_registry, ""

    # é€’å½’æ‰«ææ‰€æœ‰ .py æ–‡ä»¶
    py_files = glob.glob(os.path.join(skills_dir, "**/*.py"), recursive=True)

    for py_file in py_files:
        try:
            rel_path = os.path.relpath(py_file, skills_dir)

            # åŠ¨æ€å¯¼å…¥æ¨¡å—
            spec = importlib.util.spec_from_file_location(
                os.path.splitext(os.path.basename(py_file))[0],
                py_file
            )
            mod = importlib.util.module_from_spec(spec)
            spec.loader.exec_module(mod)

            # æŸ¥æ‰¾å·¥å…·å‡½æ•°å’Œ schema
            if hasattr(mod, "tool_function") and hasattr(mod, "tool_schema"):
                tool_name = mod.tool_schema["function"]["name"]
                tool_registry[tool_name] = {
                    "func": mod.tool_function,
                    "schema": mod.tool_schema
                }
                logging.info(f"âœ… åŠ è½½ Skill (py): {tool_name} | æ¥è‡ª: {rel_path}")
            else:
                logging.warning(f"âš ï¸ è·³è¿‡æ— æ•ˆ Skill æ–‡ä»¶: {rel_path}")

        except Exception as e:
            logging.error(f"âŒ åŠ è½½ Skill å¤±è´¥: {py_file} | é”™è¯¯: {e}")

    # é€’å½’æ‰«ææ‰€æœ‰ .md æ–‡ä»¶
    md_files = glob.glob(os.path.join(skills_dir, "**/*.md"), recursive=True)

    for md_file in md_files:
        try:
            rel_path = os.path.relpath(md_file, skills_dir)
            with open(md_file, "r", encoding="utf-8") as f:
                content = f.read().strip()
                if content:
                    shared_knowledge.append(f"### ğŸ“š æ¥è‡ª {rel_path} ###\n{content}")
                    logging.info(f"ğŸ“š åŠ è½½çŸ¥è¯†æ–‡ä»¶ (md): {rel_path}")
        except Exception as e:
            logging.error(f"âŒ è¯»å–çŸ¥è¯†æ–‡ä»¶å¤±è´¥: {md_file} | é”™è¯¯: {e}")

    logging.info(f"ğŸ“Š Skills åŠ è½½å®Œæˆ: {len(tool_registry)} ä¸ªå·¥å…·, {len(shared_knowledge)} ä¸ªçŸ¥è¯†æ–‡ä»¶")

    shared_knowledge_str = "\n\n".join(shared_knowledge)

    return tool_registry, shared_knowledge_str


# ====================== Agent ç±» ======================
class Agent:
    """å•ä¸ªæ™ºèƒ½ä½“ä»£ç†"""

    def __init__(
            self,
            config: Dict,
            default_model: str,
            default_max_tokens: int,
            tool_registry: Dict,
            shared_knowledge: str = "",
            vector_memory: Optional[VectorMemory] = None,
            knowledge_graph: Optional[KnowledgeGraph] = None
    ):
        self.name = config["name"]
        self.role = config["role"]
        self.shared_knowledge = shared_knowledge
        self.vector_memory = vector_memory
        self.knowledge_graph = knowledge_graph

        # OpenAI å®¢æˆ·ç«¯é…ç½®
        self.client = OpenAI(
            api_key=config.get("api_key"),
            base_url=config.get("base_url")
        )
        self.model = config.get("model", default_model)
        self.temperature = config.get("temperature", 0.7)
        self.stream = config.get("stream", False)
        self.max_tokens = config.get("max_tokens", default_max_tokens)

        # å·¥å…·é…ç½®
        enabled = config.get("enabled_tools", [])
        self.tools = [
            tool_registry[name]["schema"]
            for name in enabled
            if name in tool_registry
        ]
        self.tool_map = {
            name: tool_registry[name]["func"]
            for name in enabled
            if name in tool_registry
        }

        if self.tools:
            logging.debug(f"  {self.name} å·²å¯ç”¨å·¥å…·: {list(self.tool_map.keys())}")

    def _execute_tool(self, tool_call) -> Dict:
        """æ‰§è¡Œå·¥å…·è°ƒç”¨"""
        func_name = tool_call.function.name

        try:
            args = json.loads(tool_call.function.arguments)
            logging.info(f"ğŸ”§ {self.name} è°ƒç”¨å·¥å…·: {func_name}({args})")

            result = self.tool_map[func_name](**args)

            return {
                "role": "tool",
                "tool_call_id": tool_call.id,
                "name": func_name,
                "content": str(result)
            }
        except Exception as e:
            logging.error(f"å·¥å…·æ‰§è¡Œå¤±è´¥ {func_name}: {e}")
            return {
                "role": "tool",
                "tool_call_id": tool_call.id,
                "name": func_name,
                "content": f"Tool error: {str(e)}"
            }

    def generate_response(
            self,
            history: List[Dict],
            round_num: int,
            system_extra: str = "",
            force_non_stream: bool = False,
            critique_previous: bool = False,
            stream_callback=None,
            log_callback=None
    ) -> str:
        """
        ç”Ÿæˆ Agent å“åº”

        Args:
            history: å¯¹è¯å†å²
            round_num: å½“å‰è½®æ¬¡
            system_extra: é¢å¤–ç³»ç»Ÿæç¤ºè¯
            force_non_stream: å¼ºåˆ¶å…³é—­æµå¼è¾“å‡º
            critique_previous: æ˜¯å¦å¯ç”¨æ‰¹åˆ¤æ¨¡å¼
            stream_callback: æµå¼å›è°ƒå‡½æ•° callback(agent_name, chunk)
            log_callback: æ—¥å¿—å›è°ƒå‡½æ•° callback(message)

        Returns:
            str: Agent çš„å®Œæ•´å“åº”
        """
        start_time = time.time()

        # âœ… åˆ¤æ–­æ˜¯å¦ä½¿ç”¨æµå¼è¾“å‡ºï¼ˆå¦‚æœæœ‰ stream_callbackï¼Œå¼ºåˆ¶å¯ç”¨ï¼‰
        use_stream = (
                (self.stream or stream_callback is not None) and
                not force_non_stream and
                not self.tools  # æœ‰å·¥å…·æ—¶æš‚æ—¶ä¸ç”¨æµå¼
        )

        # âœ¨ æ‰¹åˆ¤æ¨¡å¼å¢å¼º
        if critique_previous and len(history) > 3:
            critique_prompt = (
                "ğŸ” åœ¨ç»™å‡ºä½ çš„è´¡çŒ®å‰ï¼Œè¯·å…ˆç”¨ [CRITIQUE] æ ‡è®°æŒ‡å‡ºä¸Šä¸€è½®è®¨è®ºä¸­ï¼š\n"
                "1. è‡³å°‘ 1 ä¸ªæ½œåœ¨é€»è¾‘æ¼æ´æˆ–çŸ›ç›¾ç‚¹\n"
                "2. è‡³å°‘ 1 ä¸ªå¯æ”¹è¿›æˆ–è¡¥å……çš„åœ°æ–¹\n"
                "ç„¶åå†ç»™å‡ºä½ çš„å»ºè®¾æ€§è´¡çŒ®ã€‚"
            )
            system_extra = critique_prompt + "\n\n" + system_extra

        # æ„å»ºç³»ç»Ÿæç¤ºè¯
        system_prompt = (
            f"{self.role}\n"
            f"{self.shared_knowledge}\n"
            f"{system_extra}\n"
            "ä½ æ˜¯å¤šæ™ºèƒ½ä½“åä½œå›¢é˜Ÿçš„ä¸€å‘˜ï¼Œè¯·æä¾›æœ‰ä»·å€¼ã€å‡†ç¡®ã€æœ‰æ·±åº¦çš„è´¡çŒ®ã€‚"
        )

        messages = [{"role": "system", "content": system_prompt}]

        # å¤„ç†å†å²æ¶ˆæ¯
        for h in history:
            if h["speaker"] == "User":
                messages.append({"role": "user", "content": h["content"]})
            elif h["speaker"] == "System":
                messages.append({"role": "system", "content": h["content"]})
            else:
                messages.append({
                    "role": "assistant",
                    "content": f"[{h['speaker']}] {h.get('content', '')}"
                })

        try:
            # âœ… æ·»åŠ å¼€å§‹æ—¥å¿—
            if log_callback:
                log_callback(f"[{self.name}] å¼€å§‹ç”Ÿæˆå“åº” (è½®æ¬¡ {round_num})")

            if use_stream:
                print(f"\nğŸ’¬ ã€{self.name}ã€‘æ­£åœ¨æ€è€ƒ... ", end="", flush=True)
                if log_callback:
                    log_callback(f"[{self.name}] æ­£åœ¨æ€è€ƒ...")

            # è°ƒç”¨ API
            response = self.client.chat.completions.create(
                model=self.model,
                messages=messages,
                temperature=self.temperature,
                max_tokens=self.max_tokens,
                tools=self.tools if self.tools else None,
                tool_choice="auto" if self.tools else None,
                stream=use_stream,
            )

            full_response = ""

            # ===== æµå¼è¾“å‡º =====
            if use_stream:
                for chunk in response:
                    if chunk.choices[0].delta.content:
                        delta = chunk.choices[0].delta.content
                        print(delta, end="", flush=True)
                        full_response += delta

                        # âœ… æµå¼å›è°ƒ
                        if stream_callback:
                            stream_callback(self.name, delta)
                print()

            # ===== éæµå¼è¾“å‡º =====
            else:
                full_response = response.choices[0].message.content or ""

                # âœ… å³ä½¿éæµå¼ï¼Œä¹Ÿè°ƒç”¨å›è°ƒï¼ˆæ¨¡æ‹Ÿæµå¼æ•ˆæœï¼‰
                if stream_callback and full_response:
                    chunk_size = 20
                    for i in range(0, len(full_response), chunk_size):
                        chunk = full_response[i:i + chunk_size]
                        stream_callback(self.name, chunk)
                        time.sleep(0.02)  # å¯é€‰ï¼šæ¨¡æ‹Ÿæ‰“å­—å»¶è¿Ÿ

            # ===== ğŸ”§ å·¥å…·è°ƒç”¨å¤„ç†ï¼ˆæ”¯æŒå¤šè½®å¾ªç¯ï¼‰=====
            if (not use_stream and
                    hasattr(response.choices[0].message, 'tool_calls') and
                    response.choices[0].message.tool_calls):

                max_tool_iterations = 5
                iteration = 0

                while (hasattr(response.choices[0].message, 'tool_calls') and
                       response.choices[0].message.tool_calls and
                       iteration < max_tool_iterations):

                    iteration += 1
                    print(f"\nğŸ”§ [{self.name}] å·¥å…·è°ƒç”¨ (ç¬¬ {iteration} è½®)")

                    if log_callback:
                        log_callback(f"[{self.name}] å·¥å…·è°ƒç”¨ (ç¬¬ {iteration} è½®)")

                    # æ·»åŠ  assistant æ¶ˆæ¯ï¼ˆåŒ…å« tool_callsï¼‰
                    messages.append(response.choices[0].message.model_dump())

                    # æ‰§è¡Œæ‰€æœ‰å·¥å…·è°ƒç”¨
                    for tool_call in response.choices[0].message.tool_calls:
                        tool_result = self._execute_tool(tool_call)
                        messages.append(tool_result)

                        # æ˜¾ç¤ºå·¥å…·è°ƒç”¨ç»“æœï¼ˆæˆªæ–­é¢„è§ˆï¼‰
                        result_preview = tool_result.get("content", "")[:150]
                        if len(tool_result.get("content", "")) > 150:
                            result_preview += "..."
                        print(f"   âœ… {tool_result['name']}: {result_preview}")

                        if log_callback:
                            log_callback(f"[{self.name}] å·¥å…·: {tool_result['name']}")

                    # é‡æ–°è°ƒç”¨ APIï¼ˆå¸¦å·¥å…·ç»“æœï¼‰
                    response = self.client.chat.completions.create(
                        model=self.model,
                        messages=messages,
                        temperature=self.temperature,
                        max_tokens=self.max_tokens,
                        tools=self.tools if self.tools else None,
                        tool_choice="auto" if self.tools else None,
                        stream=False  # å·¥å…·è°ƒç”¨åæš‚æ—¶ç”¨éæµå¼
                    )

                    # âœ… å…³é”®ä¿®å¤ï¼šå¦‚æœä¸å†è°ƒç”¨å·¥å…·ï¼Œæå–æœ€ç»ˆç­”æ¡ˆå¹¶å‘é€ç»™å‰ç«¯
                    if not (hasattr(response.choices[0].message, 'tool_calls') and
                            response.choices[0].message.tool_calls):
                        full_response = response.choices[0].message.content or ""

                        # âœ… æ¨¡æ‹Ÿæµå¼å‘é€ï¼ˆåˆ†å—å‘é€ç»™å‰ç«¯ï¼‰
                        if stream_callback and full_response:
                            chunk_size = 20  # æ¯å— 20 ä¸ªå­—ç¬¦ï¼ˆå¯è°ƒæ•´ï¼‰
                            for i in range(0, len(full_response), chunk_size):
                                chunk = full_response[i:i + chunk_size]
                                stream_callback(self.name, chunk)
                                time.sleep(0.02)  # æ¨¡æ‹Ÿæ‰“å­—æ•ˆæœ

                        print(f"   ğŸ’¬ [{self.name}] å·¥å…·è°ƒç”¨å®Œæˆï¼Œç”Ÿæˆæœ€ç»ˆç­”æ¡ˆ")
                        if log_callback:
                            log_callback(f"[{self.name}] å·¥å…·è°ƒç”¨å®Œæˆ")
                        break

                # âœ… å·¥å…·è°ƒç”¨è¶…é™å¤„ç†
                if iteration >= max_tool_iterations:
                    print(f"   âš ï¸ [{self.name}] å·¥å…·è°ƒç”¨è¾¾åˆ°ä¸Šé™ ({max_tool_iterations} è½®)")
                    full_response = response.choices[0].message.content or "[å·¥å…·è°ƒç”¨è¶…é™ï¼Œè¯·ç®€åŒ–ä»»åŠ¡]"

                    # âœ… è¶…é™æ—¶ä¹Ÿå‘é€ç»™å‰ç«¯
                    if stream_callback and full_response:
                        chunk_size = 20
                        for i in range(0, len(full_response), chunk_size):
                            chunk = full_response[i:i + chunk_size]
                            stream_callback(self.name, chunk)
                            time.sleep(0.02)

                    if log_callback:
                        log_callback(f"[{self.name}] å·¥å…·è°ƒç”¨è¶…é™")

            # ===== è®¡ç®—å¹¶æ˜¾ç¤ºè€—æ—¶ =====
            elapsed = time.time() - start_time
            elapsed_str = f"{elapsed:.2f}ç§’" if elapsed < 60 else f"{int(elapsed // 60)}åˆ†{elapsed % 60:.1f}ç§’"

            if not use_stream:
                print(f"â±ï¸  ã€{self.name}ã€‘å“åº”å®Œæˆ | è€—æ—¶: {elapsed_str}")

            if log_callback:
                log_callback(f"[{self.name}] å“åº”å®Œæˆ (è€—æ—¶ {elapsed_str})")

            logging.info(f"â±ï¸  {self.name} å“åº”è€—æ—¶: {elapsed_str}")

            return full_response.strip()

        except Exception as e:
            elapsed = time.time() - start_time
            err = f"[Error in {self.name}]: {str(e)}"
            logging.error(f"{err} | è€—æ—¶: {elapsed:.2f}ç§’")
            print(f"âŒ ã€{self.name}ã€‘æ‰§è¡Œå¤±è´¥ | è€—æ—¶: {elapsed:.2f}ç§’")

            if log_callback:
                log_callback(f"[{self.name}] âŒ æ‰§è¡Œå¤±è´¥: {str(e)[:50]}")

            return err


# ====================== ä¸»ç±» MultiAgentSwarm ======================
class MultiAgentSwarm:
    """å¤šæ™ºèƒ½ä½“ç¾¤æ™ºæ…§æ¡†æ¶ v3.0.0"""

    def __init__(self, config_path: str = "swarm_config.yaml"):
        if not os.path.exists(config_path):
            raise FileNotFoundError(f"âŒ é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_path}")

        with open(config_path, "r", encoding="utf-8") as f:
            cfg = yaml.safe_load(f)

        # OpenAI é…ç½®
        oai = cfg.get("openai", {})
        self.default_model = oai.get("default_model", "gpt-4o-mini")
        self.default_max_tokens = oai.get("default_max_tokens", 4096)

        # Swarm é…ç½®
        swarm = cfg.get("swarm", {})
        self.mode = swarm.get("mode", "fixed")
        self.max_rounds = swarm.get("max_rounds", 3 if self.mode == "fixed" else 10)
        self.max_concurrent_agents = swarm.get("max_concurrent_agents", 2)
        self.reflection_planning = swarm.get("reflection_planning", True)
        self.enable_web_search = swarm.get("enable_web_search", False)
        self.max_images = swarm.get("max_images", 2)

        self.log_file = swarm.get("log_file", "swarm.log")
        self.skills_dir = swarm.get("skills_dir", "skills")
        self.memory_file = swarm.get("memory_file", "memory.json")
        self.max_memory_items = swarm.get("max_memory_items", 50)

        # âœ¨ æ–°å¢å¢å¼ºé…ç½®
        advanced = cfg.get("advanced_features", {})
        self.enable_adversarial_debate = advanced.get("adversarial_debate", {}).get("enabled", True)
        self.enable_meta_critic = advanced.get("meta_critic", {}).get("enabled", True)
        self.enable_task_decomposition = advanced.get("task_decomposition", {}).get("enabled", True)
        self.enable_knowledge_graph = advanced.get("knowledge_graph", {}).get("enabled", True)
        self.enable_adaptive_depth = advanced.get("adaptive_reflection", {}).get("enabled", True)

        self.max_reflection_rounds = advanced.get("adaptive_reflection", {}).get("max_rounds", 3)
        self.reflection_quality_threshold = advanced.get("adaptive_reflection", {}).get("quality_threshold", 85)
        self.stop_quality_threshold = advanced.get("adaptive_reflection", {}).get("stop_threshold", 80)
        self.quality_convergence_delta = advanced.get("adaptive_reflection", {}).get("convergence_delta", 3)

        # âœ¨âœ¨âœ¨ æ™ºèƒ½è·¯ç”±é…ç½®ï¼ˆæ–°å¢ï¼‰âœ¨âœ¨âœ¨
        routing_cfg = cfg.get("intelligent_routing", {})
        self.intelligent_routing_enabled = routing_cfg.get("enabled", True)
        self.force_complexity = routing_cfg.get("force_complexity", None)  # "simple"/"medium"/"complex"/None

        # å‘é‡è®°å¿†é…ç½®
        vector_cfg = swarm.get("vector_memory", {})
        self.vector_memory_enabled = vector_cfg.get("enabled", False)
        self.vector_persist_directory = vector_cfg.get("persist_directory", "./memory_db")
        self.vector_model_cache_dir = vector_cfg.get("model_cache_dir", "./cached_model/")
        self.vector_embedding_model = vector_cfg.get("embedding_model",
                                                     "sentence-transformers/distiluse-base-multilingual-cased-v2")

        # æ—¥å¿—é…ç½®
        logging.basicConfig(
            filename=self.log_file,
            level=logging.INFO,
            format="%(asctime)s | %(levelname)s | %(message)s",
            encoding="utf-8",
            force=True
        )
        logging.getLogger().addHandler(logging.StreamHandler())

        # æ‰“å°å¯åŠ¨ä¿¡æ¯
        self._print_startup_banner()

        # åŠ è½½ Skills
        self.tool_registry, self.shared_knowledge = load_skills(self.skills_dir)

        # ====================== ã€æ–°å¢ã€‘æ”¯æŒ YAML ä¸­çš„ shared_knowledge ======================
        yaml_shared = cfg.get("shared_knowledge", "") or ""
        if yaml_shared.strip():
            self.shared_knowledge = (yaml_shared.strip() + "\n\n" + self.shared_knowledge).strip()
            logging.info(f"âœ… å·²åˆå¹¶ YAML shared_knowledgeï¼ˆ{len(yaml_shared)} å­—ç¬¦ï¼‰")
            print(f"âœ… å·²åŠ è½½ YAML å…¨å±€çŸ¥è¯†ï¼ˆ{len(yaml_shared)} å­—ç¬¦ï¼‰")

        # æ·»åŠ å†…ç½®ç½‘ç»œæœç´¢å·¥å…·
        if self.enable_web_search:
            self.tool_registry["web_search"] = {
                "func": web_search,
                "schema": {
                    "type": "function",
                    "function": {
                        "name": "web_search",
                        "description": "å®æ—¶ç½‘é¡µæœç´¢æœ€æ–°ä¿¡æ¯ï¼ˆDuckDuckGoï¼‰",
                        "parameters": {
                            "type": "object",
                            "properties": {
                                "query": {"type": "string", "description": "æœç´¢å…³é”®è¯"},
                                "num_results": {"type": "integer", "description": "è¿”å›ç»“æœæ•°é‡", "default": 5}
                            },
                            "required": ["query"]
                        }
                    }
                }
            }
            logging.info("âœ… å·²å¯ç”¨ç½‘ç»œæœç´¢å·¥å…·")

        # åˆå§‹åŒ–æŒä¹…åŒ–è®°å¿†
        self.memory = self._load_memory()

        # åˆå§‹åŒ–å‘é‡è®°å¿†
        self.vector_memory = None
        if self.vector_memory_enabled:
            try:
                self.vector_memory = VectorMemory(
                    persist_directory=self.vector_persist_directory,
                    model_name=self.vector_embedding_model,
                    cache_dir=self.vector_model_cache_dir
                )
                logging.info("âœ… å‘é‡è®°å¿†ç³»ç»Ÿåˆå§‹åŒ–æˆåŠŸ")
            except Exception as e:
                logging.warning(f"âš ï¸ å‘é‡è®°å¿†åˆå§‹åŒ–å¤±è´¥: {e}")
                self.vector_memory_enabled = False

        # âœ¨ åˆå§‹åŒ–çŸ¥è¯†å›¾è°±
        self.knowledge_graph = None
        if self.enable_knowledge_graph:
            self.knowledge_graph = KnowledgeGraph(enable_distillation=True)
            logging.info("âœ… çŸ¥è¯†å›¾è°±ç³»ç»Ÿåˆå§‹åŒ–æˆåŠŸ")

        # åˆå§‹åŒ– Agents
        self.agents = []
        for a_cfg in cfg.get("agents", [])[:swarm.get("num_agents", 4)]:
            agent = Agent(
                a_cfg,
                self.default_model,
                self.default_max_tokens,
                self.tool_registry,
                self.shared_knowledge,
                self.vector_memory,
                self.knowledge_graph
            )
            self.agents.append(agent)
            logging.info(f"âœ… Agent åŠ è½½: {agent.name} | Model: {agent.model}")

        if not self.agents:
            raise ValueError("âŒ è‡³å°‘éœ€è¦é…ç½®ä¸€ä¸ª Agent")

        self.leader = self.agents[0]
        logging.info(f"ğŸ‘‘ Leader: {self.leader.name}")

    def _print_startup_banner(self):
        """æ‰“å°å¯åŠ¨æ¨ªå¹…"""
        banner = f"""
    {'=' * 80}
    ğŸš€ MultiAgentSwarm v3.1.0 åˆå§‹åŒ–ï¼ˆæ™ºèƒ½è·¯ç”±ç‰ˆï¼‰
    {'=' * 80}
    ğŸ“Š åŸºç¡€é…ç½®:
       Mode: {self.mode} | Max Rounds: {self.max_rounds}
       Max Concurrent: {self.max_concurrent_agents}
       Reflection: {self.reflection_planning} | Web Search: {self.enable_web_search}
       Vector Memory: {self.vector_memory_enabled}

    âœ¨ å¢å¼ºåŠŸèƒ½:
       ğŸ¥Š å¯¹æŠ—è¾©è®º (Adversarial Debate): {'âœ… å¯ç”¨' if self.enable_adversarial_debate else 'âŒ ç¦ç”¨'}
       ğŸ¯ å…ƒæ‰¹è¯„ (Meta-Critic): {'âœ… å¯ç”¨' if self.enable_meta_critic else 'âŒ ç¦ç”¨'}
       ğŸ­ ä»»åŠ¡åˆ†è§£ (Task Decomposition): {'âœ… å¯ç”¨' if self.enable_task_decomposition else 'âŒ ç¦ç”¨'}
       ğŸ§  çŸ¥è¯†å›¾è°± (Knowledge Graph): {'âœ… å¯ç”¨' if self.enable_knowledge_graph else 'âŒ ç¦ç”¨'}
       ğŸ“ˆ è‡ªé€‚åº”åæ€ (Adaptive Depth): {'âœ… å¯ç”¨' if self.enable_adaptive_depth else 'âŒ ç¦ç”¨'}
       ğŸ§­ æ™ºèƒ½è·¯ç”± (Intelligent Routing): {'âœ… å¯ç”¨' if self.intelligent_routing_enabled else 'âŒ ç¦ç”¨'}
          â””â”€ å¼ºåˆ¶æ¨¡å¼: {self.force_complexity or 'è‡ªåŠ¨åˆ¤æ–­'}
    {'=' * 80}
    """
        print(banner)
        logging.info(banner)

    def _load_memory(self) -> Dict:
        """åŠ è½½æŒä¹…åŒ–è®°å¿†"""
        if os.path.exists(self.memory_file):
            try:
                with open(self.memory_file, "r", encoding="utf-8") as f:
                    memory = json.load(f)
                logging.info(f"ğŸ“– åŠ è½½è®°å¿†æ–‡ä»¶: {self.memory_file} ({len(memory)} keys)")
                return memory
            except Exception as e:
                logging.error(f"åŠ è½½è®°å¿†å¤±è´¥: {e}")
        return {}

    def _save_memory(self, key: str, summary: str):
        """ä¿å­˜è®°å¿†åˆ°æŒä¹…åŒ–æ–‡ä»¶"""
        if key not in self.memory:
            self.memory[key] = []

        self.memory[key].append({
            "timestamp": datetime.now().isoformat(),
            "summary": summary[:3000]
        })

        if len(self.memory[key]) > self.max_memory_items:
            self.memory[key] = self.memory[key][-self.max_memory_items:]

        try:
            with open(self.memory_file, "w", encoding="utf-8") as f:
                json.dump(self.memory, f, ensure_ascii=False, indent=2)
            logging.info(f"ğŸ’¾ ä¿å­˜è®°å¿†: {key}")
        except Exception as e:
            logging.error(f"ä¿å­˜è®°å¿†å¤±è´¥: {e}")

    def _decompose_task(self, task: str) -> str:
        """
        âœ¨ ä»»åŠ¡åˆ†è§£å™¨
        å°†å¤æ‚ä»»åŠ¡åˆ†è§£ä¸ºå­ä»»åŠ¡å¹¶åˆ†é…ç»™æœ€é€‚åˆçš„ Agent
        """
        if not self.enable_task_decomposition or len(self.agents) <= 1:
            return ""

        logging.info("ğŸ­ å¯åŠ¨ä»»åŠ¡åˆ†è§£...")

        decompose_prompt = (
            f"è¯·å°†ä»¥ä¸‹ä»»åŠ¡åˆ†è§£ä¸º {min(len(self.agents) + 2, 7)} ä¸ªå¯å¹¶è¡Œæˆ–é¡ºåºæ‰§è¡Œçš„å­ä»»åŠ¡ã€‚\n"
            f"ä»»åŠ¡: {task}\n\n"
            f"å¯ç”¨ Agent åŠå…¶ä¸“é•¿ï¼š\n"
            + "\n".join([f"- {a.name}: {a.role[:100]}" for a in self.agents])
            + "\n\næ ¼å¼è¦æ±‚ï¼š\n"
              "```json\n"
              '{"subtasks": [{"id": 1, "description": "å­ä»»åŠ¡æè¿°", "assigned_agent": "Agentåç§°", "priority": "high/medium/low"}]}\n'
              "```"
        )

        try:
            decomposition = self.leader.generate_response(
                [{"speaker": "System", "content": decompose_prompt}],
                0,
                force_non_stream=True
            )

            logging.info(f"ğŸ“‹ ä»»åŠ¡åˆ†è§£ç»“æœ:\n{decomposition[:500]}...")
            return f"ğŸ“‹ ä»»åŠ¡åˆ†è§£:\n{decomposition}"

        except Exception as e:
            logging.error(f"ä»»åŠ¡åˆ†è§£å¤±è´¥: {e}")
            return ""

    def _adversarial_debate(self, history: List[Dict], round_num: int) -> Tuple[int, str]:
        """
        âœ¨ å¯¹æŠ—å¼è¾©è®ºæœºåˆ¶
        ä¸‰è§’è‰²å¹¶è¡Œè¾©è®ºï¼šProï¼ˆå»ºè®¾è€…ï¼‰ã€Conï¼ˆæ‰¹åˆ¤è€…ï¼‰ã€Judgeï¼ˆè£åˆ¤ï¼‰
        è¿”å›ï¼š(è´¨é‡åˆ†æ•° 0-100, å†³ç­– "continue"/"stop")
        """
        if not self.enable_adversarial_debate:
            return 50, "continue"

        logging.info(f"\n{'â”€' * 80}")
        logging.info(f"ğŸ¥Š å¯åŠ¨å¯¹æŠ—å¼è¾©è®º (ç¬¬ {round_num} è½®)")
        logging.info(f"{'â”€' * 80}")

        # è§’è‰²åˆ†é…ï¼ˆåˆ©ç”¨ç°æœ‰ Agentï¼‰
        debate_agents = {
            "Pro": self.agents[1] if len(self.agents) > 1 else self.leader,  # Harper - åˆ›æ„å»ºè®¾è€…
            "Con": self.agents[2] if len(self.agents) > 2 else self.leader,  # Benjamin - ä¸¥æ ¼æ‰¹åˆ¤è€…
            "Judge": self.leader  # Grok - ç»¼åˆè£åˆ¤
        }

        # è¾©è®ºæç¤ºè¯
        reflection_prompts = {
            "Pro": (
                "ğŸŸ¢ ä½ æ˜¯ä¹è§‚å»ºè®¾è€…ï¼Œè¯·ï¼š\n"
                "1. æŒ‡å‡ºæœ¬è½®è®¨è®ºçš„ 3 ä¸ªæœ€å¤§äº®ç‚¹\n"
                "2. æä¾› 2-3 ä¸ªå»ºè®¾æ€§æ”¹è¿›å»ºè®®\n"
                "3. ç»™å‡ºè´¨é‡è¯„åˆ†ï¼ˆ0-100ï¼‰"
            ),
            "Con": (
                "ğŸ”´ ä½ æ˜¯ä¸¥æ ¼æ‰¹åˆ¤è€…ï¼Œè¯·ï¼š\n"
                "1. æ‰¾å‡ºè‡³å°‘ 3 ä¸ªé€»è¾‘æ¼æ´ã€äº‹å®é£é™©æˆ–é—æ¼ç‚¹\n"
                "2. æŒ‡å‡ºå¯èƒ½å¯¼è‡´é”™è¯¯ç»“è®ºçš„å‡è®¾\n"
                "3. ç»™å‡ºé£é™©è¯„åˆ†ï¼ˆ0-100ï¼Œè¶Šé«˜é£é™©è¶Šå¤§ï¼‰"
            ),
            "Judge": (
                "âš–ï¸ ä½ æ˜¯æœ€ç»ˆè£åˆ¤ï¼Œè¯·ï¼š\n"
                "1. ç»¼åˆ Pro å’Œ Con çš„è§‚ç‚¹\n"
                "2. ç»™å‡ºç»¼åˆè´¨é‡è¯„åˆ†ï¼ˆ0-100ï¼‰\n"
                "3. å†³ç­–æ˜¯å¦ç»§ç»­è®¨è®ºï¼ˆcontinue/stopï¼‰\n"
                "æ ¼å¼ï¼š```json\n{\"quality_score\": 0-100, \"decision\": \"continue/stop\", \"reason\": \"åŸå› \"}\n```"
            )
        }

        # å¹¶è¡Œæ‰§è¡Œè¾©è®º
        reflections = {}
        with ThreadPoolExecutor(max_workers=3) as executor:
            future_to_role = {
                executor.submit(
                    agent.generate_response,
                    history.copy(),
                    round_num,
                    system_extra=reflection_prompts[role],
                    critique_previous=True,  # å¼ºåˆ¶æ‰¹åˆ¤æ¨¡å¼
                    force_non_stream=True
                ): role
                for role, agent in debate_agents.items()
            }

            for future in as_completed(future_to_role):
                role = future_to_role[future]
                try:
                    reflections[role] = future.result()
                    logging.info(f"âœ… {role} å®Œæˆè¾©è®º")
                except Exception as e:
                    logging.error(f"âŒ {role} è¾©è®ºå¤±è´¥: {e}")
                    reflections[role] = f"[æ‰§è¡Œå¤±è´¥: {str(e)}]"

        # Meta-Critic ç»¼åˆè¯„ä¼°
        if self.enable_meta_critic:
            synthesis_prompt = (
                f"ğŸ¯ Meta-Critic ç»¼åˆè¯„ä¼°\n\n"
                f"Pro è§‚ç‚¹:\n{reflections.get('Pro', 'N/A')[:800]}\n\n"
                f"Con è§‚ç‚¹:\n{reflections.get('Con', 'N/A')[:800]}\n\n"
                f"Judge è§‚ç‚¹:\n{reflections.get('Judge', 'N/A')[:800]}\n\n"
                f"è¯·ç»¼åˆä¸‰æ–¹è¾©è®ºï¼Œç»™å‡ºæœ€ç»ˆå†³ç­–ï¼ˆJSON æ ¼å¼ï¼‰ï¼š\n"
                f"```json\n"
                f'{{"quality_score": 0-100, "decision": "continue/stop", "reason": "ç»¼åˆåŸå› ", "key_issues": ["é—®é¢˜1", "é—®é¢˜2"]}}\n'
                f"```"
            )

            final_eval = self.leader.generate_response(
                history + [{"speaker": "System", "content": synthesis_prompt}],
                round_num,
                force_non_stream=True
            )
        else:
            final_eval = reflections.get("Judge", "{}")

        # è§£æå†³ç­–
        try:
            eval_json = json.loads(
                final_eval.strip()
                .replace("```json", "")
                .replace("```", "")
                .strip()
            )

            quality_score = eval_json.get("quality_score", 50)
            decision = eval_json.get("decision", "continue").lower()

            logging.info(f"ğŸ“Š è¾©è®ºç»“æœ: è´¨é‡åˆ†æ•° {quality_score}/100 | å†³ç­–: {decision}")
            logging.info(f"ğŸ’¡ åŸå› : {eval_json.get('reason', 'N/A')[:200]}")

            return quality_score, decision

        except (json.JSONDecodeError, Exception) as e:
            logging.error(f"âŒ è§£æè¾©è®ºç»“æœå¤±è´¥: {e}")
            return 50, "continue"

    def solve(
            self,
            task: str,
            use_memory: bool = False,
            memory_key: str = "default",
            image_paths: Optional[List[str]] = None,
            force_complexity: Optional[str] = None,
            stream_callback=None,  # âœ… æ–°å¢ï¼šæµå¼è¾“å‡ºå›è°ƒ
            log_callback=None  # âœ… æ–°å¢ï¼šæ—¥å¿—å›è°ƒ
    ) -> str:
        """
        è§£å†³ä»»åŠ¡çš„ä¸»å…¥å£ï¼ˆæ™ºèƒ½è·¯ç”±ç‰ˆ v3.1.0ï¼‰

        Args:
            task: ä»»åŠ¡æè¿°
            use_memory: æ˜¯å¦ä½¿ç”¨æŒä¹…åŒ–è®°å¿†
            memory_key: è®°å¿†é”®å
            image_paths: å›¾ç‰‡è·¯å¾„åˆ—è¡¨ï¼ˆæœ€å¤š max_images å¼ ï¼‰
            force_complexity: å¼ºåˆ¶æŒ‡å®šå¤æ‚åº¦ "simple"/"medium"/"complex"ï¼ˆä¼˜å…ˆçº§æœ€é«˜ï¼Œç”¨äºè°ƒè¯•ï¼‰
            stream_callback: æµå¼è¾“å‡ºå›è°ƒ func(agent_name, content)
            log_callback: æ—¥å¿—å›è°ƒ func(message)

        Returns:
            æœ€ç»ˆç­”æ¡ˆå­—ç¬¦ä¸²
        """
        tracker = TimeTracker()
        tracker.start()

        logging.info(f"\n{'=' * 80}")
        logging.info(f"ğŸ“‹ æ–°ä»»åŠ¡: {task}")
        logging.info(f"{'=' * 80}")

        print(f"\n{'=' * 80}")
        print(f"ğŸš€ ä»»åŠ¡å¼€å§‹: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"{'=' * 80}\n")

        # âœ… å‘é€å¼€å§‹æ—¥å¿—
        if log_callback:
            log_callback("ğŸš€ ä»»åŠ¡å¼€å§‹")

        # âœ¨âœ¨âœ¨ æ ¸å¿ƒï¼šæ™ºèƒ½ä»»åŠ¡åˆ†ç±»ï¼ˆå¸¦é™çº§ä¿æŠ¤ï¼‰âœ¨âœ¨âœ¨
        try:
            if self.intelligent_routing_enabled:
                # ä¼˜å…ˆçº§ï¼šæ–¹æ³•å‚æ•° > é…ç½®æ–‡ä»¶ > è‡ªåŠ¨åˆ¤æ–­
                complexity = force_complexity or self.force_complexity or self._classify_task_complexity(task)

                # éªŒè¯å¤æ‚åº¦å€¼
                if complexity not in ["simple", "medium", "complex"]:
                    logging.warning(f"âš ï¸ æ— æ•ˆçš„å¤æ‚åº¦å€¼: {complexity}ï¼Œå›é€€åˆ°è‡ªåŠ¨åˆ¤æ–­")
                    complexity = self._classify_task_complexity(task)
            else:
                logging.info("ğŸ”´ æ™ºèƒ½è·¯ç”±å·²ç¦ç”¨ï¼Œä½¿ç”¨å®Œæ•´æ¨¡å¼")
                if log_callback:
                    log_callback("ğŸ”´ æ™ºèƒ½è·¯ç”±å·²ç¦ç”¨ï¼Œä½¿ç”¨å®Œæ•´æ¨¡å¼")
                complexity = "complex"

            tracker.checkpoint("1ï¸âƒ£ ä»»åŠ¡åˆ†ç±»")

            # âœ… å‘é€åˆ†ç±»ç»“æœ
            if log_callback:
                log_callback(f"ğŸ“Š ä»»åŠ¡å¤æ‚åº¦: {complexity.upper()}")

        except Exception as e:
            logging.error(f"âŒ ä»»åŠ¡åˆ†ç±»å¤±è´¥: {e}ï¼Œå›é€€åˆ°å®Œæ•´æ¨¡å¼")
            if log_callback:
                log_callback(f"âš ï¸ ä»»åŠ¡åˆ†ç±»å¤±è´¥ï¼Œä½¿ç”¨å®Œæ•´æ¨¡å¼")
            complexity = "complex"

        # å¤„ç†å›¾åƒ
        if image_paths:
            image_paths = image_paths[:self.max_images]
            logging.info(f"ğŸ“· å¤„ç† {len(image_paths)} å¼ å›¾ç‰‡")
            if log_callback:
                log_callback(f"ğŸ“· å¤„ç† {len(image_paths)} å¼ å›¾ç‰‡")

        history: List[Dict] = []

        # æ„å»ºåˆå§‹ history
        if image_paths:
            image_content = [{"type": "text", "text": task}]
            for idx, path in enumerate(image_paths, 1):
                if not os.path.exists(path):
                    logging.warning(f"âš ï¸ å›¾ç‰‡ä¸å­˜åœ¨: {path}")
                    continue
                try:
                    mime_type, _ = mimetypes.guess_type(path)
                    if not mime_type or not mime_type.startswith("image/"):
                        mime_type = "image/jpeg"
                    with open(path, "rb") as f:
                        base64_image = base64.b64encode(f.read()).decode('utf-8')
                    image_content.append({
                        "type": "image_url",
                        "image_url": {"url": f"data:{mime_type};base64,{base64_image}"}
                    })
                except Exception as e:
                    logging.error(f"  âŒ è¯»å–å›¾ç‰‡å¤±è´¥ {path}: {e}")
            history.append({"speaker": "User", "content": image_content})
        else:
            history.append({"speaker": "User", "content": task})

        # âœ¨âœ¨âœ¨ ä¸‰çº§è·¯ç”±æ‰§è¡Œï¼ˆå¸¦å¼‚å¸¸é™çº§ï¼‰âœ¨âœ¨âœ¨
        final_answer = ""
        execution_mode = complexity  # è®°å½•å®é™…æ‰§è¡Œæ¨¡å¼

        try:
            if complexity == "simple":
                final_answer = self._solve_simple(
                    task, history,
                    stream_callback, log_callback  # âœ… ä¼ é€’
                )

            elif complexity == "medium":
                final_answer = self._solve_medium(
                    task, history, tracker,
                    stream_callback, log_callback  # âœ… ä¼ é€’
                )

            else:  # complex
                final_answer = self._solve_complex(
                    task, history, tracker, use_memory, memory_key,
                    stream_callback, log_callback  # âœ… ä¼ é€’
                )

        except Exception as e:
            logging.error(f"âŒ {complexity.upper()} æ¨¡å¼æ‰§è¡Œå¤±è´¥: {e}")
            print(f"\n{'!' * 80}")
            print(f"âš ï¸  {complexity.upper()} æ¨¡å¼æ‰§è¡Œå¤±è´¥: {str(e)[:100]}")
            print(f"ğŸ”„ è‡ªåŠ¨é™çº§åˆ° COMPLEX å®Œæ•´æ¨¡å¼...")
            print(f"{'!' * 80}\n")

            # âœ… å‘é€é™çº§æ—¥å¿—
            if log_callback:
                log_callback(f"âš ï¸ {complexity.upper()} æ¨¡å¼å¤±è´¥ï¼Œé™çº§åˆ° COMPLEX")

            # é™çº§åˆ°å®Œæ•´æ¨¡å¼
            execution_mode = "complex (é™çº§)"
            try:
                final_answer = self._solve_complex(
                    task, history, tracker, use_memory, memory_key,
                    stream_callback, log_callback  # âœ… ä¼ é€’
                )
            except Exception as fallback_error:
                logging.error(f"âŒ é™çº§æ‰§è¡Œä¹Ÿå¤±è´¥: {fallback_error}")
                final_answer = f"[ç³»ç»Ÿé”™è¯¯] ä»»åŠ¡æ‰§è¡Œå¤±è´¥:\nåŸå§‹é”™è¯¯: {str(e)}\né™çº§é”™è¯¯: {str(fallback_error)}"
                if log_callback:
                    log_callback(f"âŒ ç³»ç»Ÿé”™è¯¯: ä»»åŠ¡æ‰§è¡Œå¤±è´¥")

        # ===== ç»Ÿä¸€è¾“å‡ºï¼ˆä¸‰ç§æ¨¡å¼å…±ç”¨ï¼‰ =====
        print("\n" + "=" * 100)
        print(f"ğŸ¯ ã€æœ€ç»ˆç­”æ¡ˆã€‘ï¼ˆæ‰§è¡Œæ¨¡å¼: {execution_mode.upper()}ï¼‰")
        print("=" * 100)
        print(final_answer)
        print("=" * 100)

        # âœ… å‘é€å®Œæˆæ—¥å¿—
        if log_callback:
            log_callback(f"âœ… ä»»åŠ¡å®Œæˆ (æ¨¡å¼: {execution_mode.upper()})")

        # çŸ¥è¯†å›¾è°±è’¸é¦ï¼ˆä»… complex æ¨¡å¼ï¼‰
        if complexity == "complex" and self.knowledge_graph:
            kg_summary = self.knowledge_graph.distill(max_items=10)
            if kg_summary:
                print("\n" + "â”€" * 100)
                print("ğŸ§  ã€çŸ¥è¯†å›¾è°±è’¸é¦ã€‘")
                print("â”€" * 100)
                print(kg_summary)
                print("â”€" * 100)

        # æ—¶é—´ç»Ÿè®¡
        print(tracker.summary())
        logging.info(tracker.summary())

        return final_answer

    def _solve_complex(
            self,
            task: str,
            history: List[Dict],
            tracker: TimeTracker,
            use_memory: bool,
            memory_key: str,
            stream_callback=None,  # âœ… æ–°å¢
            log_callback=None  # âœ… æ–°å¢
    ) -> str:
        """
        ğŸ”´ å®Œæ•´æ¨¡å¼ï¼šå…¨åŠŸèƒ½åä½œï¼ˆåŸ solve() çš„ complex åˆ†æ”¯ï¼‰
        """
        logging.info("ğŸ”´ æ‰§è¡Œå®Œæ•´æ¨¡å¼ï¼ˆå…¨åŠŸèƒ½åä½œï¼‰")
        print(f"\n{'=' * 80}")
        print("ğŸ”´ æ£€æµ‹åˆ°å¤æ‚ä»»åŠ¡ï¼Œå¯ç”¨å…¨åŠŸèƒ½åä½œæ¨¡å¼")
        print(f"{'=' * 80}\n")

        # âœ… å‘é€æ—¥å¿—
        if log_callback:
            log_callback("ğŸ”´ æ‰§è¡Œå®Œæ•´æ¨¡å¼ï¼ˆå…¨åŠŸèƒ½åä½œï¼‰")

        # ===== ä»»åŠ¡åˆ†è§£ =====
        if self.enable_task_decomposition and self.mode == "intelligent":
            decomposition = self._decompose_task(task)
            if decomposition:
                history.insert(0, {"speaker": "System", "content": decomposition})
                tracker.checkpoint("2ï¸âƒ£ ä»»åŠ¡åˆ†è§£")
                if log_callback:
                    log_callback("ğŸ“‹ ä»»åŠ¡åˆ†è§£å®Œæˆ")

        # ===== åŠ è½½å†å²è®°å¿† =====
        if use_memory and memory_key in self.memory:
            memory_text = "\n".join([
                f"- {item['summary']}"
                for item in self.memory[memory_key][-5:]
            ])
            history.insert(0, {
                "speaker": "System",
                "content": f"ğŸ“š å†å²è®°å¿†ï¼ˆ{memory_key}ï¼‰ï¼š\n{memory_text}"
            })
            if log_callback:
                log_callback(f"ğŸ“š åŠ è½½å†å²è®°å¿†: {memory_key}")

        # ===== ä¸»å¾ªç¯ï¼ˆå¤šè½®è®¨è®º + è¾©è®ºï¼‰=====
        round_num = 0
        previous_quality = 0

        while True:
            round_num += 1
            if round_num > self.max_rounds:
                logging.info(f"â¸ï¸  è¾¾åˆ°æœ€å¤§è½®æ¬¡ {self.max_rounds}ï¼Œåœæ­¢è®¨è®º")
                if log_callback:
                    log_callback(f"â¸ï¸ è¾¾åˆ°æœ€å¤§è½®æ¬¡ {self.max_rounds}")
                break

            logging.info(f"\n{'â”€' * 80}")
            logging.info(f"ğŸ”„ ç¬¬ {round_num} è½®è®¨è®ºå¼€å§‹")
            logging.info(f"{'â”€' * 80}")

            # âœ… å‘é€è½®æ¬¡æ—¥å¿—
            if log_callback:
                log_callback(f"ğŸ”„ ç¬¬ {round_num}/{self.max_rounds} è½®è®¨è®ºå¼€å§‹")

            round_start = time.time()

            # å¹¶å‘æ‰§è¡Œ Agent è®¨è®º
            with ThreadPoolExecutor(max_workers=self.max_concurrent_agents) as executor:
                future_to_agent = {
                    executor.submit(
                        agent.generate_response,
                        history.copy(),
                        round_num,
                        critique_previous=(round_num > 1 and self.enable_adversarial_debate),
                        log_callback=log_callback  # âœ… ä»…ä¼ é€’æ—¥å¿—ï¼ˆé¿å…æµå¼æ··ä¹±ï¼‰
                    ): agent
                    for agent in self.agents
                }

                for future in as_completed(future_to_agent):
                    agent = future_to_agent[future]
                    try:
                        contribution = future.result()
                        history.append({
                            "speaker": agent.name,
                            "content": contribution
                        })

                        # çŸ¥è¯†å›¾è°±æå–
                        if self.knowledge_graph and len(contribution) > 50:
                            words = contribution.split()
                            for i, word in enumerate(words[:50]):
                                if word.istitle() and len(word) > 3:
                                    self.knowledge_graph.add_entity(
                                        word, "concept", f"{agent.name}æåŠ"
                                    )
                    except Exception as e:
                        logging.error(f"âŒ {agent.name} æ‰§è¡Œå¤±è´¥: {e}")
                        if log_callback:
                            log_callback(f"âŒ {agent.name} æ‰§è¡Œå¤±è´¥")

            round_elapsed = time.time() - round_start
            tracker.checkpoint(f"3ï¸âƒ£ ç¬¬{round_num}è½®è®¨è®º ({round_elapsed:.1f}ç§’)")

            # ===== å¯¹æŠ—å¼è¾©è®ºä¸è‡ªé€‚åº”åæ€ =====
            if self.mode == "intelligent" and self.reflection_planning:
                if log_callback:
                    log_callback(f"ğŸ¥Š å¯åŠ¨å¯¹æŠ—å¼è¾©è®º (ç¬¬ {round_num} è½®)")

                quality_score, decision = self._adversarial_debate(history, round_num)
                tracker.checkpoint(f"4ï¸âƒ£ ç¬¬{round_num}è½®è¾©è®º")

                # âœ… å‘é€è¾©è®ºç»“æœ
                if log_callback:
                    log_callback(f"ğŸ“Š è¾©è®ºç»“æœ: è´¨é‡ {quality_score}/100, å†³ç­– {decision}")

                if self.enable_adaptive_depth:
                    # è´¨é‡è¾¾æ ‡ç«‹å³åœæ­¢
                    if quality_score >= self.reflection_quality_threshold:
                        logging.info(f"âœ… è´¨é‡è¾¾æ ‡ ({quality_score} >= {self.reflection_quality_threshold})ï¼Œåœæ­¢è®¨è®º")
                        if log_callback:
                            log_callback(f"âœ… è´¨é‡è¾¾æ ‡ ({quality_score}åˆ†)")
                        break

                    # è£åˆ¤å»ºè®®åœæ­¢ + è´¨é‡å¯æ¥å—
                    if decision == "stop" and quality_score >= self.stop_quality_threshold:
                        logging.info(f"âœ… è£åˆ¤å»ºè®®åœæ­¢ + è´¨é‡å¯æ¥å— ({quality_score} >= {self.stop_quality_threshold})")
                        if log_callback:
                            log_callback(f"âœ… è£åˆ¤å»ºè®®åœæ­¢ (è´¨é‡ {quality_score}åˆ†)")
                        break

                    # è´¨é‡æ”¶æ•›åˆ¤å®š
                    if round_num > 1 and previous_quality > 0:
                        quality_delta = quality_score - previous_quality
                        if abs(quality_delta) < self.quality_convergence_delta:
                            logging.info(
                                f"ğŸ“‰ è´¨é‡æ”¶æ•› (Î”={quality_delta:.1f} < {self.quality_convergence_delta})ï¼Œåœæ­¢è®¨è®º")
                            if log_callback:
                                log_callback(f"ğŸ“‰ è´¨é‡æ”¶æ•› (Î”={quality_delta:.1f})")
                            break

                    previous_quality = quality_score
                else:
                    # éè‡ªé€‚åº”æ¨¡å¼ï¼šä»…å¬ä»è£åˆ¤å†³ç­–
                    if decision == "stop":
                        logging.info("âœ… è£åˆ¤å»ºè®®åœæ­¢ï¼Œç»“æŸè®¨è®º")
                        if log_callback:
                            log_callback("âœ… è£åˆ¤å»ºè®®åœæ­¢")
                        break

        # ===== æœ€ç»ˆç»¼åˆ =====
        if log_callback:
            log_callback("ğŸ¯ å¼€å§‹æœ€ç»ˆç»¼åˆ")

        kg_context = ""
        if self.knowledge_graph:
            kg_context = self.knowledge_graph.distill(max_items=10)
            if kg_context:
                history.insert(-1, {"speaker": "System", "content": kg_context})

        history.append({
            "speaker": "System",
            "content": (
                "è¯·ç»¼åˆä»¥ä¸Šå…¨éƒ¨è®¨è®ºï¼Œç»™å‡ºæœ€å‡†ç¡®ã€æœ€å®Œæ•´ã€æœ€é«˜è´¨é‡çš„æœ€ç»ˆç­”æ¡ˆã€‚\n"
                "è¦æ±‚ï¼šé€»è¾‘ä¸¥å¯†ã€ä¿¡æ¯å®Œæ•´ã€ç»“æ„æ¸…æ™°ã€æ•´åˆçŸ¥è¯†å›¾è°±ã€‚"
            )
        })

        final_answer = self.leader.generate_response(
            history,
            round_num + 1,
            force_non_stream=False,
            stream_callback=stream_callback,  # âœ… ä¼ é€’æµå¼
            log_callback=log_callback  # âœ… ä¼ é€’æ—¥å¿—
        )

        tracker.checkpoint("5ï¸âƒ£ æœ€ç»ˆç»¼åˆ")

        # ===== ä¿å­˜è®°å¿† =====
        if use_memory:
            try:
                if log_callback:
                    log_callback("ğŸ’¾ ä¿å­˜è®°å¿†ä¸­...")

                summary = self.leader.generate_response(
                    history + [{
                        "speaker": "System",
                        "content": "è¯·ç”¨500å­—æ€»ç»“ï¼šæ ¸å¿ƒç»“è®ºã€å…³é”®å‘ç°ã€å¯å¤ç”¨ç»éªŒã€é—ç•™é—®é¢˜"
                    }],
                    round_num + 1,
                    force_non_stream=True
                )
                self._save_memory(memory_key, summary)

                # å‘é‡è®°å¿†
                if self.vector_memory:
                    self.vector_memory.add(
                        summary,
                        metadata={"task": task[:100], "memory_key": memory_key}
                    )

                tracker.checkpoint("6ï¸âƒ£ ä¿å­˜è®°å¿†")
                if log_callback:
                    log_callback("ğŸ’¾ è®°å¿†ä¿å­˜å®Œæˆ")

            except Exception as e:
                logging.error(f"âŒ ä¿å­˜è®°å¿†å¤±è´¥: {e}")
                if log_callback:
                    log_callback(f"âš ï¸ è®°å¿†ä¿å­˜å¤±è´¥")

        return final_answer

    def _classify_task_complexity(self, task: str) -> str:
        """
        âœ¨ æ™ºèƒ½ä»»åŠ¡åˆ†ç±»å™¨
        è¿”å›: "simple" | "medium" | "complex"
        """
        # å¿«é€Ÿè§„åˆ™è¿‡æ»¤ï¼ˆ0msï¼Œæ— APIè°ƒç”¨ï¼‰
        task_lower = task.lower().strip()

        # ç®€å•ä»»åŠ¡ç‰¹å¾ï¼ˆç›´æ¥åˆ¤å®šï¼‰
        simple_patterns = [
            # é—®å€™ç±»
            len(task) < 20 and any(word in task_lower for word in ["ä½ å¥½", "hi", "hello", "hey", "è°¢è°¢", "thank"]),
            # ç®€å•é—®ç­”
            task.endswith("?") and len(task) < 30,
            # å•ä¸€æŸ¥è¯¢
            task.startswith(("ä»€ä¹ˆæ˜¯", "who is", "when", "where")) and len(task) < 50,
        ]

        if any(simple_patterns):
            logging.info("ğŸŸ¢ ä»»åŠ¡åˆ†ç±»: SIMPLE (è§„åˆ™åŒ¹é…)")
            return "simple"

        # å¤æ‚ä»»åŠ¡ç‰¹å¾ï¼ˆç›´æ¥åˆ¤å®šï¼‰
        complex_patterns = [
            # æ˜ç¡®è¦æ±‚åä½œ
            any(word in task_lower for word in ["åˆ†ææŠ¥å‘Š", "æ·±åº¦", "å¯¹æ¯”", "è¯„ä¼°", "æˆ˜ç•¥", "æ–¹æ¡ˆ", "ä»£ç å®¡æŸ¥"]),
            # å¤šæ­¥éª¤ä»»åŠ¡
            task.count("å¹¶ä¸”") + task.count("ç„¶å") + task.count("åŒæ—¶") + task.count("and then") >= 2,
            # æ–‡ä»¶æ“ä½œ
            any(word in task_lower for word in ["å†™å…¥", "ä¿å­˜", "ç”Ÿæˆæ–‡ä»¶", "write to", "save to"]),
            # é•¿æ–‡æœ¬
            len(task) > 200,
        ]

        if any(complex_patterns):
            logging.info("ğŸ”´ ä»»åŠ¡åˆ†ç±»: COMPLEX (è§„åˆ™åŒ¹é…)")
            return "complex"

        # ä¸­ç­‰å¤æ‚åº¦ï¼šç”¨ Leader å¿«é€Ÿåˆ¤æ–­ï¼ˆå•æ¬¡ API è°ƒç”¨ï¼Œ~0.5ç§’ï¼‰
        try:
            classify_prompt = (
                f"ä»»åŠ¡: {task}\n\n"
                "è¯·åˆ¤æ–­æ­¤ä»»åŠ¡çš„å¤æ‚åº¦ï¼ˆä»…å›å¤ä¸€ä¸ªè¯ï¼‰ï¼š\n"
                "- simple: ç®€å•é—®å€™/å•å¥é—®ç­”/æŸ¥è¯¢\n"
                "- medium: éœ€è¦åˆ†æä½†ä¸å¤æ‚ï¼ˆå¦‚è§£é‡Šæ¦‚å¿µã€ç®€å•å»ºè®®ï¼‰\n"
                "- complex: éœ€è¦æ·±åº¦åˆ†æ/å¤šæ­¥éª¤/åä½œ\n\n"
                "å›å¤æ ¼å¼: ä»…è¾“å‡º simple/medium/complex"
            )

            response = self.leader.client.chat.completions.create(
                model=self.leader.model,
                messages=[
                    {"role": "system", "content": "ä½ æ˜¯ä»»åŠ¡å¤æ‚åº¦åˆ†ç±»å™¨ï¼Œä»…å›å¤ simple/medium/complex"},
                    {"role": "user", "content": classify_prompt}
                ],
                temperature=0.0,
                max_tokens=10,
                stream=False
            )

            # âœ… æ ¸å¿ƒä¿®å¤ï¼šå¤„ç† None è¿”å›å€¼
            content = response.choices[0].message.content

            if content is None or not content.strip():
                logging.warning("âš ï¸ API è¿”å›ç©ºå€¼ï¼Œé»˜è®¤ä½¿ç”¨ medium")
                return "medium"

            complexity = content.strip().lower()

            if complexity in ["simple", "medium", "complex"]:
                logging.info(f"ğŸŸ¡ ä»»åŠ¡åˆ†ç±»: {complexity.upper()} (AIåˆ¤æ–­)")
                return complexity
            else:
                logging.warning(f"âš ï¸ AIåˆ†ç±»è¿”å›æ— æ•ˆå€¼: {complexity}ï¼Œé»˜è®¤ä½¿ç”¨ medium")
                return "medium"

        except Exception as e:
            logging.error(f"âŒ ä»»åŠ¡åˆ†ç±»å¤±è´¥: {e}ï¼Œé»˜è®¤ä½¿ç”¨ medium")
            return "medium"

    def _solve_simple(
            self,
            task: str,
            history: List[Dict],
            stream_callback=None,  # âœ… æ–°å¢
            log_callback=None  # âœ… æ–°å¢
    ) -> str:
        """
        ğŸŸ¢ ç®€å•æ¨¡å¼ï¼šå• Agent ç›´æ¥å›ç­”
        """
        logging.info("ğŸŸ¢ æ‰§è¡Œç®€å•æ¨¡å¼ï¼ˆå•Agentç›´ç­”ï¼‰")
        print(f"\n{'=' * 80}")
        print("ğŸŸ¢ æ£€æµ‹åˆ°ç®€å•ä»»åŠ¡ï¼Œä½¿ç”¨å¿«é€Ÿæ¨¡å¼")
        print(f"{'=' * 80}\n")

        # âœ… å‘é€æ—¥å¿—
        if log_callback:
            log_callback("ğŸŸ¢ æ‰§è¡Œç®€å•æ¨¡å¼")

        # ç›´æ¥ç”¨ Leader å›ç­”ï¼ˆå…è®¸æµå¼è¾“å‡ºï¼‰
        answer = self.leader.generate_response(
            history,
            round_num=1,
            system_extra="è¯·ç®€æ´ã€ç›´æ¥åœ°å›ç­”ç”¨æˆ·é—®é¢˜ã€‚",
            force_non_stream=False,
            stream_callback=stream_callback,  # âœ… ä¼ é€’
            log_callback=log_callback  # âœ… ä¼ é€’
        )

        return answer

    def _solve_medium(
            self,
            task: str,
            history: List[Dict],
            tracker: TimeTracker,
            stream_callback=None,  # âœ… æ–°å¢
            log_callback=None  # âœ… æ–°å¢
    ) -> str:
        """
        ğŸŸ¡ ä¸­ç­‰æ¨¡å¼ï¼š2 Agents + å•è½®è®¨è®º
        """
        logging.info("ğŸŸ¡ æ‰§è¡Œä¸­ç­‰æ¨¡å¼ï¼ˆ2 Agents + 1è½®ï¼‰")
        print(f"\n{'=' * 80}")
        print("ğŸŸ¡ æ£€æµ‹åˆ°ä¸­ç­‰ä»»åŠ¡ï¼Œä½¿ç”¨ç²¾ç®€åä½œæ¨¡å¼")
        print(f"{'=' * 80}\n")

        # âœ… å‘é€æ—¥å¿—
        if log_callback:
            log_callback("ğŸŸ¡ æ‰§è¡Œä¸­ç­‰æ¨¡å¼ï¼ˆ2 Agents + 1è½®ï¼‰")

        # é€‰æ‹© 2 ä¸ªæœ€é€‚åˆçš„ Agentï¼ˆLeader + 1ä¸ªä¸“å®¶ï¼‰
        selected_agents = [self.leader]

        if len(self.agents) > 1:
            # ç®€å•ç­–ç•¥ï¼šé€‰æ‹©ç¬¬äºŒä¸ªAgentï¼ˆé€šå¸¸æ˜¯åˆ›æ„/åˆ†æä¸“å®¶ï¼‰
            selected_agents.append(self.agents[1])

        # å•è½®å¹¶å‘è®¨è®º
        with ThreadPoolExecutor(max_workers=2) as executor:
            future_to_agent = {
                executor.submit(
                    agent.generate_response,
                    history.copy(),
                    1,
                    log_callback=log_callback  # âœ… ä¼ é€’æ—¥å¿—ï¼ˆä¸ä¼  stream é¿å…æ··ä¹±ï¼‰
                ): agent
                for agent in selected_agents
            }

            for future in as_completed(future_to_agent):
                agent = future_to_agent[future]
                try:
                    contribution = future.result()
                    history.append({
                        "speaker": agent.name,
                        "content": contribution
                    })
                except Exception as e:
                    logging.error(f"âŒ {agent.name} æ‰§è¡Œå¤±è´¥: {e}")
                    if log_callback:
                        log_callback(f"âŒ {agent.name} æ‰§è¡Œå¤±è´¥")

        tracker.checkpoint("2ï¸âƒ£ å•è½®è®¨è®º")

        # Leader å¿«é€Ÿç»¼åˆ
        history.append({
            "speaker": "System",
            "content": "è¯·ç®€æ´ç»¼åˆä»¥ä¸Šè§‚ç‚¹ï¼Œç»™å‡ºæ¸…æ™°ç­”æ¡ˆã€‚"
        })

        final_answer = self.leader.generate_response(
            history,
            2,
            force_non_stream=False,
            stream_callback=stream_callback,  # âœ… ä¼ é€’æµå¼
            log_callback=log_callback  # âœ… ä¼ é€’æ—¥å¿—
        )

        tracker.checkpoint("3ï¸âƒ£ å¿«é€Ÿç»¼åˆ")

        return final_answer


# ====================== ä¸»å‡½æ•° ======================
if __name__ == "__main__":
    try:
        swarm = MultiAgentSwarm()

        print("\n" + "ğŸ§ª" * 40)
        print("å¼€å§‹æµ‹è¯•æ™ºèƒ½è·¯ç”±åŠŸèƒ½")
        print("ğŸ§ª" * 40 + "\n")

        # ===== æµ‹è¯• 1ï¼šç®€å•æ¨¡å¼ =====
        print("\nğŸ“ æµ‹è¯• 1: ç®€å•é—®å€™ï¼ˆé¢„æœŸ: SIMPLE æ¨¡å¼ï¼Œ~1-2ç§’ï¼‰")
        print("=" * 80)
        msg = swarm.solve("ä½ å¥½ï¼Œä»Šå¤©å¤©æ°”æ€ä¹ˆæ ·ï¼Ÿ")
        print('æµ‹è¯•1 å›ç­”:\n', msg)
        input("\næŒ‰å›è½¦ç»§ç»­ä¸‹ä¸€ä¸ªæµ‹è¯•...")

        # ===== æµ‹è¯• 2ï¼šä¸­ç­‰æ¨¡å¼ =====
        print("\nğŸ“ æµ‹è¯• 2: æ¦‚å¿µè§£é‡Šï¼ˆé¢„æœŸ: MEDIUM æ¨¡å¼ï¼Œ~10-20ç§’ï¼‰")
        print("=" * 80)
        msg = swarm.solve("è¯·è§£é‡Šä¸€ä¸‹ä»€ä¹ˆæ˜¯ Transformer æ³¨æ„åŠ›æœºåˆ¶")
        print('æµ‹è¯•2 å›ç­”:\n', msg)
        input("\næŒ‰å›è½¦ç»§ç»­ä¸‹ä¸€ä¸ªæµ‹è¯•...")

        # ===== æµ‹è¯• 3ï¼šå¤æ‚æ¨¡å¼ =====
        print("\nğŸ“ æµ‹è¯• 3: æ·±åº¦åˆ†æï¼ˆé¢„æœŸ: COMPLEX æ¨¡å¼ï¼Œ~40-60ç§’ï¼‰")
        print("=" * 80)
        msg = swarm.solve(
            "è¯·å†™ä¸€ç¯‡å…³äºå¤§è¯­è¨€æ¨¡å‹è®­ç»ƒæŠ€æœ¯çš„æ·±åº¦åˆ†ææŠ¥å‘Šï¼Œ"
            "åŒ…æ‹¬æ•°æ®å‡†å¤‡ã€æ¨¡å‹æ¶æ„ã€è®­ç»ƒç­–ç•¥çš„å¯¹æ¯”åˆ†æ",
            use_memory=True,
            memory_key="llm_training"
        )
        print('æµ‹è¯•3 å›ç­”:\n', msg)
        input("\næŒ‰å›è½¦ç»§ç»­ä¸‹ä¸€ä¸ªæµ‹è¯•...")

        # ===== æµ‹è¯• 4ï¼šå¼ºåˆ¶æ¨¡å¼ =====
        print("\nğŸ“ æµ‹è¯• 4: å¼ºåˆ¶ä½¿ç”¨ COMPLEX æ¨¡å¼å¤„ç†ç®€å•ä»»åŠ¡")
        print("=" * 80)
        msg = swarm.solve("ä½ å¥½", force_complexity="complex")
        print('æµ‹è¯•4 å›ç­”:\n', msg)

        # ç¤ºä¾‹5ï¼šå›¾åƒåˆ†æï¼ˆéœ€è¦æä¾›çœŸå®å›¾ç‰‡è·¯å¾„ï¼‰
        # swarm.solve(
        #     "è¯·åˆ†æè¿™äº›å›¾ç‰‡ä¸­çš„ä»£ç é—®é¢˜",
        #     image_paths=["./screenshot1.png", "./screenshot2.png"]
        # )

        print("\n" + "âœ…" * 40)
        print("æ‰€æœ‰æµ‹è¯•å®Œæˆï¼")
        print("âœ…" * 40 + "\n")

    except Exception as e:
        logging.error(f"âŒ ç¨‹åºå¼‚å¸¸: {e}", exc_info=True)
        print(f"\nâŒ é”™è¯¯: {e}")