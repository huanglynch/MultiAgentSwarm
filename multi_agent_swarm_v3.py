#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
å¤šæ™ºèƒ½ä½“åä½œç³»ç»Ÿ (Multi-Agent Swarm) v3.0.0
âœ¨ æ–°å¢åŠŸèƒ½ï¼š
- å¤šå±‚å¯¹æŠ—è¾©è®º (Adversarial Debate)
- Meta-Critic å…ƒæ‰¹è¯„æœºåˆ¶
- åŠ¨æ€ Agent å·¥å‚ + ä»»åŠ¡åˆ†è§£
- ä¸»åŠ¨çŸ¥è¯†å›¾è°± + è’¸é¦
- è‡ªé€‚åº”åæ€æ·±åº¦
"""

import yaml
import logging
import importlib.util
import requests
import random
import time
import threading
import base64
import mimetypes
from bs4 import BeautifulSoup
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import List, Dict, Optional, Tuple
from openai import OpenAI
import json
import chromadb
from chromadb.config import Settings
from sentence_transformers import SentenceTransformer
from duckduckgo_search import DDGS
import os
from datetime import datetime, timedelta
from pathlib import Path
import glob

# ====================== æ—¶é—´ç»Ÿè®¡å·¥å…· ======================
from contextlib import contextmanager


class TimeTracker:
    """æ—¶é—´ç»Ÿè®¡å·¥å…·ç±»"""

    def __init__(self):
        self.start_time = None
        self.checkpoints = {}

    def start(self):
        """å¼€å§‹è®¡æ—¶"""
        self.start_time = time.time()
        return self.start_time

    def checkpoint(self, name: str):
        """è®°å½•æ£€æŸ¥ç‚¹"""
        if self.start_time is None:
            self.start()
        elapsed = time.time() - self.start_time
        self.checkpoints[name] = elapsed
        return elapsed

    def get_elapsed(self) -> float:
        """è·å–æ€»è€—æ—¶"""
        if self.start_time is None:
            return 0
        return time.time() - self.start_time

    def format_time(self, seconds: float) -> str:
        """æ ¼å¼åŒ–æ—¶é—´æ˜¾ç¤º"""
        if seconds < 60:
            return f"{seconds:.2f}ç§’"
        elif seconds < 3600:
            minutes = int(seconds // 60)
            secs = seconds % 60
            return f"{minutes}åˆ†{secs:.1f}ç§’"
        else:
            hours = int(seconds // 3600)
            minutes = int((seconds % 3600) // 60)
            secs = seconds % 60
            return f"{hours}å°æ—¶{minutes}åˆ†{secs:.0f}ç§’"

    def summary(self) -> str:
        """ç”Ÿæˆè€—æ—¶æ‘˜è¦"""
        total = self.get_elapsed()
        lines = [f"\n{'=' * 60}"]
        lines.append(f"â±ï¸  æ€»è€—æ—¶: {self.format_time(total)}")
        lines.append(f"{'â”€' * 60}")

        if self.checkpoints:
            lines.append("ğŸ“Š å„é˜¶æ®µè€—æ—¶:")
            for name, elapsed in self.checkpoints.items():
                percentage = (elapsed / total * 100) if total > 0 else 0
                lines.append(f"   {name}: {self.format_time(elapsed)} ({percentage:.1f}%)")

        lines.append(f"{'=' * 60}")
        return "\n".join(lines)


@contextmanager
def timer(description: str):
    """ä¸Šä¸‹æ–‡ç®¡ç†å™¨ï¼šè‡ªåŠ¨è®¡æ—¶å¹¶æ‰“å°"""
    start = time.time()
    print(f"â±ï¸  å¼€å§‹: {description}", flush=True)
    try:
        yield
    finally:
        elapsed = time.time() - start
        print(f"âœ… å®Œæˆ: {description} | è€—æ—¶: {TimeTracker().format_time(elapsed)}", flush=True)


# ====================== çº¿ç¨‹å®‰å…¨çš„å·¥å…·ç¼“å­˜ ======================
tool_cache = {}
cache_count = 0
cache_lock = threading.Lock()


def clean_cache():
    """è‡ªåŠ¨æ¸…ç†å·¥å…·ç¼“å­˜ï¼ˆçº¿ç¨‹å®‰å…¨ï¼‰"""
    global cache_count
    with cache_lock:
        if cache_count > 50:
            tool_cache.clear()
            cache_count = 0
            logging.info("ğŸ§¹ è‡ªåŠ¨æ¸…ç†å·¥å…·ç¼“å­˜")


# ====================== å·¥å…·å‡½æ•° ======================
def web_search(query: str, num_results: int = 5) -> str:
    """DuckDuckGo ç½‘é¡µæœç´¢ï¼ˆå¸¦ç¼“å­˜ + éšæœºå»¶æ—¶ï¼‰"""
    global cache_count
    clean_cache()

    with cache_lock:
        if query in tool_cache:
            return tool_cache[query]

    time.sleep(random.uniform(0.5, 2.0))

    try:
        with DDGS() as ddgs:
            results = [r for r in ddgs.text(query, max_results=num_results)]

        result = "\n".join([
            f"æ ‡é¢˜: {r['title']}\næ‘˜è¦: {r['body']}\né“¾æ¥: {r['href']}"
            for r in results
        ])

        with cache_lock:
            tool_cache[query] = result
            cache_count += 1

        return result
    except Exception as e:
        logging.error(f"æœç´¢å¤±è´¥: {e}")
        return f"æœç´¢å¤±è´¥: {str(e)}"


def browse_page(url: str) -> str:
    """æµè§ˆç½‘é¡µå¹¶æå–æ–‡æœ¬ï¼ˆå¸¦ç¼“å­˜ï¼‰"""
    global cache_count
    clean_cache()

    with cache_lock:
        if url in tool_cache:
            return tool_cache[url]

    try:
        headers = {"User-Agent": "Mozilla/5.0"}
        resp = requests.get(url, headers=headers, timeout=10)
        resp.raise_for_status()

        soup = BeautifulSoup(resp.text, "html.parser")
        for script in soup(["script", "style"]):
            script.decompose()

        text = soup.get_text()
        lines = (line.strip() for line in text.splitlines())
        chunks = (phrase.strip() for line in lines for phrase in line.split("  "))
        result = "\n".join(chunk for chunk in chunks if chunk)

        with cache_lock:
            tool_cache[url] = result
            cache_count += 1

        return result
    except Exception as e:
        logging.error(f"æµè§ˆå¤±è´¥ {url}: {e}")
        return f"æµè§ˆå¤±è´¥: {str(e)}"


def run_python(code: str) -> str:
    """
    æ²™ç®±æ‰§è¡Œ Python ä»£ç ï¼ˆ10ç§’è¶…æ—¶ï¼‰
    æ³¨æ„ï¼šthreading.Timer æ— æ³•çœŸæ­£ç»ˆæ­¢é˜»å¡ä»£ç ï¼Œä»…ä½œè½¯è¶…æ—¶
    """
    result_container = {"output": None, "done": False}

    def target():
        try:
            restricted_globals = {
                "__builtins__": {
                    "print": print,
                    "range": range,
                    "len": len,
                    "str": str,
                    "int": int,
                    "float": float,
                    "list": list,
                    "dict": dict,
                    "sum": sum,
                    "min": min,
                    "max": max,
                }
            }
            local_vars = {}
            exec(code, restricted_globals, local_vars)
            result_container["output"] = str(local_vars.get("result", "æ‰§è¡ŒæˆåŠŸï¼Œæ— è¿”å›ç»“æœ"))
        except Exception as e:
            result_container["output"] = f"æ‰§è¡Œé”™è¯¯: {str(e)}"
        finally:
            result_container["done"] = True

    thread = threading.Thread(target=target, daemon=True)
    thread.start()
    thread.join(timeout=10.0)

    if not result_container["done"]:
        return "â±ï¸ æ‰§è¡Œè¶…æ—¶ï¼ˆ10ç§’ï¼‰"

    return result_container["output"]


# ====================== å‘é‡è®°å¿† ======================
class VectorMemory:
    """
    åŸºäº ChromaDB å’Œ SentenceTransformer çš„å‘é‡è®°å¿†ç³»ç»Ÿ
    âœ… ä¼˜å…ˆä½¿ç”¨ç¼“å­˜æ¨¡å‹ï¼Œé¿å…é‡å¤ä¸‹è½½
    """

    def __init__(
            self,
            persist_directory: str = "./memory_db",
            model_name: str = "sentence-transformers/all-MiniLM-L6-v2",
            cache_dir: str = "./cached_model/"
    ):
        """
        åˆå§‹åŒ–å‘é‡è®°å¿†ç³»ç»Ÿ

        Args:
            persist_directory: ChromaDB æ•°æ®åº“è·¯å¾„
            model_name: SentenceTransformer æ¨¡å‹åç§°
            cache_dir: æ¨¡å‹ç¼“å­˜ç›®å½•ï¼ˆä¼˜å…ˆä»æ­¤å¤„åŠ è½½ï¼‰
        """
        self.persist_directory = persist_directory
        self.model_name = model_name
        self.cache_dir = os.path.abspath(cache_dir)

        # ç¡®ä¿ç¼“å­˜ç›®å½•å­˜åœ¨
        os.makedirs(self.cache_dir, exist_ok=True)

        # åˆå§‹åŒ–åµŒå…¥æ¨¡å‹ï¼ˆä¼˜å…ˆä½¿ç”¨ç¼“å­˜ï¼‰
        self._init_embedding_model()

        # åˆå§‹åŒ– ChromaDB
        self._init_chromadb()

    def _init_embedding_model(self):
        """åˆå§‹åŒ–åµŒå…¥æ¨¡å‹ï¼ˆä¼˜å…ˆä»ç¼“å­˜åŠ è½½ï¼‰"""
        try:
            # æ£€æŸ¥ç¼“å­˜æ˜¯å¦å­˜åœ¨
            cached_model_path = os.path.join(self.cache_dir, self.model_name.replace('/', '_'))

            if os.path.exists(cached_model_path):
                logging.info(f"ğŸ“¦ ä»ç¼“å­˜åŠ è½½å‘é‡æ¨¡å‹: {cached_model_path}")
                print(f"ğŸ“¦ ä»ç¼“å­˜åŠ è½½å‘é‡æ¨¡å‹: {self.model_name}")
                self.embedding_model = SentenceTransformer(cached_model_path)
            else:
                logging.info(f"â¬‡ï¸  ä¸‹è½½å‘é‡æ¨¡å‹: {self.model_name} â†’ {cached_model_path}")
                print(f"â¬‡ï¸  é¦–æ¬¡ä½¿ç”¨ï¼Œæ­£åœ¨ä¸‹è½½å‘é‡æ¨¡å‹: {self.model_name}")
                print(f"   ä¸‹è½½åå°†ç¼“å­˜åˆ°: {cached_model_path}")

                # ä¸‹è½½æ¨¡å‹
                self.embedding_model = SentenceTransformer(self.model_name)

                # ä¿å­˜åˆ°ç¼“å­˜
                self.embedding_model.save(cached_model_path)
                logging.info(f"âœ… æ¨¡å‹å·²ç¼“å­˜åˆ°: {cached_model_path}")
                print(f"âœ… æ¨¡å‹å·²ç¼“å­˜ï¼Œä¸‹æ¬¡å°†ç›´æ¥ä½¿ç”¨")

        except Exception as e:
            logging.error(f"âŒ å‘é‡æ¨¡å‹åˆå§‹åŒ–å¤±è´¥: {e}")
            raise

    def _init_chromadb(self):
        """åˆå§‹åŒ– ChromaDB å®¢æˆ·ç«¯"""
        try:
            os.makedirs(os.path.dirname(self.persist_directory) if os.path.dirname(
                self.persist_directory) else ".", exist_ok=True)

            self.client = chromadb.PersistentClient(
                path=self.persist_directory,
                settings=Settings(anonymized_telemetry=False)
            )

            # è·å–æˆ–åˆ›å»ºé›†åˆ
            self.collection = self.client.get_or_create_collection(
                name="swarm_memory",
                metadata={"description": "Agent memory storage"}
            )

            logging.info(f"âœ… ChromaDB åˆå§‹åŒ–æˆåŠŸ: {self.persist_directory}")

        except Exception as e:
            logging.error(f"âŒ ChromaDB åˆå§‹åŒ–å¤±è´¥: {e}")
            self.collection = None

    def add(self, text: str, metadata: Optional[Dict] = None):
        """æ·»åŠ è®°å¿†åˆ°å‘é‡æ•°æ®åº“"""
        if not self.collection:
            return

        try:
            # ç”ŸæˆåµŒå…¥å‘é‡
            embedding = self.embedding_model.encode(text).tolist()

            # ç”Ÿæˆ ID
            memory_id = f"mem_{datetime.now().strftime('%Y%m%d_%H%M%S_%f')}"

            # æ·»åŠ åˆ°æ•°æ®åº“
            self.collection.add(
                ids=[memory_id],
                embeddings=[embedding],
                documents=[text],
                metadatas=[metadata or {"timestamp": datetime.now().isoformat()}]
            )

            logging.info(f"âœ… è®°å¿†å·²ä¿å­˜: {memory_id}")

        except Exception as e:
            logging.error(f"âŒ ä¿å­˜è®°å¿†å¤±è´¥: {e}")

    def search(self, query: str, n_results: int = 5) -> str:
        """æœç´¢ç›¸å…³è®°å¿†"""
        if not self.collection:
            return ""

        try:
            # ç”ŸæˆæŸ¥è¯¢åµŒå…¥
            query_embedding = self.embedding_model.encode(query).tolist()

            # æœç´¢
            results = self.collection.query(
                query_embeddings=[query_embedding],
                n_results=n_results
            )

            # æ ¼å¼åŒ–ç»“æœ
            if results and results["documents"]:
                return "\n\n---\n\n".join(results["documents"][0])

        except Exception as e:
            logging.error(f"âŒ æœç´¢è®°å¿†å¤±è´¥: {e}")

        return ""


class PrimalMemory:
    """PrimalClawæç®€ç‰ˆï¼šæ ‘çŠ¶æ—¥å¿— + åŸå­KB + è¡°é€€ + å®Œæ•´QMDæ£€ç´¢ï¼ˆå·²ä¼˜åŒ–ï¼‰"""
    def __init__(self, base_dir: str = "./memory", vector_memory=None):  # â† æ”¹è¿™é‡Œ
        self.base_dir = Path(base_dir)
        self.logs_dir = self.base_dir / "logs"
        self.kb_dir = self.base_dir / "kb"
        self.archive_dir = self.base_dir / "archive"
        for d in [self.logs_dir, self.kb_dir / "lessons", self.kb_dir / "decisions", self.archive_dir]:
            d.mkdir(parents=True, exist_ok=True)

        self.vector_memory = vector_memory   # â† æ–°å¢è¿™ä¸€è¡Œ

    def save_episode(self, task: str, history: list, final_answer: str, memory_key: str):
        """å†™å…¥Layer2æ ‘çŠ¶æ—¥å¿— + åŸå­KBï¼ˆä¸è¦†ç›–ï¼‰"""
        today = datetime.now().strftime("%Y-%m-%d")
        timestamp = datetime.now().strftime('%H%M%S')
        log_path = self.logs_dir / today / f"{memory_key}_{timestamp}.md"
        log_path.parent.mkdir(exist_ok=True)

        content = f"# Episode: {memory_key}\n**æ—¶é—´**: {datetime.now()}\n**ä»»åŠ¡**: {task[:200]}\n\n"
        content += "## è®¨è®ºå†å²\n" + "\n".join([f"- **{h.get('speaker','')}:** {h.get('content','')[:300]}..." for h in history[-5:]])
        content += f"\n\n## æœ€ç»ˆç­”æ¡ˆ\n{final_answer[:2000]}\n"

        log_path.write_text(content, encoding="utf-8")

        # åŸå­KBï¼ˆä¸è¦†ç›–ï¼Œç”¨æ—¶é—´æˆ³ï¼‰
        kb_path = self.kb_dir / "lessons" / f"{memory_key}_{timestamp}.md"
        kb_text = f"## lesson-{memory_key}-{timestamp}\n"
        kb_text += f"**Title**: ä» {memory_key} æç‚¼çš„å…³é”®ç»éªŒ\n"
        kb_text += f"**Content**: {final_answer[:800].replace('**','')}\n"
        kb_text += f"**Source**: {log_path.relative_to(self.base_dir)}\n"
        kb_text += f"**Importance**: 0.85\n**LastAccess**: {datetime.now().isoformat()}\n"
        kb_path.write_text(kb_text, encoding="utf-8")

    def get_relevant_memory(self, query: str, n: int = 3) -> str:
        """å®Œæ•´QMDæ··åˆæ£€ç´¢ï¼šVectorè¯­ä¹‰ä¼˜å…ˆ + æ–‡ä»¶å…³é”®è¯"""
        results = []

        # 1. ä¼˜å…ˆç”¨å·²æœ‰VectorMemoryï¼ˆè¯­ä¹‰å¼ºï¼‰
        if hasattr(self, 'vector_memory') and self.vector_memory:  # æ³¨æ„ï¼šéœ€åœ¨MultiAgentSwarmä¸­ä¼ å¼•ç”¨
            vec_result = self.vector_memory.search(query, n_results=n)
            if vec_result:
                results.append(f"ğŸ” Vectorè¯­ä¹‰è®°å¿†:\n{vec_result}")

        # 2. æ–‡ä»¶ç²¾ç¡®åŒ¹é…ï¼ˆfallbackï¼‰
        for f in glob.glob(str(self.kb_dir / "**/*.md"), recursive=True)[:n]:
            try:
                text = Path(f).read_text(encoding="utf-8")[:600]
                if any(k in text.lower() for k in query.lower().split()[:4]):
                    results.append(f"ğŸ“ {Path(f).name}:\n{text}")
            except:
                pass

        # return "\n\n---\n\n".join(results[:n]) if results else ""
        result = "\n\n---\n\n".join(results[:n]) if results else ""
        # return result[:12000]  # â† æ–°å¢ï¼šå•æ¬¡æ³¨å…¥æœ€å¤š12Kå­—ç¬¦ï¼ˆâ‰ˆ6K tokensï¼‰
        return result[:12000]  # â† æ–°å¢ï¼šå•æ¬¡æ³¨å…¥æœ€å¤š12Kå­—ç¬¦ï¼ˆâ‰ˆ6K tokensï¼‰

    def decay(self):
        """æŒ‡æ•°è¡°é€€ + GCï¼ˆæ›´å®‰å…¨ï¼‰"""
        threshold = 0.4
        for md_file in glob.glob(str(self.kb_dir / "**/*.md"), recursive=True):
            try:
                text = Path(md_file).read_text(encoding="utf-8")
                if "Importance:" in text and "LastAccess:" in text:
                    imp_line = text.split("Importance:")[1].split("\n")[0].strip()
                    last_line = text.split("LastAccess:")[1].split("\n")[0].strip()
                    imp = float(imp_line)
                    last = datetime.fromisoformat(last_line)
                    days = (datetime.now() - last).days
                    new_imp = imp * (0.985 ** max(days, 0))
                    if new_imp < threshold:
                        archive_path = self.archive_dir / Path(md_file).name
                        Path(md_file).rename(archive_path)
            except:
                pass

# ====================== âœ¨ çŸ¥è¯†å›¾è°±ç®¡ç†å™¨ ======================
class KnowledgeGraph:
    """
    è½»é‡çº§çŸ¥è¯†å›¾è°± + è‡ªåŠ¨è’¸é¦
    ç”¨äºè¿½è¸ªå…³é”®æ¦‚å¿µã€å…³ç³»å’Œå‘ç°
    """

    def __init__(self, enable_distillation: bool = True):
        self.graph = {}  # {entity: {"type": str, "relations": [(rel, target)], "evidence": [str]}}
        self.enable_distillation = enable_distillation
        self.distilled_knowledge = []

    def add_entity(self, entity: str, entity_type: str = "concept", evidence: str = ""):
        """æ·»åŠ å®ä½“"""
        if entity not in self.graph:
            self.graph[entity] = {
                "type": entity_type,
                "relations": [],
                "evidence": [evidence] if evidence else []
            }
        elif evidence:
            self.graph[entity]["evidence"].append(evidence)

    def add_relation(self, source: str, relation: str, target: str):
        """æ·»åŠ å…³ç³»"""
        if source in self.graph:
            self.graph[source]["relations"].append((relation, target))
        else:
            self.add_entity(source)
            self.graph[source]["relations"].append((relation, target))

    def distill(self, max_items: int = 10) -> str:
        """
        è’¸é¦çŸ¥è¯†å›¾è°±ï¼ˆæå–æ ¸å¿ƒæ¦‚å¿µå’Œå…³ç³»ï¼‰
        """
        if not self.enable_distillation or not self.graph:
            return ""

        # æŒ‰å…³ç³»æ•°é‡æ’åºï¼ˆæœ€é‡è¦çš„æ¦‚å¿µï¼‰
        sorted_entities = sorted(
            self.graph.items(),
            key=lambda x: len(x[1]["relations"]),
            reverse=True
        )[:max_items]

        distilled = ["ğŸ§  æ ¸å¿ƒçŸ¥è¯†è’¸é¦:"]
        for entity, data in sorted_entities:
            relations_str = ", ".join([f"{rel}â†’{tgt}" for rel, tgt in data["relations"][:3]])
            distilled.append(f"â€¢ {entity} ({data['type']}): {relations_str}")

        result = "\n".join(distilled)
        self.distilled_knowledge.append(result)
        return result

    def get_context(self, entity: str, depth: int = 1) -> str:
        """è·å–å®ä½“çš„ä¸Šä¸‹æ–‡"""
        if entity not in self.graph:
            return ""

        context = [f"ğŸ“Œ {entity} ({self.graph[entity]['type']})"]

        # ä¸€çº§å…³ç³»
        for rel, target in self.graph[entity]["relations"]:
            context.append(f"  â””â”€ {rel} â†’ {target}")

            # äºŒçº§å…³ç³»ï¼ˆå¦‚æœ depth > 1ï¼‰
            if depth > 1 and target in self.graph:
                for sub_rel, sub_target in self.graph[target]["relations"][:2]:
                    context.append(f"     â””â”€ {sub_rel} â†’ {sub_target}")

        return "\n".join(context)


# ====================== Skill åŠ¨æ€åŠ è½½å™¨ ======================
def load_skills(skills_dir: str = "skills"):
    """
    é€’å½’åŠ è½½ skills ç›®å½•ä¸‹çš„æ‰€æœ‰ Python å·¥å…·å’Œ Markdown çŸ¥è¯†æ–‡ä»¶
    æ”¯æŒå­ç›®å½•ç»“æ„
    """
    tool_registry = {}
    shared_knowledge = []

    if not os.path.exists(skills_dir):
        logging.warning(f"âš ï¸ Skills ç›®å½•ä¸å­˜åœ¨: {skills_dir}")
        return tool_registry, ""

    # é€’å½’æ‰«ææ‰€æœ‰ .py æ–‡ä»¶
    py_files = glob.glob(os.path.join(skills_dir, "**/*.py"), recursive=True)

    for py_file in py_files:
        try:
            rel_path = os.path.relpath(py_file, skills_dir)

            # åŠ¨æ€å¯¼å…¥æ¨¡å—
            spec = importlib.util.spec_from_file_location(
                os.path.splitext(os.path.basename(py_file))[0],
                py_file
            )
            mod = importlib.util.module_from_spec(spec)
            spec.loader.exec_module(mod)

            # æŸ¥æ‰¾å·¥å…·å‡½æ•°å’Œ schema
            if hasattr(mod, "tool_function") and hasattr(mod, "tool_schema"):
                tool_name = mod.tool_schema["function"]["name"]
                tool_registry[tool_name] = {
                    "func": mod.tool_function,
                    "schema": mod.tool_schema
                }
                logging.info(f"âœ… åŠ è½½ Skill (py): {tool_name} | æ¥è‡ª: {rel_path}")
            else:
                logging.warning(f"âš ï¸ è·³è¿‡æ— æ•ˆ Skill æ–‡ä»¶: {rel_path}")

        except Exception as e:
            logging.error(f"âŒ åŠ è½½ Skill å¤±è´¥: {py_file} | é”™è¯¯: {e}")

    # é€’å½’æ‰«ææ‰€æœ‰ .md æ–‡ä»¶
    md_files = glob.glob(os.path.join(skills_dir, "**/*.md"), recursive=True)

    for md_file in md_files:
        try:
            rel_path = os.path.relpath(md_file, skills_dir)
            with open(md_file, "r", encoding="utf-8") as f:
                content = f.read().strip()
                if content:
                    shared_knowledge.append(f"### ğŸ“š æ¥è‡ª {rel_path} ###\n{content}")
                    logging.info(f"ğŸ“š åŠ è½½çŸ¥è¯†æ–‡ä»¶ (md): {rel_path}")
        except Exception as e:
            logging.error(f"âŒ è¯»å–çŸ¥è¯†æ–‡ä»¶å¤±è´¥: {md_file} | é”™è¯¯: {e}")

    logging.info(f"ğŸ“Š Skills åŠ è½½å®Œæˆ: {len(tool_registry)} ä¸ªå·¥å…·, {len(shared_knowledge)} ä¸ªçŸ¥è¯†æ–‡ä»¶")

    shared_knowledge_str = "\n\n".join(shared_knowledge)

    return tool_registry, shared_knowledge_str


# ====================== Agent ç±» ======================
class Agent:
    """å•ä¸ªæ™ºèƒ½ä½“ä»£ç†"""

    def __init__(
            self,
            config: Dict,
            default_model: str,
            default_max_tokens: int,
            tool_registry: Dict,
            shared_knowledge: str = "",
            vector_memory: Optional[VectorMemory] = None,
            knowledge_graph: Optional[KnowledgeGraph] = None,
            context_limit_k: str = "64"  # â† æ–°å¢è¿™ä¸€è¡Œ
    ):
        self.name = config["name"]
        self.role = config["role"]
        self.shared_knowledge = shared_knowledge
        self.vector_memory = vector_memory
        self.knowledge_graph = knowledge_graph
        self.context_limit_k = context_limit_k  # â† æ–°å¢è¿™ä¸€è¡Œï¼Œä¿å­˜ä¸‹æ¥

        # OpenAI å®¢æˆ·ç«¯é…ç½®
        self.client = OpenAI(
            api_key=config.get("api_key"),
            base_url=config.get("base_url")
        )
        self.model = config.get("model", default_model)
        self.temperature = config.get("temperature", 0.7)
        self.stream = config.get("stream", False)
        self.max_tokens = config.get("max_tokens", default_max_tokens)

        # å·¥å…·é…ç½®
        enabled = config.get("enabled_tools", [])
        self.tools = [
            tool_registry[name]["schema"]
            for name in enabled
            if name in tool_registry
        ]
        self.tool_map = {
            name: tool_registry[name]["func"]
            for name in enabled
            if name in tool_registry
        }

        if self.tools:
            logging.debug(f"  {self.name} å·²å¯ç”¨å·¥å…·: {list(self.tool_map.keys())}")

    def _execute_tool(self, tool_call) -> Dict:
        """æ‰§è¡Œå·¥å…·è°ƒç”¨"""
        func_name = tool_call.function.name

        try:
            args = json.loads(tool_call.function.arguments)
            logging.info(f"ğŸ”§ {self.name} è°ƒç”¨å·¥å…·: {func_name}({args})")

            result = self.tool_map[func_name](**args)

            return {
                "role": "tool",
                "tool_call_id": tool_call.id,
                "name": func_name,
                "content": str(result)
            }
        except Exception as e:
            logging.error(f"å·¥å…·æ‰§è¡Œå¤±è´¥ {func_name}: {e}")
            return {
                "role": "tool",
                "tool_call_id": tool_call.id,
                "name": func_name,
                "content": f"Tool error: {str(e)}"
            }

    def generate_response(
            self,
            history: List[Dict],
            round_num: int,
            system_extra: str = "",
            force_non_stream: bool = False,
            critique_previous: bool = False,
            stream_callback=None,
            log_callback=None
    ) -> str:
        """
        ç”Ÿæˆ Agent å“åº”ï¼ˆå¢å¼ºç‰ˆï¼šå·¥å…·è°ƒç”¨è¶…é™å…œåº•ï¼‰

        Args:
            history: å¯¹è¯å†å²
            round_num: å½“å‰è½®æ¬¡
            system_extra: é¢å¤–ç³»ç»Ÿæç¤ºè¯
            force_non_stream: å¼ºåˆ¶å…³é—­æµå¼è¾“å‡º
            critique_previous: æ˜¯å¦å¯ç”¨æ‰¹åˆ¤æ¨¡å¼
            stream_callback: æµå¼å›è°ƒå‡½æ•° callback(agent_name, chunk)
            log_callback: æ—¥å¿—å›è°ƒå‡½æ•° callback(message)

        Returns:
            str: Agent çš„å®Œæ•´å“åº”
        """
        # âœ… åˆ¤æ–­æ˜¯å¦ä½¿ç”¨æµå¼è¾“å‡ºï¼ˆå¦‚æœæœ‰ stream_callbackï¼Œå¼ºåˆ¶å¯ç”¨ï¼‰
        use_stream = (
                (self.stream or stream_callback is not None) and
                not force_non_stream and
                not self.tools  # æœ‰å·¥å…·æ—¶æš‚æ—¶ä¸ç”¨æµå¼
        )

        # âœ¨ æ‰¹åˆ¤æ¨¡å¼å¢å¼º
        if critique_previous and len(history) > 3:
            critique_prompt = (
                "ğŸ” åœ¨ç»™å‡ºä½ çš„è´¡çŒ®å‰ï¼Œè¯·å…ˆç”¨ [CRITIQUE] æ ‡è®°æŒ‡å‡ºä¸Šä¸€è½®è®¨è®ºä¸­ï¼š\n"
                "1. è‡³å°‘ 1 ä¸ªæ½œåœ¨é€»è¾‘æ¼æ´æˆ–çŸ›ç›¾ç‚¹\n"
                "2. è‡³å°‘ 1 ä¸ªå¯æ”¹è¿›æˆ–è¡¥å……çš„åœ°æ–¹\n"
                "ç„¶åå†ç»™å‡ºä½ çš„å»ºè®¾æ€§è´¡çŒ®ã€‚"
            )
            system_extra = critique_prompt + "\n\n" + system_extra

        # åœ¨ system_prompt = f"{self.role}\n..." ä¹‹å‰åŠ ï¼š
        # ğŸ”¥ã€Plan æ³¨å…¥ã€‘æ¯ä¸ª Agent éƒ½çŸ¥é“å½“å‰ Master Planï¼ˆæœ€å°æ”¹åŠ¨ï¼‰
        plan_summary = next(
            (h["content"] for h in history if "Master Plan" in str(h.get("content", ""))),
            ""
        )[:300]

        # æ„å»ºç³»ç»Ÿæç¤ºè¯ï¼ˆPlan æ‘˜è¦æ”¾åœ¨æœ€å‰é¢ï¼Œè®©æ‰€æœ‰ Agent å¯¹é½ï¼‰
        system_prompt = (
            f"{self.role}\n"
            f"ã€å½“å‰Master Planæ‘˜è¦ã€‘\n{plan_summary}\n\n"  # â† ç›´æ¥å†…åµŒ
            f"{self.shared_knowledge}\n"
            f"{system_extra}\n"
            "ä½ æ˜¯å¤šæ™ºèƒ½ä½“åä½œå›¢é˜Ÿçš„ä¸€å‘˜ï¼Œè¯·æä¾›æœ‰ä»·å€¼ã€å‡†ç¡®ã€æœ‰æ·±åº¦çš„è´¡çŒ®ã€‚"
        )

        messages = [{"role": "system", "content": system_prompt}]

        # å¤„ç†å†å²æ¶ˆæ¯
        for h in history:
            if h["speaker"] == "User":
                messages.append({"role": "user", "content": h["content"]})
            elif h["speaker"] == "System":
                messages.append({"role": "system", "content": h["content"]})
            else:
                messages.append({
                    "role": "assistant",
                    "content": f"[{h['speaker']}] {h.get('content', '')}"
                })

        est_tokens = sum(len(str(m.get("content", ""))) // 2 for m in messages)
        if est_tokens > 110000 and "128" in str(self.context_limit_k):
            logging.warning(f"âš ï¸ ä¸Šä¸‹æ–‡æ¥è¿‘128Kä¸Šé™ï¼å½“å‰ä¼°ç®— {est_tokens} tokens")
        start_time = time.time()

        try:
            # âœ… æ·»åŠ å¼€å§‹æ—¥å¿—
            if log_callback:
                log_callback(f"[{self.name}] å¼€å§‹ç”Ÿæˆå“åº” (è½®æ¬¡ {round_num})")

            if use_stream:
                print(f"\nğŸ’¬ ã€{self.name}ã€‘æ­£åœ¨æ€è€ƒ... ", end="", flush=True)
                if log_callback:
                    log_callback(f"[{self.name}] æ­£åœ¨æ€è€ƒ...")

            # è°ƒç”¨ API
            response = self.client.chat.completions.create(
                model=self.model,
                messages=messages,
                temperature=self.temperature,
                max_tokens=self.max_tokens,
                tools=self.tools if self.tools else None,
                tool_choice="auto" if self.tools else None,
                stream=use_stream,
            )

            full_response = ""

            # ===== æµå¼è¾“å‡º =====
            if use_stream:
                for chunk in response:
                    if chunk.choices[0].delta.content:
                        delta = chunk.choices[0].delta.content
                        print(delta, end="", flush=True)
                        full_response += delta

                        # âœ… æµå¼å›è°ƒ
                        if stream_callback:
                            stream_callback(self.name, delta)
                print()

            # ===== éæµå¼è¾“å‡º =====
            else:
                full_response = response.choices[0].message.content or ""

                # âœ… å³ä½¿éæµå¼ï¼Œä¹Ÿè°ƒç”¨å›è°ƒï¼ˆæ¨¡æ‹Ÿæµå¼æ•ˆæœï¼‰
                if stream_callback and full_response:
                    chunk_size = 20
                    for i in range(0, len(full_response), chunk_size):
                        chunk = full_response[i:i + chunk_size]
                        stream_callback(self.name, chunk)
                        time.sleep(0.02)

            # ===== ğŸ”§ å·¥å…·è°ƒç”¨å¤„ç†ï¼ˆæ”¯æŒå¤šè½®å¾ªç¯ + è¶…é™å…œåº•ï¼‰=====
            if (not use_stream and
                    hasattr(response.choices[0].message, 'tool_calls') and
                    response.choices[0].message.tool_calls):

                max_tool_iterations = 10
                iteration = 0
                tool_call_history = []  # ğŸ”¥ è®°å½•æ‰€æœ‰å·¥å…·è°ƒç”¨

                while (hasattr(response.choices[0].message, 'tool_calls') and
                       response.choices[0].message.tool_calls and
                       iteration < max_tool_iterations):

                    iteration += 1
                    print(f"\nğŸ”§ [{self.name}] å·¥å…·è°ƒç”¨ (ç¬¬ {iteration} è½®)")

                    if log_callback:
                        log_callback(f"[{self.name}] å·¥å…·è°ƒç”¨ (ç¬¬ {iteration} è½®)")

                    # æ·»åŠ  assistant æ¶ˆæ¯ï¼ˆåŒ…å« tool_callsï¼‰
                    messages.append(response.choices[0].message.model_dump())

                    # æ‰§è¡Œæ‰€æœ‰å·¥å…·è°ƒç”¨
                    for tool_call in response.choices[0].message.tool_calls:
                        tool_result = self._execute_tool(tool_call)
                        messages.append(tool_result)

                        # ğŸ”¥ è®°å½•å·¥å…·è°ƒç”¨
                        tool_call_history.append(tool_result['name'])

                        # æ˜¾ç¤ºå·¥å…·è°ƒç”¨ç»“æœï¼ˆæˆªæ–­é¢„è§ˆï¼‰
                        result_preview = tool_result.get("content", "")[:150]
                        if len(tool_result.get("content", "")) > 150:
                            result_preview += "..."
                        print(f"   âœ… {tool_result['name']}: {result_preview}")

                        if log_callback:
                            log_callback(f"[{self.name}] å·¥å…·: {tool_result['name']}")

                    # é‡æ–°è°ƒç”¨ APIï¼ˆå¸¦å·¥å…·ç»“æœï¼‰
                    response = self.client.chat.completions.create(
                        model=self.model,
                        messages=messages,
                        temperature=self.temperature,
                        max_tokens=self.max_tokens,
                        tools=self.tools if self.tools else None,
                        tool_choice="auto" if self.tools else None,
                        stream=False
                    )

                    # âœ… å¦‚æœä¸å†è°ƒç”¨å·¥å…·ï¼Œæå–æœ€ç»ˆç­”æ¡ˆå¹¶å‘é€ç»™å‰ç«¯
                    if not (hasattr(response.choices[0].message, 'tool_calls') and
                            response.choices[0].message.tool_calls):
                        full_response = response.choices[0].message.content or ""

                        # âœ… æ¨¡æ‹Ÿæµå¼å‘é€
                        if stream_callback and full_response:
                            chunk_size = 20
                            for i in range(0, len(full_response), chunk_size):
                                chunk = full_response[i:i + chunk_size]
                                stream_callback(self.name, chunk)
                                time.sleep(0.02)

                        print(f"   ğŸ’¬ [{self.name}] å·¥å…·è°ƒç”¨å®Œæˆï¼Œç”Ÿæˆæœ€ç»ˆç­”æ¡ˆ")
                        if log_callback:
                            log_callback(f"[{self.name}] å·¥å…·è°ƒç”¨å®Œæˆ")
                        break

                # ğŸ”¥ğŸ”¥ğŸ”¥ æ ¸å¿ƒä¿®å¤ï¼šå·¥å…·è°ƒç”¨è¶…é™åçš„å…œåº•é€»è¾‘ ğŸ”¥ğŸ”¥ğŸ”¥
                if iteration >= max_tool_iterations:
                    print(f"   âš ï¸ [{self.name}] å·¥å…·è°ƒç”¨è¾¾åˆ°ä¸Šé™ ({max_tool_iterations} è½®)")

                    # ğŸ”¥ æ£€æŸ¥æ˜¯å¦æœ‰æœ‰æ•ˆå›å¤
                    if not full_response or not full_response.strip():
                        # ç”Ÿæˆå‹å¥½å…œåº•æ¶ˆæ¯
                        unique_tools = list(set(tool_call_history))
                        full_response = (
                            f"âš ï¸ å·¥å…·è°ƒç”¨è¾¾åˆ°ä¸Šé™ï¼ˆ{max_tool_iterations}æ¬¡ï¼‰ï¼Œå·²æ”¶é›†éƒ¨åˆ†ä¿¡æ¯ã€‚\n\n"
                            f"**å·²è°ƒç”¨å·¥å…·**: {', '.join(unique_tools)}\n\n"
                            "**å»ºè®®æ“ä½œ**ï¼š\n"
                            "1. å°è¯•ç®€åŒ–é—®é¢˜æˆ–åˆ†æ­¥æé—®\n"
                            "2. æ£€æŸ¥é™„ä»¶å¤§å°ï¼ˆå»ºè®®<5MBï¼‰\n"
                            "3. å¦‚éœ€å®Œæ•´åˆ†æï¼Œè¯·æ˜ç¡®æŒ‡å®šåˆ†æèŒƒå›´\n\n"
                            "ğŸ’¡ **æç¤º**: å½“å‰ä»»åŠ¡å¯èƒ½è¿‡äºå¤æ‚ï¼Œå»ºè®®å°†å…¶æ‹†åˆ†ä¸ºå¤šä¸ªå°ä»»åŠ¡åˆ†åˆ«å¤„ç†ã€‚"
                        )

                        print(f"   âš ï¸ æœªç”Ÿæˆæœ‰æ•ˆå›å¤ï¼Œå·²ä½¿ç”¨å…œåº•æ¶ˆæ¯")
                        if log_callback:
                            log_callback(f"[{self.name}] âš ï¸ å·¥å…·è¶…é™ï¼Œä½¿ç”¨å…œåº•å›å¤")

                    # âœ… æ–°å¢ï¼šå…œåº•æ¶ˆæ¯ä¸€æ¬¡æ€§å‘é€ï¼ˆé¿å…ç©ºç™½åˆ†å—ï¼‰
                    if stream_callback and full_response:
                        stream_callback(self.name, full_response)  # â† ä¸€æ¬¡æ€§å‘é€å®Œæ•´æ¶ˆæ¯

                    if log_callback:
                        log_callback(f"[{self.name}] å·¥å…·è°ƒç”¨è¶…é™")

            # ===== è®¡ç®—å¹¶æ˜¾ç¤ºè€—æ—¶ =====
            elapsed = time.time() - start_time
            elapsed_str = f"{elapsed:.2f}ç§’" if elapsed < 60 else f"{int(elapsed // 60)}åˆ†{elapsed % 60:.1f}ç§’"

            if not use_stream:
                print(f"â±ï¸  ã€{self.name}ã€‘å“åº”å®Œæˆ | è€—æ—¶: {elapsed_str}")

            if log_callback:
                log_callback(f"[{self.name}] å“åº”å®Œæˆ (è€—æ—¶ {elapsed_str})")

            logging.info(f"â±ï¸  {self.name} å“åº”è€—æ—¶: {elapsed_str}")

            # ğŸ”¥ æœ€ç»ˆå…œåº•æ£€æŸ¥ï¼ˆé˜²æ­¢æ‰€æœ‰æƒ…å†µæ¼ç½‘ï¼‰
            if not full_response or not full_response.strip():
                full_response = (
                    f"âš ï¸ [{self.name}] æœªèƒ½ç”Ÿæˆæœ‰æ•ˆå›å¤ã€‚\n\n"
                    "å¯èƒ½åŸå› ï¼š\n"
                    "- å·¥å…·è°ƒç”¨è¶…é™æˆ–å¤±è´¥\n"
                    "- ç½‘ç»œå¼‚å¸¸æˆ–æ¨¡å‹è¶…æ—¶\n"
                    "- è¾“å…¥å†…å®¹æ— æ³•å¤„ç†\n\n"
                    "**å»ºè®®æ“ä½œ**ï¼š\n"
                    "1. æ£€æŸ¥è¾“å…¥å†…å®¹æ˜¯å¦å®Œæ•´\n"
                    "2. ç®€åŒ–é—®é¢˜åé‡è¯•\n"
                    "3. è”ç³»æŠ€æœ¯æ”¯æŒ"
                )
                logging.warning(f"âš ï¸ {self.name} æœªç”Ÿæˆæœ‰æ•ˆå›å¤ï¼Œä½¿ç”¨å…œåº•æ¶ˆæ¯")

            return full_response.strip()

        except Exception as e:
            elapsed = time.time() - start_time
            err = f"[Error in {self.name}]: {str(e)}"
            logging.error(f"{err} | è€—æ—¶: {elapsed:.2f}ç§’")
            print(f"âŒ ã€{self.name}ã€‘æ‰§è¡Œå¤±è´¥ | è€—æ—¶: {elapsed:.2f}ç§’")

            if log_callback:
                log_callback(f"[{self.name}] âŒ æ‰§è¡Œå¤±è´¥: {str(e)[:50]}")

            # ğŸ”¥ å¼‚å¸¸æƒ…å†µä¹Ÿè¿”å›å‹å¥½æ¶ˆæ¯
            return (
                f"âŒ [{self.name}] æ‰§è¡Œå¤±è´¥\n\n"
                f"**é”™è¯¯ä¿¡æ¯**: {str(e)[:200]}\n\n"
                "**å»ºè®®æ“ä½œ**ï¼š\n"
                "1. æ£€æŸ¥ç½‘ç»œè¿æ¥\n"
                "2. ç¡®è®¤ API é…ç½®æ­£ç¡®\n"
                "3. ç¨åé‡è¯•æˆ–è”ç³»ç®¡ç†å‘˜"
            )


# ====================== ä¸»ç±» MultiAgentSwarm ======================
class MultiAgentSwarm:
    """å¤šæ™ºèƒ½ä½“ç¾¤æ™ºæ…§æ¡†æ¶ v3.0.0"""

    def __init__(self, config_path: str = "swarm_config.yaml"):
        if not os.path.exists(config_path):
            raise FileNotFoundError(f"âŒ é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_path}")

        with open(config_path, "r", encoding="utf-8") as f:
            cfg = yaml.safe_load(f)

        # OpenAI é…ç½®
        oai = cfg.get("openai", {})
        self.default_model = oai.get("default_model", "gpt-4o-mini")
        self.default_max_tokens = oai.get("default_max_tokens", 4096)
        self.context_limit_k = oai.get("context_limit_k", "64")

        # Swarm é…ç½®
        swarm = cfg.get("swarm", {})
        self.mode = swarm.get("mode", "fixed")
        # self.max_rounds = swarm.get("max_rounds", 3 if self.mode == "fixed" else 10)
        # æ ¹æ®ä¸Šä¸‹æ–‡é™åˆ¶åŠ¨æ€è½®æ¬¡ï¼ˆ64Kæ—¶ä¿å®ˆï¼Œ128Kæ—¶æ¿€è¿›ï¼‰
        self.max_rounds = 8 if "64" in str(self.context_limit_k) else 12  # æˆ–è€…è¯»é…ç½®
        self.max_concurrent_agents = swarm.get("max_concurrent_agents", 2)
        self.reflection_planning = swarm.get("reflection_planning", True)
        self.enable_web_search = swarm.get("enable_web_search", False)
        self.max_images = swarm.get("max_images", 2)

        self.log_file = swarm.get("log_file", "swarm.log")
        self.skills_dir = swarm.get("skills_dir", "skills")
        self.memory_file = swarm.get("memory_file", "memory.json")
        self.max_memory_items = swarm.get("max_memory_items", 50)

        # âœ¨ æ–°å¢å¢å¼ºé…ç½®
        advanced = cfg.get("advanced_features", {})
        self.enable_adversarial_debate = advanced.get("adversarial_debate", {}).get("enabled", True)
        self.enable_meta_critic = advanced.get("meta_critic", {}).get("enabled", True)
        self.enable_task_decomposition = advanced.get("task_decomposition", {}).get("enabled", True)
        self.enable_knowledge_graph = advanced.get("knowledge_graph", {}).get("enabled", True)
        self.enable_adaptive_depth = advanced.get("adaptive_reflection", {}).get("enabled", True)

        self.max_reflection_rounds = advanced.get("adaptive_reflection", {}).get("max_rounds", 3)
        self.reflection_quality_threshold = advanced.get("adaptive_reflection", {}).get("quality_threshold", 85)
        self.stop_quality_threshold = advanced.get("adaptive_reflection", {}).get("stop_threshold", 80)
        self.quality_convergence_delta = advanced.get("adaptive_reflection", {}).get("convergence_delta", 3)

        # âœ¨âœ¨âœ¨ æ™ºèƒ½è·¯ç”±é…ç½®ï¼ˆæ–°å¢ï¼‰âœ¨âœ¨âœ¨
        routing_cfg = cfg.get("intelligent_routing", {})
        self.intelligent_routing_enabled = routing_cfg.get("enabled", True)
        self.force_complexity = routing_cfg.get("force_complexity", None)  # "simple"/"medium"/"complex"/None

        # å‘é‡è®°å¿†é…ç½®
        vector_cfg = swarm.get("vector_memory", {})
        self.vector_memory_enabled = vector_cfg.get("enabled", False)
        self.vector_persist_directory = vector_cfg.get("persist_directory", "./memory_db")
        self.vector_model_cache_dir = vector_cfg.get("model_cache_dir", "./cached_model/")
        self.vector_embedding_model = vector_cfg.get("embedding_model",
                                                     "sentence-transformers/distiluse-base-multilingual-cased-v2")

        # æ—¥å¿—é…ç½®
        logging.basicConfig(
            filename=self.log_file,
            level=logging.INFO,
            format="%(asctime)s | %(levelname)s | %(message)s",
            encoding="utf-8",
            force=True
        )
        logging.getLogger().addHandler(logging.StreamHandler())

        # æ‰“å°å¯åŠ¨ä¿¡æ¯
        self._print_startup_banner()

        # åŠ è½½ Skills
        self.tool_registry, self.shared_knowledge = load_skills(self.skills_dir)

        # ====================== ã€æ–°å¢ã€‘æ”¯æŒ YAML ä¸­çš„ shared_knowledge ======================
        yaml_shared = cfg.get("shared_knowledge", "") or ""
        if yaml_shared.strip():
            self.shared_knowledge = (yaml_shared.strip() + "\n\n" + self.shared_knowledge).strip()
            logging.info(f"âœ… å·²åˆå¹¶ YAML shared_knowledgeï¼ˆ{len(yaml_shared)} å­—ç¬¦ï¼‰")
            print(f"âœ… å·²åŠ è½½ YAML å…¨å±€çŸ¥è¯†ï¼ˆ{len(yaml_shared)} å­—ç¬¦ï¼‰")

        # æ·»åŠ å†…ç½®ç½‘ç»œæœç´¢å·¥å…·
        if self.enable_web_search:
            self.tool_registry["web_search"] = {
                "func": web_search,
                "schema": {
                    "type": "function",
                    "function": {
                        "name": "web_search",
                        "description": "å®æ—¶ç½‘é¡µæœç´¢æœ€æ–°ä¿¡æ¯ï¼ˆDuckDuckGoï¼‰",
                        "parameters": {
                            "type": "object",
                            "properties": {
                                "query": {"type": "string", "description": "æœç´¢å…³é”®è¯"},
                                "num_results": {"type": "integer", "description": "è¿”å›ç»“æœæ•°é‡", "default": 5}
                            },
                            "required": ["query"]
                        }
                    }
                }
            }
            logging.info("âœ… å·²å¯ç”¨ç½‘ç»œæœç´¢å·¥å…·")

        # åˆå§‹åŒ–æŒä¹…åŒ–è®°å¿†
        self.memory = self._load_memory()

        # åˆå§‹åŒ–å‘é‡è®°å¿†
        self.vector_memory = None
        if self.vector_memory_enabled:
            try:
                self.vector_memory = VectorMemory(
                    persist_directory=self.vector_persist_directory,
                    model_name=self.vector_embedding_model,
                    cache_dir=self.vector_model_cache_dir
                )
                logging.info("âœ… å‘é‡è®°å¿†ç³»ç»Ÿåˆå§‹åŒ–æˆåŠŸ")
            except Exception as e:
                logging.warning(f"âš ï¸ å‘é‡è®°å¿†åˆå§‹åŒ–å¤±è´¥: {e}")
                self.vector_memory_enabled = False

        # âœ¨ PrimalClawæç®€è®°å¿†ç³»ç»Ÿï¼ˆæœ€å°æ”¹åŠ¨æ ¸å¿ƒï¼‰
        self.primal_memory = PrimalMemory(
            base_dir="./memory",
            vector_memory=self.vector_memory  # â† ç›´æ¥ä¼ å…¥ï¼Œæ›´ä¼˜é›…
        )
        logging.info("âœ… PrimalMemory (æ ‘çŠ¶æ—¥å¿—+åŸå­KB+è¡°é€€) åˆå§‹åŒ–æˆåŠŸ")

        # âœ¨ åˆå§‹åŒ–çŸ¥è¯†å›¾è°±
        self.knowledge_graph = None
        if self.enable_knowledge_graph:
            self.knowledge_graph = KnowledgeGraph(enable_distillation=True)
            logging.info("âœ… çŸ¥è¯†å›¾è°±ç³»ç»Ÿåˆå§‹åŒ–æˆåŠŸ")

        # åˆå§‹åŒ– Agents
        self.agents = []
        for a_cfg in cfg.get("agents", [])[:swarm.get("num_agents", 4)]:
            agent = Agent(
                a_cfg,
                self.default_model,
                self.default_max_tokens,
                self.tool_registry,
                self.shared_knowledge,
                self.vector_memory,
                self.knowledge_graph,
                context_limit_k=self.context_limit_k
            )
            self.agents.append(agent)
            logging.info(f"âœ… Agent åŠ è½½: {agent.name} | Model: {agent.model}")

        if not self.agents:
            raise ValueError("âŒ è‡³å°‘éœ€è¦é…ç½®ä¸€ä¸ª Agent")

        # âœ¨ æ–°å¢ï¼šå–æ¶ˆæ ‡å¿—ï¼ˆçº¿ç¨‹å®‰å…¨ï¼‰
        self._cancel_flag = threading.Event()
        self._cancel_lock = threading.Lock()

        self.leader = self.agents[0]
        logging.info(f"ğŸ‘‘ Leader: {self.leader.name}")

    def cancel_current_task(self):
        """å–æ¶ˆå½“å‰æ­£åœ¨æ‰§è¡Œçš„ä»»åŠ¡ï¼ˆçº¿ç¨‹å®‰å…¨ï¼‰"""
        with self._cancel_lock:
            self._cancel_flag.set()
            logging.info("ğŸ›‘ æ”¶åˆ°å–æ¶ˆè¯·æ±‚")

    def _reset_cancel_flag(self):
        """é‡ç½®å–æ¶ˆæ ‡å¿—ï¼ˆæ–°ä»»åŠ¡å¼€å§‹æ—¶è°ƒç”¨ï¼‰"""
        with self._cancel_lock:
            self._cancel_flag.clear()

    def _check_cancellation(self) -> bool:
        """æ£€æŸ¥æ˜¯å¦è¢«å–æ¶ˆ"""
        return self._cancel_flag.is_set()

    def _generate_detailed_plan(self, task: str, history: List[Dict]) -> str:
        """ã€æœ€å°æ”¹åŠ¨æ ¸å¿ƒã€‘ç”Ÿæˆç»“æ„åŒ–Master Plan"""
        plan_prompt = (
            f"ä»»åŠ¡ï¼š{task}\n\n"  # â† ä¿æŒåŸæ ·
            "è¯·ç«‹å³åˆ¶å®šä¸€ä¸ª**æ¸…æ™°ã€å¯æ‰§è¡Œã€é˜¶æ®µæ€§**çš„Master Planï¼ˆç”¨ä¸­æ–‡ç¼–å·åˆ—è¡¨ï¼‰ï¼š\n"
            "1. ä»»åŠ¡åˆ†è§£ä¸º3-5ä¸ªä¸»è¦é˜¶æ®µï¼ˆPhaseï¼‰\n"
            "2. æ¯ä¸ªé˜¶æ®µï¼šç›®æ ‡ + ä¸»è¦è´Ÿè´£Agent + é¢„æœŸè¾“å‡º\n"
            "3. å…³é”®æ£€æŸ¥ç‚¹ï¼ˆquality gateï¼‰å’ŒæˆåŠŸæŒ‡æ ‡\n"
            "4. æ½œåœ¨é£é™©åŠåº”å¯¹ï¼ˆBenjaminç‰¹åˆ«å…³æ³¨é€»è¾‘æ¼æ´ï¼‰\n"
            f"5. æ€»è½®æ¬¡æ§åˆ¶å»ºè®®ï¼ˆä¸è¶…è¿‡{self.max_rounds}è½®ï¼‰\n\n"  # â† åªæ”¹è¿™ä¸€è¡Œï¼ŒåŠ  f å’Œ self.
            "è¦æ±‚ï¼šæç®€æ¸…æ™°ã€å¯ç›´æ¥ä½œä¸ºSystem Promptä½¿ç”¨ã€‚ä¸è¦å¤šä½™åºŸè¯ã€‚"
        )
        try:
            plan_response = self.leader.generate_response(
                [{"speaker": "System", "content": plan_prompt}],
                0,
                force_non_stream=True
            )
            return f"ğŸ“‹ ã€Master Planã€‘\n{plan_response}"
        except Exception as e:
            logging.warning(f"Planç”Ÿæˆå¤±è´¥: {e}")
            return ""

    def _print_startup_banner(self):
        """æ‰“å°å¯åŠ¨æ¨ªå¹…"""
        banner = f"""
    {'=' * 80}
    ğŸš€ MultiAgentSwarm v3.1.0 åˆå§‹åŒ–ï¼ˆæ™ºèƒ½è·¯ç”±ç‰ˆï¼‰
    {'=' * 80}
    ğŸ“Š åŸºç¡€é…ç½®:
       Mode: {self.mode} | Max Rounds: {self.max_rounds}
       Max Concurrent: {self.max_concurrent_agents}
       Reflection: {self.reflection_planning} | Web Search: {self.enable_web_search}
       Vector Memory: {self.vector_memory_enabled}

    âœ¨ å¢å¼ºåŠŸèƒ½:
       ğŸ¥Š å¯¹æŠ—è¾©è®º (Adversarial Debate): {'âœ… å¯ç”¨' if self.enable_adversarial_debate else 'âŒ ç¦ç”¨'}
       ğŸ¯ å…ƒæ‰¹è¯„ (Meta-Critic): {'âœ… å¯ç”¨' if self.enable_meta_critic else 'âŒ ç¦ç”¨'}
       ğŸ­ ä»»åŠ¡åˆ†è§£ (Task Decomposition): {'âœ… å¯ç”¨' if self.enable_task_decomposition else 'âŒ ç¦ç”¨'}
       ğŸ§  çŸ¥è¯†å›¾è°± (Knowledge Graph): {'âœ… å¯ç”¨' if self.enable_knowledge_graph else 'âŒ ç¦ç”¨'}
       ğŸ“ˆ è‡ªé€‚åº”åæ€ (Adaptive Depth): {'âœ… å¯ç”¨' if self.enable_adaptive_depth else 'âŒ ç¦ç”¨'}
       ğŸ§­ æ™ºèƒ½è·¯ç”± (Intelligent Routing): {'âœ… å¯ç”¨' if self.intelligent_routing_enabled else 'âŒ ç¦ç”¨'}
          â””â”€ å¼ºåˆ¶æ¨¡å¼: {self.force_complexity or 'è‡ªåŠ¨åˆ¤æ–­'}
    {'=' * 80}
    """
        print(banner)
        logging.info(banner)

    def _load_memory(self) -> Dict:
        """åŠ è½½æŒä¹…åŒ–è®°å¿†"""
        if os.path.exists(self.memory_file):
            try:
                with open(self.memory_file, "r", encoding="utf-8") as f:
                    memory = json.load(f)
                logging.info(f"ğŸ“– åŠ è½½è®°å¿†æ–‡ä»¶: {self.memory_file} ({len(memory)} keys)")
                return memory
            except Exception as e:
                logging.error(f"åŠ è½½è®°å¿†å¤±è´¥: {e}")
        return {}

    def _save_memory(self, key: str, summary: str):
        """ä¿å­˜è®°å¿†åˆ°æŒä¹…åŒ–æ–‡ä»¶"""
        if key not in self.memory:
            self.memory[key] = []

        self.memory[key].append({
            "timestamp": datetime.now().isoformat(),
            "summary": summary[:3000]
        })

        if len(self.memory[key]) > self.max_memory_items:
            self.memory[key] = self.memory[key][-self.max_memory_items:]

        try:
            with open(self.memory_file, "w", encoding="utf-8") as f:
                json.dump(self.memory, f, ensure_ascii=False, indent=2)
            logging.info(f"ğŸ’¾ ä¿å­˜è®°å¿†: {key}")
        except Exception as e:
            logging.error(f"ä¿å­˜è®°å¿†å¤±è´¥: {e}")

    def _decompose_task(self, task: str) -> str:
        """
        âœ¨ ä»»åŠ¡åˆ†è§£å™¨
        å°†å¤æ‚ä»»åŠ¡åˆ†è§£ä¸ºå­ä»»åŠ¡å¹¶åˆ†é…ç»™æœ€é€‚åˆçš„ Agent
        """
        if not self.enable_task_decomposition or len(self.agents) <= 1:
            return ""

        logging.info("ğŸ­ å¯åŠ¨ä»»åŠ¡åˆ†è§£...")

        decompose_prompt = (
            f"è¯·å°†ä»¥ä¸‹ä»»åŠ¡åˆ†è§£ä¸º {min(len(self.agents) + 2, 7)} ä¸ªå¯å¹¶è¡Œæˆ–é¡ºåºæ‰§è¡Œçš„å­ä»»åŠ¡ã€‚\n"
            f"ä»»åŠ¡: {task}\n\n"
            f"å¯ç”¨ Agent åŠå…¶ä¸“é•¿ï¼š\n"
            + "\n".join([f"- {a.name}: {a.role[:100]}" for a in self.agents])
            + "\n\næ ¼å¼è¦æ±‚ï¼š\n"
              "```json\n"
              '{"subtasks": [{"id": 1, "description": "å­ä»»åŠ¡æè¿°", "assigned_agent": "Agentåç§°", "priority": "high/medium/low"}]}\n'
              "```"
        )

        try:
            decomposition = self.leader.generate_response(
                [{"speaker": "System", "content": decompose_prompt}],
                0,
                force_non_stream=True
            )

            logging.info(f"ğŸ“‹ ä»»åŠ¡åˆ†è§£ç»“æœ:\n{decomposition[:500]}...")
            return f"ğŸ“‹ ä»»åŠ¡åˆ†è§£:\n{decomposition}"

        except Exception as e:
            logging.error(f"ä»»åŠ¡åˆ†è§£å¤±è´¥: {e}")
            return ""

    def _adversarial_debate(self, history: List[Dict], round_num: int) -> Tuple[int, str]:
        """
        âœ¨ å¯¹æŠ—å¼è¾©è®ºæœºåˆ¶
        ä¸‰è§’è‰²å¹¶è¡Œè¾©è®ºï¼šProï¼ˆå»ºè®¾è€…ï¼‰ã€Conï¼ˆæ‰¹åˆ¤è€…ï¼‰ã€Judgeï¼ˆè£åˆ¤ï¼‰
        è¿”å›ï¼š(è´¨é‡åˆ†æ•° 0-100, å†³ç­– "continue"/"stop")
        """
        if not self.enable_adversarial_debate:
            return 50, "continue"

        logging.info(f"\n{'â”€' * 80}")
        logging.info(f"ğŸ¥Š å¯åŠ¨å¯¹æŠ—å¼è¾©è®º (ç¬¬ {round_num} è½®)")
        logging.info(f"{'â”€' * 80}")

        # è§’è‰²åˆ†é…ï¼ˆåˆ©ç”¨ç°æœ‰ Agentï¼‰
        debate_agents = {
            "Pro": self.agents[1] if len(self.agents) > 1 else self.leader,  # Harper - åˆ›æ„å»ºè®¾è€…
            "Con": self.agents[2] if len(self.agents) > 2 else self.leader,  # Benjamin - ä¸¥æ ¼æ‰¹åˆ¤è€…
            "Judge": self.leader  # Grok - ç»¼åˆè£åˆ¤
        }

        # è¾©è®ºæç¤ºè¯
        reflection_prompts = {
            "Pro": (
                "ğŸŸ¢ ä½ æ˜¯ä¹è§‚å»ºè®¾è€…ï¼Œè¯·ï¼š\n"
                "1. æŒ‡å‡ºæœ¬è½®è®¨è®ºçš„ 3 ä¸ªæœ€å¤§äº®ç‚¹\n"
                "2. æä¾› 2-3 ä¸ªå»ºè®¾æ€§æ”¹è¿›å»ºè®®\n"
                "3. ç»™å‡ºè´¨é‡è¯„åˆ†ï¼ˆ0-100ï¼‰"
            ),
            "Con": (
                "ğŸ”´ ä½ æ˜¯ä¸¥æ ¼æ‰¹åˆ¤è€…ï¼Œè¯·ï¼š\n"
                "1. æ‰¾å‡ºè‡³å°‘ 3 ä¸ªé€»è¾‘æ¼æ´ã€äº‹å®é£é™©æˆ–é—æ¼ç‚¹\n"
                "2. æŒ‡å‡ºå¯èƒ½å¯¼è‡´é”™è¯¯ç»“è®ºçš„å‡è®¾\n"
                "3. ç»™å‡ºé£é™©è¯„åˆ†ï¼ˆ0-100ï¼Œè¶Šé«˜é£é™©è¶Šå¤§ï¼‰"
            ),
            "Judge": (
                "âš–ï¸ ä½ æ˜¯æœ€ç»ˆè£åˆ¤ï¼Œè¯·ï¼š\n"
                "1. ç»¼åˆ Pro å’Œ Con çš„è§‚ç‚¹\n"
                "2. ç»™å‡ºç»¼åˆè´¨é‡è¯„åˆ†ï¼ˆ0-100ï¼‰\n"
                "3. å†³ç­–æ˜¯å¦ç»§ç»­è®¨è®ºï¼ˆcontinue/stopï¼‰\n"
                "æ ¼å¼ï¼š```json\n{\"quality_score\": 0-100, \"decision\": \"continue/stop\", \"reason\": \"åŸå› \"}\n```"
            )
        }

        # å¹¶è¡Œæ‰§è¡Œè¾©è®º
        reflections = {}
        with ThreadPoolExecutor(max_workers=3) as executor:
            future_to_role = {
                executor.submit(
                    agent.generate_response,
                    history.copy(),
                    round_num,
                    system_extra=reflection_prompts[role],
                    critique_previous=True,  # å¼ºåˆ¶æ‰¹åˆ¤æ¨¡å¼
                    force_non_stream=True
                ): role
                for role, agent in debate_agents.items()
            }

            for future in as_completed(future_to_role):
                role = future_to_role[future]
                try:
                    reflections[role] = future.result()
                    logging.info(f"âœ… {role} å®Œæˆè¾©è®º")
                except Exception as e:
                    logging.error(f"âŒ {role} è¾©è®ºå¤±è´¥: {e}")
                    reflections[role] = f"[æ‰§è¡Œå¤±è´¥: {str(e)}]"

        # Meta-Critic ç»¼åˆè¯„ä¼°
        if self.enable_meta_critic:
            synthesis_prompt = (
                f"ğŸ¯ Meta-Critic ç»¼åˆè¯„ä¼°\n\n"
                f"Pro è§‚ç‚¹:\n{reflections.get('Pro', 'N/A')[:800]}\n\n"
                f"Con è§‚ç‚¹:\n{reflections.get('Con', 'N/A')[:800]}\n\n"
                f"Judge è§‚ç‚¹:\n{reflections.get('Judge', 'N/A')[:800]}\n\n"
                f"è¯·ç»¼åˆä¸‰æ–¹è¾©è®ºï¼Œç»™å‡ºæœ€ç»ˆå†³ç­–ï¼ˆJSON æ ¼å¼ï¼‰ï¼š\n"
                f"```json\n"
                f'{{"quality_score": 0-100, "decision": "continue/stop", "reason": "ç»¼åˆåŸå› ", "key_issues": ["é—®é¢˜1", "é—®é¢˜2"]}}\n'
                f"```"
            )

            final_eval = self.leader.generate_response(
                history + [{"speaker": "System", "content": synthesis_prompt}],
                round_num,
                force_non_stream=True
            )
        else:
            final_eval = reflections.get("Judge", "{}")

        # è§£æå†³ç­–
        try:
            eval_json = json.loads(
                final_eval.strip()
                .replace("```json", "")
                .replace("```", "")
                .strip()
            )

            quality_score = eval_json.get("quality_score", 50)
            decision = eval_json.get("decision", "continue").lower()

            logging.info(f"ğŸ“Š è¾©è®ºç»“æœ: è´¨é‡åˆ†æ•° {quality_score}/100 | å†³ç­–: {decision}")
            logging.info(f"ğŸ’¡ åŸå› : {eval_json.get('reason', 'N/A')[:200]}")

            return quality_score, decision

        except (json.JSONDecodeError, Exception) as e:
            logging.error(f"âŒ è§£æè¾©è®ºç»“æœå¤±è´¥: {e}")
            return 50, "continue"

    def solve(
            self,
            task: str,
            use_memory: bool = False,
            memory_key: str = "default",
            image_paths: Optional[List[str]] = None,
            force_complexity: Optional[str] = None,
            stream_callback=None,
            log_callback=None
    ) -> str:
        """
        è§£å†³ä»»åŠ¡çš„ä¸»å…¥å£ï¼ˆæ™ºèƒ½è·¯ç”±ç‰ˆ v3.1.0 + å–æ¶ˆæ”¯æŒï¼‰

        Args:
            task: ä»»åŠ¡æè¿°
            use_memory: æ˜¯å¦ä½¿ç”¨æŒä¹…åŒ–è®°å¿†
            memory_key: è®°å¿†é”®å
            image_paths: å›¾ç‰‡è·¯å¾„åˆ—è¡¨ï¼ˆæœ€å¤š max_images å¼ ï¼‰
            force_complexity: å¼ºåˆ¶æŒ‡å®šå¤æ‚åº¦ "simple"/"medium"/"complex"ï¼ˆä¼˜å…ˆçº§æœ€é«˜ï¼Œç”¨äºè°ƒè¯•ï¼‰
            stream_callback: æµå¼è¾“å‡ºå›è°ƒ func(agent_name, content)
            log_callback: æ—¥å¿—å›è°ƒ func(message)

        Returns:
            æœ€ç»ˆç­”æ¡ˆå­—ç¬¦ä¸²
        """
        # âœ… ä»»åŠ¡å¼€å§‹æ—¶é‡ç½®å–æ¶ˆæ ‡å¿—
        self._reset_cancel_flag()

        tracker = TimeTracker()
        tracker.start()
        logging.info(f"\n{'=' * 80}")
        logging.info(f"ğŸ“‹ æ–°ä»»åŠ¡: {task[:100]}{'...' if len(task) > 100 else ''}")

        # âœ… æ£€æŸ¥ç‚¹ 0ï¼šä»»åŠ¡å¼€å§‹å‰
        if self._check_cancellation():
            return "â¸ï¸ ä»»åŠ¡å·²è¢«ç”¨æˆ·å–æ¶ˆ"

        # ğŸ”¥ã€æ ¸å¿ƒä¿®å¤ã€‘ä»… 12 è¡Œï¼Œå½»åº•è§£å†³å†å²æ±¡æŸ“
        classification_task = task
        if isinstance(task, str) and "=== ğŸ’¬ å½“å‰é—®é¢˜ ===" in task:
            try:
                classification_task = task.split("=== ğŸ’¬ å½“å‰é—®é¢˜ ===")[-1].strip()
                if classification_task.startswith("User:") or classification_task.startswith("Userï¼š"):
                    classification_task = classification_task.split(":", 1)[-1].strip()
                classification_task = classification_task[:300]
            except Exception:
                classification_task = task[:200]

        logging.info(f"ğŸ“Š åˆ†ç±»ä½¿ç”¨çº¯æŸ¥è¯¢: {classification_task[:80]}{'...' if len(classification_task) > 80 else ''}")

        try:
            # âœ… æ£€æŸ¥ç‚¹ 1ï¼šåˆ†ç±»å‰æ£€æŸ¥
            if self._check_cancellation():
                return "â¸ï¸ ä»»åŠ¡å·²è¢«ç”¨æˆ·å–æ¶ˆ"

            if self.intelligent_routing_enabled:
                complexity = (force_complexity or
                              self.force_complexity or
                              self._classify_task_complexity(classification_task))
            else:
                complexity = "complex"

            tracker.checkpoint("1ï¸âƒ£ ä»»åŠ¡åˆ†ç±»")

            if log_callback:
                log_callback(f"ğŸ“Š ä»»åŠ¡å¤æ‚åº¦: {complexity.upper()}")

        except Exception as e:
            logging.error(f"âŒ ä»»åŠ¡åˆ†ç±»å¤±è´¥: {e}ï¼Œå›é€€åˆ°å®Œæ•´æ¨¡å¼")
            if log_callback:
                log_callback(f"âš ï¸ ä»»åŠ¡åˆ†ç±»å¤±è´¥ï¼Œä½¿ç”¨å®Œæ•´æ¨¡å¼")
            complexity = "complex"

        # å¤„ç†å›¾åƒ
        if image_paths:
            image_paths = image_paths[:self.max_images]
            logging.info(f"ğŸ“· å¤„ç† {len(image_paths)} å¼ å›¾ç‰‡")
            if log_callback:
                log_callback(f"ğŸ“· å¤„ç† {len(image_paths)} å¼ å›¾ç‰‡")

        history: List[Dict] = []

        # æ„å»ºåˆå§‹ history
        if image_paths:
            image_content = [{"type": "text", "text": task}]
            for idx, path in enumerate(image_paths, 1):
                if not os.path.exists(path):
                    logging.warning(f"âš ï¸ å›¾ç‰‡ä¸å­˜åœ¨: {path}")
                    continue
                try:
                    mime_type, _ = mimetypes.guess_type(path)
                    if not mime_type or not mime_type.startswith("image/"):
                        mime_type = "image/jpeg"
                    with open(path, "rb") as f:
                        base64_image = base64.b64encode(f.read()).decode('utf-8')
                    image_content.append({
                        "type": "image_url",
                        "image_url": {"url": f"data:{mime_type};base64,{base64_image}"}
                    })
                except Exception as e:
                    logging.error(f"  âŒ è¯»å–å›¾ç‰‡å¤±è´¥ {path}: {e}")
            history.append({"speaker": "User", "content": image_content})
        else:
            history.append({"speaker": "User", "content": task})

        # âœ¨ PrimalClawï¼šæ³¨å…¥ç›¸å…³å†å²è®°å¿†ï¼ˆQMDæ£€ç´¢ï¼‰
        if use_memory and hasattr(self, 'primal_memory'):
            relevant = self.primal_memory.get_relevant_memory(task)
            if relevant:
                history.insert(0, {"speaker": "System", "content": f"ğŸ“š Primalè®°å¿†ï¼ˆç›¸å…³ç»éªŒï¼‰ï¼š\n{relevant}"})
                if log_callback:
                    log_callback("ğŸ“š å·²æ³¨å…¥Primalç›¸å…³è®°å¿†")

        # âœ… æ£€æŸ¥ç‚¹ 2ï¼šæ‰§è¡Œå‰æ£€æŸ¥
        if self._check_cancellation():
            return "â¸ï¸ ä»»åŠ¡å·²è¢«ç”¨æˆ·å–æ¶ˆ"

        # âœ¨âœ¨âœ¨ ä¸‰çº§è·¯ç”±æ‰§è¡Œï¼ˆå¸¦å¼‚å¸¸é™çº§ï¼‰âœ¨âœ¨âœ¨
        final_answer = ""
        execution_mode = complexity

        try:
            if complexity == "simple":
                final_answer = self._solve_simple(
                    task, history,
                    stream_callback, log_callback
                )

            elif complexity == "medium":
                final_answer = self._solve_medium(
                    task, history, tracker,
                    stream_callback, log_callback
                )

            else:  # complex
                final_answer = self._solve_complex(
                    task, history, tracker, use_memory, memory_key,
                    stream_callback, log_callback
                )

        except Exception as e:
            # âœ… æ£€æŸ¥æ˜¯å¦å› å–æ¶ˆè€Œå¼‚å¸¸
            if self._check_cancellation():
                return "â¸ï¸ ä»»åŠ¡å·²è¢«ç”¨æˆ·å–æ¶ˆ"

            logging.error(f"âŒ {complexity.upper()} æ¨¡å¼æ‰§è¡Œå¤±è´¥: {e}")
            print(f"\n{'!' * 80}")
            print(f"âš ï¸  {complexity.upper()} æ¨¡å¼æ‰§è¡Œå¤±è´¥: {str(e)[:100]}")
            print(f"ğŸ”„ è‡ªåŠ¨é™çº§åˆ° COMPLEX å®Œæ•´æ¨¡å¼...")
            print(f"{'!' * 80}\n")

            if log_callback:
                log_callback(f"âš ï¸ {complexity.upper()} æ¨¡å¼å¤±è´¥ï¼Œé™çº§åˆ° COMPLEX")

            execution_mode = "complex (é™çº§)"
            try:
                final_answer = self._solve_complex(
                    task, history, tracker, use_memory, memory_key,
                    stream_callback, log_callback
                )
            except Exception as fallback_error:
                if self._check_cancellation():
                    return "â¸ï¸ ä»»åŠ¡å·²è¢«ç”¨æˆ·å–æ¶ˆ"

                logging.error(f"âŒ é™çº§æ‰§è¡Œä¹Ÿå¤±è´¥: {fallback_error}")
                final_answer = f"[ç³»ç»Ÿé”™è¯¯] ä»»åŠ¡æ‰§è¡Œå¤±è´¥:\nåŸå§‹é”™è¯¯: {str(e)}\né™çº§é”™è¯¯: {str(fallback_error)}"
                if log_callback:
                    log_callback(f"âŒ ç³»ç»Ÿé”™è¯¯: ä»»åŠ¡æ‰§è¡Œå¤±è´¥")

        # âœ… æ£€æŸ¥ç‚¹ 3ï¼šæ‰§è¡Œåæ£€æŸ¥
        if self._check_cancellation():
            partial_result = final_answer[:500] + "..." if len(final_answer) > 500 else final_answer
            return f"â¸ï¸ ä»»åŠ¡å·²è¢«å–æ¶ˆ\n\n**éƒ¨åˆ†ç»“æœ**ï¼š\n{partial_result}" if final_answer else "â¸ï¸ ä»»åŠ¡å·²è¢«ç”¨æˆ·å–æ¶ˆ"

        # ===== ç»Ÿä¸€è¾“å‡ºï¼ˆä¸‰ç§æ¨¡å¼å…±ç”¨ï¼‰ =====
        print("\n" + "=" * 100)
        print(f"ğŸ¯ ã€æœ€ç»ˆç­”æ¡ˆã€‘ï¼ˆæ‰§è¡Œæ¨¡å¼: {execution_mode.upper()}ï¼‰")
        print("=" * 100)
        print(final_answer)
        print("=" * 100)

        if log_callback:
            log_callback(f"âœ… ä»»åŠ¡å®Œæˆ (æ¨¡å¼: {execution_mode.upper()})")

        # çŸ¥è¯†å›¾è°±è’¸é¦ï¼ˆä»… complex æ¨¡å¼ï¼‰
        if complexity == "complex" and self.knowledge_graph:
            kg_summary = self.knowledge_graph.distill(max_items=10)
            if kg_summary:
                print("\n" + "â”€" * 100)
                print("ğŸ§  ã€çŸ¥è¯†å›¾è°±è’¸é¦ã€‘")
                print("â”€" * 100)
                print(kg_summary)
                print("â”€" * 100)

        # æ—¶é—´ç»Ÿè®¡
        print(tracker.summary())
        logging.info(tracker.summary())

        return final_answer

    def _solve_complex(
            self,
            task: str,
            history: List[Dict],
            tracker: TimeTracker,
            use_memory: bool,
            memory_key: str,
            stream_callback=None,
            log_callback=None
    ) -> str:
        """
        ğŸ”´ å®Œæ•´æ¨¡å¼ï¼šå…¨åŠŸèƒ½åä½œï¼ˆæ–°å¢å–æ¶ˆæ£€æŸ¥ï¼‰
        """
        logging.info("ğŸ”´ æ‰§è¡Œå®Œæ•´æ¨¡å¼ï¼ˆå…¨åŠŸèƒ½åä½œï¼‰")
        print(f"\n{'=' * 80}")
        print("ğŸ”´ æ£€æµ‹åˆ°å¤æ‚ä»»åŠ¡ï¼Œå¯ç”¨å…¨åŠŸèƒ½åä½œæ¨¡å¼")
        print(f"{'=' * 80}\n")

        if log_callback:
            log_callback("ğŸ”´ æ‰§è¡Œå®Œæ•´æ¨¡å¼ï¼ˆå…¨åŠŸèƒ½åä½œï¼‰")

        # âœ… æ£€æŸ¥ç‚¹ï¼šå¼€å§‹å‰
        if self._check_cancellation():
            return "â¸ï¸ ä»»åŠ¡åœ¨å¼€å§‹å‰è¢«å–æ¶ˆ"

        # ===== ä»»åŠ¡åˆ†è§£ =====
        if self.enable_task_decomposition and self.mode == "intelligent":
            if self._check_cancellation():
                return "â¸ï¸ ä»»åŠ¡åœ¨åˆ†è§£å‰è¢«å–æ¶ˆ"

            decomposition = self._decompose_task(task)
            if decomposition:
                history.insert(0, {"speaker": "System", "content": decomposition})
                tracker.checkpoint("2ï¸âƒ£ ä»»åŠ¡åˆ†è§£")
                if log_callback:
                    log_callback("ğŸ“‹ ä»»åŠ¡åˆ†è§£å®Œæˆ")

        # âœ… æ£€æŸ¥ç‚¹ï¼šMaster Plan ç”Ÿæˆå‰
        if self._check_cancellation():
            return "â¸ï¸ ä»»åŠ¡åœ¨è§„åˆ’å‰è¢«å–æ¶ˆ"

        # ğŸ”¥ã€æ–°å¢ã€‘æ˜¾å¼Master Plan
        plan = self._generate_detailed_plan(task, history)
        if plan:
            history.insert(0, {"speaker": "System", "content": plan})
            if log_callback:
                log_callback("ğŸ“‹ Master Plan å·²ç”Ÿæˆå¹¶æ³¨å…¥")
            tracker.checkpoint("2.5ï¸âƒ£ Master Plan ç”Ÿæˆ")

        # ===== åŠ è½½å†å²è®°å¿† =====
        if use_memory and memory_key in self.memory:
            memory_text = "\n".join([
                f"- {item['summary']}"
                for item in self.memory[memory_key][-5:]
            ])
            history.insert(0, {
                "speaker": "System",
                "content": f"ğŸ“š å†å²è®°å¿†ï¼ˆ{memory_key}ï¼‰ï¼š\n{memory_text}"
            })
            if log_callback:
                log_callback(f"ğŸ“š åŠ è½½å†å²è®°å¿†: {memory_key}")

        # âœ… æ£€æŸ¥ç‚¹ï¼šä¸»å¾ªç¯å‰
        if self._check_cancellation():
            return "â¸ï¸ ä»»åŠ¡åœ¨è®¨è®ºå¼€å§‹å‰è¢«å–æ¶ˆ"

        # ===== ä¸»å¾ªç¯ï¼ˆå¤šè½®è®¨è®º + è¾©è®ºï¼‰=====
        round_num = 0
        previous_quality = 0

        while True:
            # âœ… å…³é”®æ£€æŸ¥ç‚¹ï¼šæ¯è½®å¼€å§‹å‰
            if self._check_cancellation():
                logging.info(f"ğŸ›‘ ç¬¬ {round_num} è½®è¢«å–æ¶ˆ")
                if log_callback:
                    log_callback(f"â¸ï¸ ç¬¬ {round_num} è½®è¢«å–æ¶ˆ")
                break

            round_num += 1
            if round_num > self.max_rounds:
                logging.info(f"â¸ï¸  è¾¾åˆ°æœ€å¤§è½®æ¬¡ {self.max_rounds}ï¼Œåœæ­¢è®¨è®º")
                if log_callback:
                    log_callback(f"â¸ï¸ è¾¾åˆ°æœ€å¤§è½®æ¬¡ {self.max_rounds}")
                break

            logging.info(f"\n{'â”€' * 80}")
            logging.info(f"ğŸ”„ ç¬¬ {round_num} è½®è®¨è®ºå¼€å§‹")
            logging.info(f"{'â”€' * 80}")

            if log_callback:
                log_callback(f"ğŸ”„ ç¬¬ {round_num}/{self.max_rounds} è½®è®¨è®ºå¼€å§‹")

            round_start = time.time()

            # å¹¶å‘æ‰§è¡Œ Agent è®¨è®º
            with ThreadPoolExecutor(max_workers=self.max_concurrent_agents) as executor:
                future_to_agent = {
                    executor.submit(
                        agent.generate_response,
                        history.copy(),
                        round_num,
                        critique_previous=(round_num > 1 and self.enable_adversarial_debate),
                        log_callback=log_callback
                    ): agent
                    for agent in self.agents
                }

                for future in as_completed(future_to_agent):
                    # âœ… æ£€æŸ¥ç‚¹ï¼šæ¯ä¸ª Agent å®Œæˆå
                    if self._check_cancellation():
                        logging.info(f"ğŸ›‘ Agent è®¨è®ºè¢«å–æ¶ˆï¼Œæ­£åœ¨æ¸…ç†...")
                        # å–æ¶ˆæœªå®Œæˆçš„ä»»åŠ¡
                        for f in future_to_agent:
                            f.cancel()
                        break

                    agent = future_to_agent[future]
                    try:
                        contribution = future.result()
                        history.append({
                            "speaker": agent.name,
                            "content": contribution
                        })

                        # çŸ¥è¯†å›¾è°±æå–
                        if self.knowledge_graph and len(contribution) > 50:
                            words = contribution.split()
                            for i, word in enumerate(words[:50]):
                                if word.istitle() and len(word) > 3:
                                    self.knowledge_graph.add_entity(
                                        word, "concept", f"{agent.name}æåŠ"
                                    )
                    except Exception as e:
                        logging.error(f"âŒ {agent.name} æ‰§è¡Œå¤±è´¥: {e}")
                        if log_callback:
                            log_callback(f"âŒ {agent.name} æ‰§è¡Œå¤±è´¥")

            # âœ… æ£€æŸ¥ç‚¹ï¼šæœ¬è½®è®¨è®ºå
            if self._check_cancellation():
                logging.info(f"ğŸ›‘ ç¬¬ {round_num} è½®è®¨è®ºåè¢«å–æ¶ˆ")
                if log_callback:
                    log_callback(f"â¸ï¸ ç¬¬ {round_num} è½®è®¨è®ºè¢«å–æ¶ˆ")
                break

            round_elapsed = time.time() - round_start
            tracker.checkpoint(f"3ï¸âƒ£ ç¬¬{round_num}è½®è®¨è®º ({round_elapsed:.1f}ç§’)")

            # âœ¨ è‡ªåŠ¨å‹ç¼©å†å²
            history = self._compress_history(history)
            tracker.checkpoint(f"3.5ï¸âƒ£ å†å²å‹ç¼©")

            # ===== å¯¹æŠ—å¼è¾©è®ºä¸è‡ªé€‚åº”åæ€ =====
            if self.mode == "intelligent" and self.reflection_planning:
                # âœ… æ£€æŸ¥ç‚¹ï¼šè¾©è®ºå‰
                if self._check_cancellation():
                    logging.info(f"ğŸ›‘ è¾©è®ºå‰è¢«å–æ¶ˆ")
                    if log_callback:
                        log_callback(f"â¸ï¸ è¾©è®ºè¢«å–æ¶ˆ")
                    break

                if log_callback:
                    log_callback(f"ğŸ¥Š å¯åŠ¨å¯¹æŠ—å¼è¾©è®º (ç¬¬ {round_num} è½®)")

                quality_score, decision = self._adversarial_debate(history, round_num)
                tracker.checkpoint(f"4ï¸âƒ£ ç¬¬{round_num}è½®è¾©è®º")

                # âœ… æ£€æŸ¥ç‚¹ï¼šè¾©è®ºå
                if self._check_cancellation():
                    logging.info(f"ğŸ›‘ è¾©è®ºåè¢«å–æ¶ˆ")
                    if log_callback:
                        log_callback(f"â¸ï¸ è¾©è®ºè¢«å–æ¶ˆ")
                    break

                if log_callback:
                    log_callback(f"ğŸ“Š è¾©è®ºç»“æœ: è´¨é‡ {quality_score}/100, å†³ç­– {decision}")

                if self.enable_adaptive_depth:
                    # è´¨é‡è¾¾æ ‡ç«‹å³åœæ­¢
                    if quality_score >= self.reflection_quality_threshold:
                        logging.info(f"âœ… è´¨é‡è¾¾æ ‡ ({quality_score} >= {self.reflection_quality_threshold})ï¼Œåœæ­¢è®¨è®º")
                        if log_callback:
                            log_callback(f"âœ… è´¨é‡è¾¾æ ‡ ({quality_score}åˆ†)")
                        break

                    # è£åˆ¤å»ºè®®åœæ­¢ + è´¨é‡å¯æ¥å—
                    if decision == "stop" and quality_score >= self.stop_quality_threshold:
                        logging.info(f"âœ… è£åˆ¤å»ºè®®åœæ­¢ + è´¨é‡å¯æ¥å— ({quality_score} >= {self.stop_quality_threshold})")
                        if log_callback:
                            log_callback(f"âœ… è£åˆ¤å»ºè®®åœæ­¢ (è´¨é‡ {quality_score}åˆ†)")
                        break

                    # è´¨é‡æ”¶æ•›åˆ¤å®š
                    if round_num > 1 and previous_quality > 0:
                        quality_delta = quality_score - previous_quality
                        if abs(quality_delta) < self.quality_convergence_delta:
                            logging.info(
                                f"ğŸ“‰ è´¨é‡æ”¶æ•› (Î”={quality_delta:.1f} < {self.quality_convergence_delta})ï¼Œåœæ­¢è®¨è®º")
                            if log_callback:
                                log_callback(f"ğŸ“‰ è´¨é‡æ”¶æ•› (Î”={quality_delta:.1f})")
                            break

                    previous_quality = quality_score
                else:
                    # éè‡ªé€‚åº”æ¨¡å¼ï¼šä»…å¬ä»è£åˆ¤å†³ç­–
                    if decision == "stop":
                        logging.info("âœ… è£åˆ¤å»ºè®®åœæ­¢ï¼Œç»“æŸè®¨è®º")
                        if log_callback:
                            log_callback("âœ… è£åˆ¤å»ºè®®åœæ­¢")
                        break

        # âœ… æœ€ç»ˆæ£€æŸ¥ï¼šç»¼åˆå‰
        if self._check_cancellation():
            # è¿”å›éƒ¨åˆ†ç»“æœ
            if history and len(history) > 0:
                last_content = ""
                for h in reversed(history):
                    if h.get("speaker") not in ["System", "User"]:
                        last_content = h.get("content", "")
                        break

                partial = last_content[:800] + "..." if len(last_content) > 800 else last_content
                return f"â¸ï¸ ä»»åŠ¡å·²è¢«å–æ¶ˆ\n\n**éƒ¨åˆ†ç»“æœ**ï¼ˆæ¥è‡ªç¬¬ {round_num} è½®è®¨è®ºï¼‰ï¼š\n\n{partial}"
            else:
                return "â¸ï¸ ä»»åŠ¡å·²è¢«å–æ¶ˆï¼Œæš‚æ— ç»“æœ"

        # ===== æœ€ç»ˆç»¼åˆ =====
        if log_callback:
            log_callback("ğŸ¯ å¼€å§‹æœ€ç»ˆç»¼åˆ")

        kg_context = ""
        if self.knowledge_graph:
            kg_context = self.knowledge_graph.distill(max_items=10)
            if kg_context:
                history.insert(-1, {"speaker": "System", "content": kg_context})

        history.append({
            "speaker": "System",
            "content": (
                "è¯·ç»¼åˆä»¥ä¸Šå…¨éƒ¨è®¨è®ºï¼Œç»™å‡ºæœ€å‡†ç¡®ã€æœ€å®Œæ•´ã€æœ€é«˜è´¨é‡çš„æœ€ç»ˆç­”æ¡ˆã€‚\n"
                "è¦æ±‚ï¼šé€»è¾‘ä¸¥å¯†ã€ä¿¡æ¯å®Œæ•´ã€ç»“æ„æ¸…æ™°ã€æ•´åˆçŸ¥è¯†å›¾è°±ã€‚"
            )
        })

        # âœ… æ£€æŸ¥ç‚¹ï¼šæœ€ç»ˆç”Ÿæˆå‰
        if self._check_cancellation():
            return "â¸ï¸ ä»»åŠ¡åœ¨æœ€ç»ˆç»¼åˆå‰è¢«å–æ¶ˆ"

        final_answer = self.leader.generate_response(
            history,
            round_num + 1,
            force_non_stream=False,
            stream_callback=stream_callback,
            log_callback=log_callback
        )

        tracker.checkpoint("5ï¸âƒ£ æœ€ç»ˆç»¼åˆ")

        # âœ… æ£€æŸ¥ç‚¹ï¼šæœ€ç»ˆç”Ÿæˆå
        if self._check_cancellation():
            partial = final_answer[:800] + "..." if len(final_answer) > 800 else final_answer
            return f"â¸ï¸ ä»»åŠ¡å·²è¢«å–æ¶ˆ\n\n**éƒ¨åˆ†ç»“æœ**ï¼š\n\n{partial}" if final_answer else "â¸ï¸ ä»»åŠ¡å·²è¢«å–æ¶ˆ"

        # ===== ä¿å­˜è®°å¿† =====
        if use_memory:
            try:
                if log_callback:
                    log_callback("ğŸ’¾ ä¿å­˜è®°å¿†ä¸­...")

                summary = self.leader.generate_response(
                    history + [{
                        "speaker": "System",
                        "content": "è¯·ç”¨500å­—æ€»ç»“ï¼šæ ¸å¿ƒç»“è®ºã€å…³é”®å‘ç°ã€å¯å¤ç”¨ç»éªŒã€é—ç•™é—®é¢˜"
                    }],
                    round_num + 1,
                    force_non_stream=True
                )
                self._save_memory(memory_key, summary)

                # âœ¨ PrimalClawï¼šä¿å­˜ç»“æ„åŒ–è®°å¿† + è‡ªåŠ¨æç‚¼
                if hasattr(self, 'primal_memory'):
                    self.primal_memory.save_episode(task, history, final_answer, memory_key)
                    self.primal_memory.decay()

                # å‘é‡è®°å¿†
                if self.vector_memory:
                    self.vector_memory.add(
                        summary,
                        metadata={"task": task[:100], "memory_key": memory_key}
                    )

                tracker.checkpoint("6ï¸âƒ£ ä¿å­˜è®°å¿†")
                if log_callback:
                    log_callback("ğŸ’¾ è®°å¿†ä¿å­˜å®Œæˆ")

            except Exception as e:
                logging.error(f"âŒ ä¿å­˜è®°å¿†å¤±è´¥: {e}")
                if log_callback:
                    log_callback(f"âš ï¸ è®°å¿†ä¿å­˜å¤±è´¥")

        return final_answer

    def _classify_task_complexity(self, task: str) -> str:
        """
        âœ¨ æ™ºèƒ½ä»»åŠ¡åˆ†ç±»å™¨
        è¿”å›: "simple" | "medium" | "complex"
        """
        # å¿«é€Ÿè§„åˆ™è¿‡æ»¤ï¼ˆ0msï¼Œæ— APIè°ƒç”¨ï¼‰
        task_lower = task.lower().strip()

        # æ–°å¢ï¼šå¯¹è¯è·Ÿè¿›åœºæ™¯ï¼ˆâ€œç»§ç»­â€â€œè¯¦ç»†è¯´è¯´â€â€œå†è§£é‡Šä¸€ä¸‹â€ï¼‰å¼ºåˆ¶ mediumï¼Œé˜²æ­¢ç®€å•é—®é¢˜è¢«è¯¯åˆ¤ complex
        if any(kw in task_lower for kw in ["ç»§ç»­", "è¯¦ç»†", "å†", "ç„¶å", "ä¸ºä»€ä¹ˆ", "æ€ä¹ˆ", "è§£é‡Šä¸€ä¸‹",
                                           "more details", "elaborate", "next"]):
            if len(task) < 150:  # çŸ­è·Ÿè¿›ä¸€å®šæ˜¯ medium
                logging.info("ğŸŸ¡ å¯¹è¯è·Ÿè¿› â†’ MEDIUM æ¨¡å¼")
                return "medium"

        # ç®€å•ä»»åŠ¡ç‰¹å¾ï¼ˆç›´æ¥åˆ¤å®šï¼‰
        simple_patterns = [
            # é—®å€™ç±»
            len(task) < 20 and any(word in task_lower for word in ["ä½ å¥½", "hi", "hello", "hey", "è°¢è°¢", "thank", "å˜¿", "thank", "ok", "å¥½çš„"]),
            # ç®€å•é—®ç­”
            task.endswith("?") and len(task) < 30,
            # å•ä¸€æŸ¥è¯¢
            task.startswith(("ä»€ä¹ˆæ˜¯", "who is", "when", "where")) and len(task) < 50,
        ]

        if any(simple_patterns):
            logging.info("ğŸŸ¢ ä»»åŠ¡åˆ†ç±»: SIMPLE (è§„åˆ™åŒ¹é…)")
            return "simple"

        # å¤æ‚ä»»åŠ¡ç‰¹å¾ï¼ˆç›´æ¥åˆ¤å®šï¼‰
        complex_patterns = [
            # æ˜ç¡®è¦æ±‚åä½œ
            any(word in task_lower for word in ["åˆ†ææŠ¥å‘Š", "æ·±åº¦", "å¯¹æ¯”", "è¯„ä¼°", "æˆ˜ç•¥", "æ–¹æ¡ˆ", "ä»£ç å®¡æŸ¥"]),
            # å¤šæ­¥éª¤ä»»åŠ¡
            task.count("å¹¶ä¸”") + task.count("ç„¶å") + task.count("åŒæ—¶") + task.count("and then") >= 2,
            # æ–‡ä»¶æ“ä½œ
            any(word in task_lower for word in ["å†™å…¥", "ä¿å­˜", "ç”Ÿæˆæ–‡ä»¶", "write to", "save to"]),
            # é•¿æ–‡æœ¬
            len(task) > 200,
        ]

        if any(complex_patterns):
            logging.info("ğŸ”´ ä»»åŠ¡åˆ†ç±»: COMPLEX (è§„åˆ™åŒ¹é…)")
            return "complex"

        # ä¸­ç­‰å¤æ‚åº¦ï¼šç”¨ Leader å¿«é€Ÿåˆ¤æ–­ï¼ˆå•æ¬¡ API è°ƒç”¨ï¼Œ~0.5ç§’ï¼‰
        try:
            classify_prompt = (
                f"ä»»åŠ¡: {task}\n\n"
                "è¯·åˆ¤æ–­æ­¤ä»»åŠ¡çš„å¤æ‚åº¦ï¼ˆä»…å›å¤ä¸€ä¸ªè¯ï¼‰ï¼š\n"
                "- simple: ç®€å•é—®å€™/å•å¥é—®ç­”/æŸ¥è¯¢\n"
                "- medium: éœ€è¦åˆ†æä½†ä¸å¤æ‚ï¼ˆå¦‚è§£é‡Šæ¦‚å¿µã€ç®€å•å»ºè®®ï¼‰\n"
                "- complex: éœ€è¦æ·±åº¦åˆ†æ/å¤šæ­¥éª¤/åä½œ\n\n"
                "å›å¤æ ¼å¼: ä»…è¾“å‡º simple/medium/complex"
            )

            response = self.leader.client.chat.completions.create(
                model=self.leader.model,
                messages=[
                    {"role": "system", "content": "ä½ æ˜¯ä»»åŠ¡å¤æ‚åº¦åˆ†ç±»å™¨ï¼Œä»…å›å¤ simple/medium/complex"},
                    {"role": "user", "content": classify_prompt}
                ],
                temperature=0.0,
                max_tokens=10,
                stream=False
            )

            # âœ… æ ¸å¿ƒä¿®å¤ï¼šå¤„ç† None è¿”å›å€¼
            content = response.choices[0].message.content

            if content is None or not content.strip():
                logging.warning("âš ï¸ API è¿”å›ç©ºå€¼ï¼Œé»˜è®¤ä½¿ç”¨ medium")
                return "medium"

            complexity = content.strip().lower()

            if complexity in ["simple", "medium", "complex"]:
                logging.info(f"ğŸŸ¡ ä»»åŠ¡åˆ†ç±»: {complexity.upper()} (AIåˆ¤æ–­)")
                return complexity
            else:
                logging.warning(f"âš ï¸ AIåˆ†ç±»è¿”å›æ— æ•ˆå€¼: {complexity}ï¼Œé»˜è®¤ä½¿ç”¨ medium")
                return "medium"

        except Exception as e:
            logging.error(f"âŒ ä»»åŠ¡åˆ†ç±»å¤±è´¥: {e}ï¼Œé»˜è®¤ä½¿ç”¨ medium")
            return "medium"

    def _solve_simple(
            self,
            task: str,
            history: List[Dict],
            stream_callback=None,  # âœ… æ–°å¢
            log_callback=None  # âœ… æ–°å¢
    ) -> str:
        """
        ğŸŸ¢ ç®€å•æ¨¡å¼ï¼šå• Agent ç›´æ¥å›ç­”
        """
        logging.info("ğŸŸ¢ æ‰§è¡Œç®€å•æ¨¡å¼ï¼ˆå•Agentç›´ç­”ï¼‰")
        print(f"\n{'=' * 80}")
        print("ğŸŸ¢ æ£€æµ‹åˆ°ç®€å•ä»»åŠ¡ï¼Œä½¿ç”¨å¿«é€Ÿæ¨¡å¼")
        print(f"{'=' * 80}\n")

        # âœ… å‘é€æ—¥å¿—
        if log_callback:
            log_callback("ğŸŸ¢ æ‰§è¡Œç®€å•æ¨¡å¼")

        # ç›´æ¥ç”¨ Leader å›ç­”ï¼ˆå…è®¸æµå¼è¾“å‡ºï¼‰
        answer = self.leader.generate_response(
            history,
            round_num=1,
            system_extra="è¯·ç®€æ´ã€ç›´æ¥åœ°å›ç­”ç”¨æˆ·é—®é¢˜ã€‚",
            force_non_stream=False,
            stream_callback=stream_callback,  # âœ… ä¼ é€’
            log_callback=log_callback  # âœ… ä¼ é€’
        )

        return answer

    def _solve_medium(
            self,
            task: str,
            history: List[Dict],
            tracker: TimeTracker,
            stream_callback=None,  # âœ… æ–°å¢
            log_callback=None  # âœ… æ–°å¢
    ) -> str:
        """
        ğŸŸ¡ ä¸­ç­‰æ¨¡å¼ï¼š2 Agents + å•è½®è®¨è®º
        """
        logging.info("ğŸŸ¡ æ‰§è¡Œä¸­ç­‰æ¨¡å¼ï¼ˆ2 Agents + 1è½®ï¼‰")
        print(f"\n{'=' * 80}")
        print("ğŸŸ¡ æ£€æµ‹åˆ°ä¸­ç­‰ä»»åŠ¡ï¼Œä½¿ç”¨ç²¾ç®€åä½œæ¨¡å¼")
        print(f"{'=' * 80}\n")

        # âœ… å‘é€æ—¥å¿—
        if log_callback:
            log_callback("ğŸŸ¡ æ‰§è¡Œä¸­ç­‰æ¨¡å¼ï¼ˆ2 Agents + 1è½®ï¼‰")

        # é€‰æ‹© 2 ä¸ªæœ€é€‚åˆçš„ Agentï¼ˆLeader + 1ä¸ªä¸“å®¶ï¼‰
        selected_agents = [self.leader]

        if len(self.agents) > 1:
            # ç®€å•ç­–ç•¥ï¼šé€‰æ‹©ç¬¬äºŒä¸ªAgentï¼ˆé€šå¸¸æ˜¯åˆ›æ„/åˆ†æä¸“å®¶ï¼‰
            selected_agents.append(self.agents[1])

        # å•è½®å¹¶å‘è®¨è®º
        with ThreadPoolExecutor(max_workers=2) as executor:
            future_to_agent = {
                executor.submit(
                    agent.generate_response,
                    history.copy(),
                    1,
                    log_callback=log_callback  # âœ… ä¼ é€’æ—¥å¿—ï¼ˆä¸ä¼  stream é¿å…æ··ä¹±ï¼‰
                ): agent
                for agent in selected_agents
            }

            for future in as_completed(future_to_agent):
                agent = future_to_agent[future]
                try:
                    contribution = future.result()
                    history.append({
                        "speaker": agent.name,
                        "content": contribution
                    })
                except Exception as e:
                    logging.error(f"âŒ {agent.name} æ‰§è¡Œå¤±è´¥: {e}")
                    if log_callback:
                        log_callback(f"âŒ {agent.name} æ‰§è¡Œå¤±è´¥")

        tracker.checkpoint("2ï¸âƒ£ å•è½®è®¨è®º")

        # Leader å¿«é€Ÿç»¼åˆ
        history.append({
            "speaker": "System",
            "content": "è¯·ç®€æ´ç»¼åˆä»¥ä¸Šè§‚ç‚¹ï¼Œç»™å‡ºæ¸…æ™°ç­”æ¡ˆã€‚"
        })

        final_answer = self.leader.generate_response(
            history,
            2,
            force_non_stream=False,
            stream_callback=stream_callback,  # âœ… ä¼ é€’æµå¼
            log_callback=log_callback  # âœ… ä¼ é€’æ—¥å¿—
        )

        tracker.checkpoint("3ï¸âƒ£ å¿«é€Ÿç»¼åˆ")

        return final_answer

    def _compress_history(self, history: List[Dict], max_tokens_approx: int = 20000) -> List[Dict]:
        """æç®€æ»šåŠ¨å‹ç¼©ï¼šåªä¿ç•™æœ€è¿‘Nè½® + Primalæ‘˜è¦"""
        if len(history) < 8:  # çŸ­å†å²ä¸å‹ç¼©
            return history

        # ä¼°ç®—tokensï¼ˆç²—ç•¥ï¼šä¸­æ–‡â‰ˆ2char=1tokenï¼‰
        total_chars = sum(len(str(h.get("content", ""))) for h in history)
        if total_chars < max_tokens_approx * 2:
            return history

        # ä¿ç•™ï¼šç³»ç»Ÿæç¤º + æœ€è¿‘3è½® + Primalè®°å¿† + æœ€ç»ˆç­”æ¡ˆ
        compressed = [h for h in history if h["speaker"] == "System" and "Primalè®°å¿†" in str(h.get("content", ""))]
        compressed.extend(history[-6:])  # ä¿ç•™æœ€è¿‘3è½®è®¨è®ºï¼ˆæ¯è½®2æ¡å·¦å³ï¼‰

        # å¦‚æœè¿˜æ˜¯å¤ªé•¿ï¼Œè®©Leaderåšä¸€æ¬¡è¶…çŸ­æ€»ç»“
        if len(str(compressed)) > max_tokens_approx * 3:
            summary_prompt = "è¯·ç”¨800å­—ä»¥å†…æ€»ç»“ä»¥ä¸Šå…¨éƒ¨å†å²ï¼Œåªä¿ç•™å…³é”®å†³ç­–ã€æ•™è®­å’Œç»“è®ºï¼Œä¸è¦é‡å¤ç»†èŠ‚ã€‚"
            short_summary = self.leader.generate_response(
                [{"speaker": "System", "content": summary_prompt}] + compressed[-4:],
                0, force_non_stream=True
            )
            compressed = [{"speaker": "System", "content": f"ğŸ“œ å†å²å‹ç¼©æ€»ç»“ï¼š\n{short_summary}"}]

        return compressed

    def nightly_reflect(self):
        """å¤œé—´/ä»»åŠ¡ååæ€ï¼ˆå¯æ‰‹åŠ¨æˆ–å®šæ—¶è°ƒç”¨ï¼‰"""
        if not hasattr(self, 'primal_memory'):
            return
        # è®©Leaderåšä¸€æ¬¡å…¨å±€åæ€ï¼ˆå¤ç”¨ç°æœ‰generate_responseï¼‰
        prompt = "è¯·æ‰«æ/memory/kb/æ‰€æœ‰lessonsï¼Œæç‚¼â‰¤5æ¡è·¨ä»»åŠ¡é€šç”¨æ™ºæ…§ï¼Œæ›´æ–°åˆ°kb/decisions/general.md"
        self.leader.generate_response([{"speaker": "System", "content": prompt}], 0, force_non_stream=True)


# ====================== ä¸»å‡½æ•° ======================
if __name__ == "__main__":
    try:
        swarm = MultiAgentSwarm()

        print("\n" + "ğŸ§ª" * 40)
        print("å¼€å§‹æµ‹è¯•æ™ºèƒ½è·¯ç”±åŠŸèƒ½")
        print("ğŸ§ª" * 40 + "\n")

        # ===== æµ‹è¯• 1ï¼šç®€å•æ¨¡å¼ =====
        print("\nğŸ“ æµ‹è¯• 1: ç®€å•é—®å€™ï¼ˆé¢„æœŸ: SIMPLE æ¨¡å¼ï¼Œ~1-2ç§’ï¼‰")
        print("=" * 80)
        msg = swarm.solve("ä½ å¥½ï¼Œä»Šå¤©å¤©æ°”æ€ä¹ˆæ ·ï¼Ÿ")
        print('æµ‹è¯•1 å›ç­”:\n', msg)
        input("\næŒ‰å›è½¦ç»§ç»­ä¸‹ä¸€ä¸ªæµ‹è¯•...")

        # ===== æµ‹è¯• 2ï¼šä¸­ç­‰æ¨¡å¼ =====
        print("\nğŸ“ æµ‹è¯• 2: æ¦‚å¿µè§£é‡Šï¼ˆé¢„æœŸ: MEDIUM æ¨¡å¼ï¼Œ~10-20ç§’ï¼‰")
        print("=" * 80)
        msg = swarm.solve("è¯·è§£é‡Šä¸€ä¸‹ä»€ä¹ˆæ˜¯ Transformer æ³¨æ„åŠ›æœºåˆ¶")
        print('æµ‹è¯•2 å›ç­”:\n', msg)
        input("\næŒ‰å›è½¦ç»§ç»­ä¸‹ä¸€ä¸ªæµ‹è¯•...")

        # ===== æµ‹è¯• 3ï¼šå¤æ‚æ¨¡å¼ =====
        print("\nğŸ“ æµ‹è¯• 3: æ·±åº¦åˆ†æï¼ˆé¢„æœŸ: COMPLEX æ¨¡å¼ï¼Œ~40-60ç§’ï¼‰")
        print("=" * 80)
        msg = swarm.solve(
            "è¯·å†™ä¸€ç¯‡å…³äºå¤§è¯­è¨€æ¨¡å‹è®­ç»ƒæŠ€æœ¯çš„æ·±åº¦åˆ†ææŠ¥å‘Šï¼Œ"
            "åŒ…æ‹¬æ•°æ®å‡†å¤‡ã€æ¨¡å‹æ¶æ„ã€è®­ç»ƒç­–ç•¥çš„å¯¹æ¯”åˆ†æ",
            use_memory=True,
            memory_key="llm_training"
        )
        print('æµ‹è¯•3 å›ç­”:\n', msg)
        input("\næŒ‰å›è½¦ç»§ç»­ä¸‹ä¸€ä¸ªæµ‹è¯•...")

        # ===== æµ‹è¯• 4ï¼šå¼ºåˆ¶æ¨¡å¼ =====
        print("\nğŸ“ æµ‹è¯• 4: å¼ºåˆ¶ä½¿ç”¨ COMPLEX æ¨¡å¼å¤„ç†ç®€å•ä»»åŠ¡")
        print("=" * 80)
        msg = swarm.solve("ä½ å¥½", force_complexity="complex")
        print('æµ‹è¯•4 å›ç­”:\n', msg)

        # ç¤ºä¾‹5ï¼šå›¾åƒåˆ†æï¼ˆéœ€è¦æä¾›çœŸå®å›¾ç‰‡è·¯å¾„ï¼‰
        # swarm.solve(
        #     "è¯·åˆ†æè¿™äº›å›¾ç‰‡ä¸­çš„ä»£ç é—®é¢˜",
        #     image_paths=["./screenshot1.png", "./screenshot2.png"]
        # )

        print("\n" + "âœ…" * 40)
        print("æ‰€æœ‰æµ‹è¯•å®Œæˆï¼")
        print("âœ…" * 40 + "\n")

    except Exception as e:
        logging.error(f"âŒ ç¨‹åºå¼‚å¸¸: {e}", exc_info=True)
        print(f"\nâŒ é”™è¯¯: {e}")