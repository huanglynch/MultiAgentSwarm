#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
å¤šæ™ºèƒ½ä½“åä½œç³»ç»Ÿ (Multi-Agent Swarm) v2.9.2
- æ”¯æŒå¹¶å‘æ§åˆ¶ï¼ˆmax_concurrent_agentsï¼‰
- æ”¯æŒå‘é‡è®°å¿†ï¼ˆä¼˜å…ˆä½¿ç”¨ç¼“å­˜æ¨¡å‹ï¼‰
- æ”¯æŒè€—æ—¶ç»Ÿè®¡
- æ”¯æŒå¤šæ¨¡æ€è¾“å…¥ï¼ˆæ–‡æœ¬ + å›¾åƒï¼‰
"""

import yaml
import logging
import os
import glob
import importlib.util
import requests
import random
import time
import threading
import base64
import mimetypes
from bs4 import BeautifulSoup
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import List, Dict, Optional
from openai import OpenAI
import json
from datetime import datetime
import chromadb
from chromadb.config import Settings
from sentence_transformers import SentenceTransformer
from duckduckgo_search import DDGS

# ====================== æ—¶é—´ç»Ÿè®¡å·¥å…· ======================
from contextlib import contextmanager


class TimeTracker:
    """æ—¶é—´ç»Ÿè®¡å·¥å…·ç±»"""

    def __init__(self):
        self.start_time = None
        self.checkpoints = {}

    def start(self):
        """å¼€å§‹è®¡æ—¶"""
        self.start_time = time.time()
        return self.start_time

    def checkpoint(self, name: str):
        """è®°å½•æ£€æŸ¥ç‚¹"""
        if self.start_time is None:
            self.start()
        elapsed = time.time() - self.start_time
        self.checkpoints[name] = elapsed
        return elapsed

    def get_elapsed(self) -> float:
        """è·å–æ€»è€—æ—¶"""
        if self.start_time is None:
            return 0
        return time.time() - self.start_time

    def format_time(self, seconds: float) -> str:
        """æ ¼å¼åŒ–æ—¶é—´æ˜¾ç¤º"""
        if seconds < 60:
            return f"{seconds:.2f}ç§’"
        elif seconds < 3600:
            minutes = int(seconds // 60)
            secs = seconds % 60
            return f"{minutes}åˆ†{secs:.1f}ç§’"
        else:
            hours = int(seconds // 3600)
            minutes = int((seconds % 3600) // 60)
            secs = seconds % 60
            return f"{hours}å°æ—¶{minutes}åˆ†{secs:.0f}ç§’"

    def summary(self) -> str:
        """ç”Ÿæˆè€—æ—¶æ‘˜è¦"""
        total = self.get_elapsed()
        lines = [f"\n{'=' * 60}"]
        lines.append(f"â±ï¸  æ€»è€—æ—¶: {self.format_time(total)}")
        lines.append(f"{'â”€' * 60}")

        if self.checkpoints:
            lines.append("ğŸ“Š å„é˜¶æ®µè€—æ—¶:")
            for name, elapsed in self.checkpoints.items():
                percentage = (elapsed / total * 100) if total > 0 else 0
                lines.append(f"   {name}: {self.format_time(elapsed)} ({percentage:.1f}%)")

        lines.append(f"{'=' * 60}")
        return "\n".join(lines)


@contextmanager
def timer(description: str):
    """ä¸Šä¸‹æ–‡ç®¡ç†å™¨ï¼šè‡ªåŠ¨è®¡æ—¶å¹¶æ‰“å°"""
    start = time.time()
    print(f"â±ï¸  å¼€å§‹: {description}", flush=True)
    try:
        yield
    finally:
        elapsed = time.time() - start
        print(f"âœ… å®Œæˆ: {description} | è€—æ—¶: {TimeTracker().format_time(elapsed)}", flush=True)


# ====================== çº¿ç¨‹å®‰å…¨çš„å·¥å…·ç¼“å­˜ ======================
tool_cache = {}
cache_count = 0
cache_lock = threading.Lock()


def clean_cache():
    """è‡ªåŠ¨æ¸…ç†å·¥å…·ç¼“å­˜ï¼ˆçº¿ç¨‹å®‰å…¨ï¼‰"""
    global cache_count
    with cache_lock:
        if cache_count > 50:
            tool_cache.clear()
            cache_count = 0
            logging.info("ğŸ§¹ è‡ªåŠ¨æ¸…ç†å·¥å…·ç¼“å­˜")


# ====================== å·¥å…·å‡½æ•° ======================
def web_search(query: str, num_results: int = 5) -> str:
    """DuckDuckGo ç½‘é¡µæœç´¢ï¼ˆå¸¦ç¼“å­˜ + éšæœºå»¶æ—¶ï¼‰"""
    global cache_count
    clean_cache()

    with cache_lock:
        if query in tool_cache:
            return tool_cache[query]

    time.sleep(random.uniform(0.5, 2.0))

    try:
        with DDGS() as ddgs:
            results = [r for r in ddgs.text(query, max_results=num_results)]

        result = "\n".join([
            f"æ ‡é¢˜: {r['title']}\næ‘˜è¦: {r['body']}\né“¾æ¥: {r['href']}"
            for r in results
        ])

        with cache_lock:
            tool_cache[query] = result
            cache_count += 1

        return result
    except Exception as e:
        logging.error(f"æœç´¢å¤±è´¥: {e}")
        return f"æœç´¢å¤±è´¥: {str(e)}"


def browse_page(url: str) -> str:
    """æµè§ˆç½‘é¡µå¹¶æå–æ–‡æœ¬ï¼ˆå¸¦ç¼“å­˜ï¼‰"""
    global cache_count
    clean_cache()

    with cache_lock:
        if url in tool_cache:
            return tool_cache[url]

    try:
        headers = {"User-Agent": "Mozilla/5.0"}
        resp = requests.get(url, headers=headers, timeout=10)
        resp.raise_for_status()

        soup = BeautifulSoup(resp.text, "html.parser")
        for script in soup(["script", "style"]):
            script.decompose()

        text = soup.get_text()
        lines = (line.strip() for line in text.splitlines())
        chunks = (phrase.strip() for line in lines for phrase in line.split("  "))
        result = "\n".join(chunk for chunk in chunks if chunk)

        with cache_lock:
            tool_cache[url] = result
            cache_count += 1

        return result
    except Exception as e:
        logging.error(f"æµè§ˆå¤±è´¥ {url}: {e}")
        return f"æµè§ˆå¤±è´¥: {str(e)}"


def run_python(code: str) -> str:
    """
    æ²™ç®±æ‰§è¡Œ Python ä»£ç ï¼ˆ10ç§’è¶…æ—¶ï¼‰
    æ³¨æ„ï¼šthreading.Timer æ— æ³•çœŸæ­£ç»ˆæ­¢é˜»å¡ä»£ç ï¼Œä»…ä½œè½¯è¶…æ—¶
    """
    result_container = {"output": None, "done": False}

    def target():
        try:
            restricted_globals = {
                "__builtins__": {
                    "print": print,
                    "range": range,
                    "len": len,
                    "str": str,
                    "int": int,
                    "float": float,
                    "list": list,
                    "dict": dict,
                    "sum": sum,
                    "min": min,
                    "max": max,
                }
            }
            local_vars = {}
            exec(code, restricted_globals, local_vars)
            result_container["output"] = str(local_vars.get("result", "æ‰§è¡ŒæˆåŠŸï¼Œæ— è¿”å›ç»“æœ"))
        except Exception as e:
            result_container["output"] = f"æ‰§è¡Œé”™è¯¯: {str(e)}"
        finally:
            result_container["done"] = True

    thread = threading.Thread(target=target, daemon=True)
    thread.start()
    thread.join(timeout=10.0)

    if not result_container["done"]:
        return "â±ï¸ æ‰§è¡Œè¶…æ—¶ï¼ˆ10ç§’ï¼‰"

    return result_container["output"]


# ====================== å‘é‡è®°å¿† ======================
class VectorMemory:
    """
    åŸºäº ChromaDB å’Œ SentenceTransformer çš„å‘é‡è®°å¿†ç³»ç»Ÿ
    âœ… ä¼˜å…ˆä½¿ç”¨ç¼“å­˜æ¨¡å‹ï¼Œé¿å…é‡å¤ä¸‹è½½
    """

    def __init__(
            self,
            persist_directory: str = "./memory_db",
            model_name: str = "sentence-transformers/all-MiniLM-L6-v2",
            cache_dir: str = "./cached_model/"
    ):
        """
        åˆå§‹åŒ–å‘é‡è®°å¿†ç³»ç»Ÿ

        Args:
            persist_directory: ChromaDB æ•°æ®åº“è·¯å¾„
            model_name: SentenceTransformer æ¨¡å‹åç§°
            cache_dir: æ¨¡å‹ç¼“å­˜ç›®å½•ï¼ˆä¼˜å…ˆä»æ­¤å¤„åŠ è½½ï¼‰
        """
        self.persist_directory = persist_directory
        self.model_name = model_name
        self.cache_dir = os.path.abspath(cache_dir)

        # ç¡®ä¿ç¼“å­˜ç›®å½•å­˜åœ¨
        os.makedirs(self.cache_dir, exist_ok=True)

        # åˆå§‹åŒ–åµŒå…¥æ¨¡å‹ï¼ˆä¼˜å…ˆä½¿ç”¨ç¼“å­˜ï¼‰
        self._init_embedding_model()

        # åˆå§‹åŒ– ChromaDB
        self._init_chromadb()

    def _init_embedding_model(self):
        """åˆå§‹åŒ–åµŒå…¥æ¨¡å‹ï¼ˆä¼˜å…ˆä»ç¼“å­˜åŠ è½½ï¼‰"""
        try:
            # æ£€æŸ¥ç¼“å­˜æ˜¯å¦å­˜åœ¨
            cached_model_path = os.path.join(self.cache_dir, self.model_name.replace('/', '_'))

            if os.path.exists(cached_model_path):
                logging.info(f"ğŸ“¦ ä»ç¼“å­˜åŠ è½½å‘é‡æ¨¡å‹: {cached_model_path}")
                print(f"ğŸ“¦ ä»ç¼“å­˜åŠ è½½å‘é‡æ¨¡å‹: {self.model_name}")
                self.embedding_model = SentenceTransformer(cached_model_path)
            else:
                logging.info(f"â¬‡ï¸  ä¸‹è½½å‘é‡æ¨¡å‹: {self.model_name} â†’ {cached_model_path}")
                print(f"â¬‡ï¸  é¦–æ¬¡ä½¿ç”¨ï¼Œæ­£åœ¨ä¸‹è½½å‘é‡æ¨¡å‹: {self.model_name}")
                print(f"   ä¸‹è½½åå°†ç¼“å­˜åˆ°: {cached_model_path}")

                # ä¸‹è½½æ¨¡å‹
                self.embedding_model = SentenceTransformer(self.model_name)

                # ä¿å­˜åˆ°ç¼“å­˜
                self.embedding_model.save(cached_model_path)
                logging.info(f"âœ… æ¨¡å‹å·²ç¼“å­˜åˆ°: {cached_model_path}")
                print(f"âœ… æ¨¡å‹å·²ç¼“å­˜ï¼Œä¸‹æ¬¡å°†ç›´æ¥ä½¿ç”¨")

        except Exception as e:
            logging.error(f"âŒ å‘é‡æ¨¡å‹åˆå§‹åŒ–å¤±è´¥: {e}")
            raise

    def _init_chromadb(self):
        """åˆå§‹åŒ– ChromaDB å®¢æˆ·ç«¯"""
        try:
            os.makedirs(os.path.dirname(self.persist_directory) if os.path.dirname(
                self.persist_directory) else ".", exist_ok=True)

            self.client = chromadb.PersistentClient(
                path=self.persist_directory,
                settings=Settings(anonymized_telemetry=False)
            )

            # è·å–æˆ–åˆ›å»ºé›†åˆ
            self.collection = self.client.get_or_create_collection(
                name="swarm_memory",
                metadata={"description": "Agent memory storage"}
            )

            logging.info(f"âœ… ChromaDB åˆå§‹åŒ–æˆåŠŸ: {self.persist_directory}")

        except Exception as e:
            logging.error(f"âŒ ChromaDB åˆå§‹åŒ–å¤±è´¥: {e}")
            self.collection = None

    def add(self, text: str, metadata: Optional[Dict] = None):
        """æ·»åŠ è®°å¿†åˆ°å‘é‡æ•°æ®åº“"""
        if not self.collection:
            return

        try:
            # ç”ŸæˆåµŒå…¥å‘é‡
            embedding = self.embedding_model.encode(text).tolist()

            # ç”Ÿæˆ ID
            memory_id = f"mem_{datetime.now().strftime('%Y%m%d_%H%M%S_%f')}"

            # æ·»åŠ åˆ°æ•°æ®åº“
            self.collection.add(
                ids=[memory_id],
                embeddings=[embedding],
                documents=[text],
                metadatas=[metadata or {"timestamp": datetime.now().isoformat()}]
            )

            logging.info(f"âœ… è®°å¿†å·²ä¿å­˜: {memory_id}")

        except Exception as e:
            logging.error(f"âŒ ä¿å­˜è®°å¿†å¤±è´¥: {e}")

    def search(self, query: str, n_results: int = 5) -> str:
        """æœç´¢ç›¸å…³è®°å¿†"""
        if not self.collection:
            return ""

        try:
            # ç”ŸæˆæŸ¥è¯¢åµŒå…¥
            query_embedding = self.embedding_model.encode(query).tolist()

            # æœç´¢
            results = self.collection.query(
                query_embeddings=[query_embedding],
                n_results=n_results
            )

            # æ ¼å¼åŒ–ç»“æœ
            if results and results["documents"]:
                return "\n\n---\n\n".join(results["documents"][0])

        except Exception as e:
            logging.error(f"âŒ æœç´¢è®°å¿†å¤±è´¥: {e}")

        return ""


# ====================== Skill åŠ¨æ€åŠ è½½å™¨ ======================
def load_skills(skills_dir: str = "skills"):
    """
    é€’å½’åŠ è½½ skills ç›®å½•ä¸‹çš„æ‰€æœ‰ Python å·¥å…·å’Œ Markdown çŸ¥è¯†æ–‡ä»¶
    æ”¯æŒå­ç›®å½•ç»“æ„
    """
    tool_registry = {}
    shared_knowledge = []

    if not os.path.exists(skills_dir):
        logging.warning(f"âš ï¸ Skills ç›®å½•ä¸å­˜åœ¨: {skills_dir}")
        return tool_registry, ""

    # é€’å½’æ‰«ææ‰€æœ‰ .py æ–‡ä»¶
    py_files = glob.glob(os.path.join(skills_dir, "**/*.py"), recursive=True)

    for py_file in py_files:
        try:
            rel_path = os.path.relpath(py_file, skills_dir)

            # åŠ¨æ€å¯¼å…¥æ¨¡å—
            spec = importlib.util.spec_from_file_location(
                os.path.splitext(os.path.basename(py_file))[0],
                py_file
            )
            mod = importlib.util.module_from_spec(spec)
            spec.loader.exec_module(mod)

            # æŸ¥æ‰¾å·¥å…·å‡½æ•°å’Œ schema
            if hasattr(mod, "tool_function") and hasattr(mod, "tool_schema"):
                tool_name = mod.tool_schema["function"]["name"]
                tool_registry[tool_name] = {
                    "func": mod.tool_function,
                    "schema": mod.tool_schema
                }
                logging.info(f"âœ… åŠ è½½ Skill (py): {tool_name} | æ¥è‡ª: {rel_path}")
            else:
                logging.warning(f"âš ï¸ è·³è¿‡æ— æ•ˆ Skill æ–‡ä»¶: {rel_path}")

        except Exception as e:
            logging.error(f"âŒ åŠ è½½ Skill å¤±è´¥: {py_file} | é”™è¯¯: {e}")

    # é€’å½’æ‰«ææ‰€æœ‰ .md æ–‡ä»¶
    md_files = glob.glob(os.path.join(skills_dir, "**/*.md"), recursive=True)

    for md_file in md_files:
        try:
            rel_path = os.path.relpath(md_file, skills_dir)
            with open(md_file, "r", encoding="utf-8") as f:
                content = f.read().strip()
                if content:
                    shared_knowledge.append(f"### ğŸ“š æ¥è‡ª {rel_path} ###\n{content}")
                    logging.info(f"ğŸ“š åŠ è½½çŸ¥è¯†æ–‡ä»¶ (md): {rel_path}")
        except Exception as e:
            logging.error(f"âŒ è¯»å–çŸ¥è¯†æ–‡ä»¶å¤±è´¥: {md_file} | é”™è¯¯: {e}")

    logging.info(f"ğŸ“Š Skills åŠ è½½å®Œæˆ: {len(tool_registry)} ä¸ªå·¥å…·, {len(shared_knowledge)} ä¸ªçŸ¥è¯†æ–‡ä»¶")

    shared_knowledge_str = "\n\n".join(shared_knowledge)

    return tool_registry, shared_knowledge_str


# ====================== Agent ç±» ======================
class Agent:
    """å•ä¸ªæ™ºèƒ½ä½“ä»£ç†"""

    def __init__(
            self,
            config: Dict,
            default_model: str,
            default_max_tokens: int,
            tool_registry: Dict,
            shared_knowledge: str = "",
            vector_memory: Optional[VectorMemory] = None
    ):
        self.name = config["name"]
        self.role = config["role"]
        self.shared_knowledge = shared_knowledge
        self.vector_memory = vector_memory

        # OpenAI å®¢æˆ·ç«¯é…ç½®
        self.client = OpenAI(
            api_key=config.get("api_key"),
            base_url=config.get("base_url")
        )
        self.model = config.get("model", default_model)
        self.temperature = config.get("temperature", 0.7)
        self.stream = config.get("stream", False)
        self.max_tokens = config.get("max_tokens", default_max_tokens)

        # å·¥å…·é…ç½®
        enabled = config.get("enabled_tools", [])
        self.tools = [
            tool_registry[name]["schema"]
            for name in enabled
            if name in tool_registry
        ]
        self.tool_map = {
            name: tool_registry[name]["func"]
            for name in enabled
            if name in tool_registry
        }

        if self.tools:
            logging.debug(f"  {self.name} å·²å¯ç”¨å·¥å…·: {list(self.tool_map.keys())}")

    def _execute_tool(self, tool_call) -> Dict:
        """æ‰§è¡Œå·¥å…·è°ƒç”¨"""
        func_name = tool_call.function.name

        try:
            args = json.loads(tool_call.function.arguments)
            logging.info(f"ğŸ”§ {self.name} è°ƒç”¨å·¥å…·: {func_name}({args})")

            result = self.tool_map[func_name](**args)

            return {
                "role": "tool",
                "tool_call_id": tool_call.id,
                "name": func_name,
                "content": str(result)
            }
        except Exception as e:
            logging.error(f"å·¥å…·æ‰§è¡Œå¤±è´¥ {func_name}: {e}")
            return {
                "role": "tool",
                "tool_call_id": tool_call.id,
                "name": func_name,
                "content": f"Tool error: {str(e)}"
            }

    def generate_response(
            self,
            history: List[Dict],
            round_num: int,
            system_extra: str = "",
            force_non_stream: bool = False
    ) -> str:
        """ç”Ÿæˆå“åº”"""
        start_time = time.time()

        use_stream = self.stream and not force_non_stream and not self.tools

        # æ„å»ºç³»ç»Ÿæç¤ºè¯
        system_prompt = (
            f"{self.role}\n"
            f"{self.shared_knowledge}\n"
            f"{system_extra}\n"
            "ä½ æ˜¯å¤šæ™ºèƒ½ä½“åä½œå›¢é˜Ÿçš„ä¸€å‘˜ï¼Œè¯·æä¾›æœ‰ä»·å€¼ã€å‡†ç¡®ã€æœ‰æ·±åº¦çš„è´¡çŒ®ã€‚"
        )

        messages = [{"role": "system", "content": system_prompt}]

        # å¤„ç†å†å²æ¶ˆæ¯
        for h in history:
            if h["speaker"] == "User":
                messages.append({"role": "user", "content": h["content"]})
            elif h["speaker"] == "System":
                messages.append({"role": "system", "content": h["content"]})
            else:
                messages.append({
                    "role": "assistant",
                    "content": f"[{h['speaker']}] {h.get('content', '')}"
                })

        try:
            if use_stream:
                print(f"\nğŸ’¬ ã€{self.name}ã€‘æ­£åœ¨æ€è€ƒ... ", end="", flush=True)

            response = self.client.chat.completions.create(
                model=self.model,
                messages=messages,
                temperature=self.temperature,
                max_tokens=self.max_tokens,
                tools=self.tools if self.tools else None,
                tool_choice="auto" if self.tools else None,
                stream=use_stream,
            )

            full_response = ""

            # æµå¼è¾“å‡º
            if use_stream:
                for chunk in response:
                    if chunk.choices[0].delta.content:
                        delta = chunk.choices[0].delta.content
                        print(delta, end="", flush=True)
                        full_response += delta
                print()
            else:
                full_response = response.choices[0].message.content or ""

            # å·¥å…·è°ƒç”¨å¤„ç†
            if (not use_stream and
                    hasattr(response.choices[0].message, 'tool_calls') and
                    response.choices[0].message.tool_calls):

                messages.append(response.choices[0].message.model_dump())

                for tool_call in response.choices[0].message.tool_calls:
                    tool_result = self._execute_tool(tool_call)
                    messages.append(tool_result)

                # è·å–å·¥å…·è°ƒç”¨åçš„æœ€ç»ˆå“åº”
                final_resp = self.client.chat.completions.create(
                    model=self.model,
                    messages=messages,
                    temperature=self.temperature,
                    max_tokens=self.max_tokens,
                    stream=False
                )
                full_response = final_resp.choices[0].message.content or ""

            # è®¡ç®—å¹¶æ˜¾ç¤ºè€—æ—¶
            elapsed = time.time() - start_time
            elapsed_str = f"{elapsed:.2f}ç§’" if elapsed < 60 else f"{int(elapsed // 60)}åˆ†{elapsed % 60:.1f}ç§’"

            if not use_stream:
                print(f"â±ï¸  ã€{self.name}ã€‘å“åº”å®Œæˆ | è€—æ—¶: {elapsed_str}")

            logging.info(f"â±ï¸  {self.name} å“åº”è€—æ—¶: {elapsed_str}")

            return full_response.strip()

        except Exception as e:
            elapsed = time.time() - start_time
            err = f"[Error in {self.name}]: {str(e)}"
            logging.error(f"{err} | è€—æ—¶: {elapsed:.2f}ç§’")
            print(f"âŒ ã€{self.name}ã€‘æ‰§è¡Œå¤±è´¥ | è€—æ—¶: {elapsed:.2f}ç§’")
            return err


# ====================== ä¸»ç±» MultiAgentSwarm ======================
class MultiAgentSwarm:
    """å¤šæ™ºèƒ½ä½“ç¾¤æ™ºæ…§æ¡†æ¶ v2.9.2"""

    def __init__(self, config_path: str = "swarm_config.yaml"):
        if not os.path.exists(config_path):
            raise FileNotFoundError(f"âŒ é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_path}")

        with open(config_path, "r", encoding="utf-8") as f:
            cfg = yaml.safe_load(f)

        # OpenAI é…ç½®
        oai = cfg.get("openai", {})
        self.default_model = oai.get("default_model", "gpt-4o-mini")
        self.default_max_tokens = oai.get("default_max_tokens", 4096)

        # Swarm é…ç½®
        swarm = cfg.get("swarm", {})
        self.mode = swarm.get("mode", "fixed")
        self.max_rounds = swarm.get("max_rounds", 3 if self.mode == "fixed" else 10)
        self.max_concurrent_agents = swarm.get("max_concurrent_agents", 2)  # âœ… æ–°å¢
        self.reflection_planning = swarm.get("reflection_planning", True)
        self.enable_web_search = swarm.get("enable_web_search", False)
        self.max_images = swarm.get("max_images", 2)

        self.log_file = swarm.get("log_file", "swarm.log")
        self.skills_dir = swarm.get("skills_dir", "skills")
        self.memory_file = swarm.get("memory_file", "memory.json")
        self.max_memory_items = swarm.get("max_memory_items", 50)

        self.max_reflection_rounds = swarm.get("max_reflection_rounds", 3)
        self.reflection_quality_threshold = swarm.get("reflection_quality_threshold", 9)
        self.stop_quality_threshold = swarm.get("stop_quality_threshold", 8)
        self.quality_convergence_delta = swarm.get("quality_convergence_delta", 0.5)

        # âœ… å‘é‡è®°å¿†é…ç½®
        vector_cfg = swarm.get("vector_memory", {})
        self.vector_memory_enabled = vector_cfg.get("enabled", False)
        self.vector_persist_directory = vector_cfg.get("persist_directory", "./memory_db")
        self.vector_model_cache_dir = vector_cfg.get("model_cache_dir", "./cached_model/")
        self.vector_embedding_model = vector_cfg.get("embedding_model",
                                                     "sentence-transformers/distiluse-base-multilingual-cased-v2")

        # æ—¥å¿—é…ç½®
        logging.basicConfig(
            filename=self.log_file,
            level=logging.INFO,
            format="%(asctime)s | %(levelname)s | %(message)s",
            encoding="utf-8",
            force=True
        )
        logging.getLogger().addHandler(logging.StreamHandler())

        logging.info(f"{'=' * 80}")
        logging.info(f"ğŸš€ MultiAgentSwarm v2.9.2 åˆå§‹åŒ–")
        logging.info(f"   Mode: {self.mode} | Max Rounds: {self.max_rounds}")
        logging.info(f"   Max Concurrent: {self.max_concurrent_agents}")  # âœ… æ–°å¢
        logging.info(f"   Reflection: {self.reflection_planning} | Web Search: {self.enable_web_search}")
        logging.info(f"   Vector Memory: {self.vector_memory_enabled}")  # âœ… æ–°å¢
        logging.info(f"{'=' * 80}")

        # åŠ è½½ Skills
        self.tool_registry, self.shared_knowledge = load_skills(self.skills_dir)

        # æ·»åŠ å†…ç½®ç½‘ç»œæœç´¢å·¥å…·
        if self.enable_web_search:
            self.tool_registry["web_search"] = {
                "func": web_search,
                "schema": {
                    "type": "function",
                    "function": {
                        "name": "web_search",
                        "description": "å®æ—¶ç½‘é¡µæœç´¢æœ€æ–°ä¿¡æ¯ï¼ˆDuckDuckGoï¼‰",
                        "parameters": {
                            "type": "object",
                            "properties": {
                                "query": {"type": "string", "description": "æœç´¢å…³é”®è¯"},
                                "num_results": {"type": "integer", "description": "è¿”å›ç»“æœæ•°é‡", "default": 5}
                            },
                            "required": ["query"]
                        }
                    }
                }
            }
            logging.info("âœ… å·²å¯ç”¨ç½‘ç»œæœç´¢å·¥å…·")

        # åˆå§‹åŒ–æŒä¹…åŒ–è®°å¿†ï¼ˆå¿…é¡»åœ¨å‘é‡è®°å¿†ä¹‹å‰ï¼‰
        self.memory = self._load_memory()

        # âœ… åˆå§‹åŒ–å‘é‡è®°å¿†
        self.vector_memory = None
        if self.vector_memory_enabled:
            try:
                self.vector_memory = VectorMemory(
                    persist_directory=self.vector_persist_directory,
                    model_name=self.vector_embedding_model,
                    cache_dir=self.vector_model_cache_dir
                )
                logging.info("âœ… å‘é‡è®°å¿†ç³»ç»Ÿåˆå§‹åŒ–æˆåŠŸ")
            except Exception as e:
                logging.warning(f"âš ï¸ å‘é‡è®°å¿†åˆå§‹åŒ–å¤±è´¥: {e}")
                self.vector_memory_enabled = False

        # åˆå§‹åŒ– Agents
        self.agents = []
        for a_cfg in cfg.get("agents", [])[:swarm.get("num_agents", 4)]:
            agent = Agent(
                a_cfg,
                self.default_model,
                self.default_max_tokens,
                self.tool_registry,
                self.shared_knowledge,
                self.vector_memory
            )
            self.agents.append(agent)
            logging.info(f"âœ… Agent åŠ è½½: {agent.name} | Model: {agent.model}")

        if not self.agents:
            raise ValueError("âŒ è‡³å°‘éœ€è¦é…ç½®ä¸€ä¸ª Agent")

        self.leader = self.agents[0]
        logging.info(f"ğŸ‘‘ Leader: {self.leader.name}")

    def _load_memory(self) -> Dict:
        """åŠ è½½æŒä¹…åŒ–è®°å¿†"""
        if os.path.exists(self.memory_file):
            try:
                with open(self.memory_file, "r", encoding="utf-8") as f:
                    memory = json.load(f)
                logging.info(f"ğŸ“– åŠ è½½è®°å¿†æ–‡ä»¶: {self.memory_file} ({len(memory)} keys)")
                return memory
            except Exception as e:
                logging.error(f"åŠ è½½è®°å¿†å¤±è´¥: {e}")
        return {}

    def _save_memory(self, key: str, summary: str):
        """ä¿å­˜è®°å¿†åˆ°æŒä¹…åŒ–æ–‡ä»¶"""
        if key not in self.memory:
            self.memory[key] = []

        self.memory[key].append({
            "timestamp": datetime.now().isoformat(),
            "summary": summary[:3000]
        })

        if len(self.memory[key]) > self.max_memory_items:
            self.memory[key] = self.memory[key][-self.max_memory_items:]

        try:
            with open(self.memory_file, "w", encoding="utf-8") as f:
                json.dump(self.memory, f, ensure_ascii=False, indent=2)
            logging.info(f"ğŸ’¾ ä¿å­˜è®°å¿†: {key}")
        except Exception as e:
            logging.error(f"ä¿å­˜è®°å¿†å¤±è´¥: {e}")

    def solve(
            self,
            task: str,
            use_memory: bool = False,
            memory_key: str = "default",
            image_paths: Optional[List[str]] = None
    ) -> str:
        """
        è§£å†³ä»»åŠ¡çš„ä¸»å…¥å£

        Args:
            task: ä»»åŠ¡æè¿°
            use_memory: æ˜¯å¦ä½¿ç”¨æŒä¹…åŒ–è®°å¿†
            memory_key: è®°å¿†é”®å
            image_paths: å›¾åƒæ–‡ä»¶è·¯å¾„åˆ—è¡¨

        Returns:
            æœ€ç»ˆç­”æ¡ˆ
        """
        tracker = TimeTracker()
        tracker.start()

        logging.info(f"\n{'=' * 80}")
        logging.info(f"ğŸ“‹ æ–°ä»»åŠ¡: {task}")
        logging.info(f"   è®°å¿†æ¨¡å¼: {use_memory} | Key: {memory_key}")
        logging.info(f"   å›¾ç‰‡æ•°é‡: {len(image_paths) if image_paths else 0}")
        logging.info(f"{'=' * 80}")

        print(f"\n{'=' * 80}")
        print(f"ğŸš€ ä»»åŠ¡å¼€å§‹: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"{'=' * 80}\n")

        if image_paths:
            image_paths = image_paths[:self.max_images]
            logging.info(f"ğŸ“· å¤„ç† {len(image_paths)} å¼ å›¾ç‰‡")

        history: List[Dict] = []

        # å›¾åƒå¤„ç†
        if image_paths:
            image_content = [{"type": "text", "text": task}]

            for idx, path in enumerate(image_paths, 1):
                if not os.path.exists(path):
                    logging.warning(f"âš ï¸ å›¾ç‰‡ä¸å­˜åœ¨: {path}")
                    continue

                try:
                    mime_type, _ = mimetypes.guess_type(path)
                    if not mime_type or not mime_type.startswith("image/"):
                        mime_type = "image/jpeg"

                    with open(path, "rb") as f:
                        base64_image = base64.b64encode(f.read()).decode('utf-8')

                    image_content.append({
                        "type": "image_url",
                        "image_url": {
                            "url": f"data:{mime_type};base64,{base64_image}"
                        }
                    })
                    logging.info(f"  âœ… å›¾ç‰‡ {idx}: {path} ({mime_type})")

                except Exception as e:
                    logging.error(f"  âŒ è¯»å–å›¾ç‰‡å¤±è´¥ {path}: {e}")
                    image_content.append({
                        "type": "text",
                        "text": f"[æ— æ³•è¯»å–å›¾ç‰‡ {idx}: {os.path.basename(path)}]"
                    })

            history.append({"speaker": "User", "content": image_content})
        else:
            history.append({"speaker": "User", "content": task})

        tracker.checkpoint("1ï¸âƒ£ åˆå§‹åŒ–")

        # åŠ è½½å†å²è®°å¿†
        if use_memory and memory_key in self.memory:
            memory_text = "\n".join([
                f"- {item['summary']}"
                for item in self.memory[memory_key][-5:]
            ])
            history.insert(0, {
                "speaker": "System",
                "content": f"ğŸ“š å†å²è®°å¿†ï¼ˆ{memory_key}ï¼‰ï¼š\n{memory_text}"
            })
            logging.info(f"ğŸ“– åŠ è½½å†å²è®°å¿†: {memory_key} ({len(self.memory[memory_key])} æ¡)")

        # ä¸»å¾ªç¯
        round_num = 0
        while True:
            round_num += 1

            if round_num > self.max_rounds:
                logging.warning(f"â±ï¸ è¾¾åˆ°æœ€å¤§è½®æ¬¡ {self.max_rounds}ï¼Œå¼ºåˆ¶ç»“æŸ")
                break

            logging.info(f"\n{'â”€' * 80}")
            logging.info(f"ğŸ”„ ç¬¬ {round_num} è½®è®¨è®ºå¼€å§‹")
            logging.info(f"{'â”€' * 80}")

            round_start = time.time()

            # âœ… ä½¿ç”¨ max_concurrent_agents é™åˆ¶å¹¶å‘æ•°
            with ThreadPoolExecutor(max_workers=self.max_concurrent_agents) as executor:
                future_to_agent = {
                    executor.submit(
                        agent.generate_response,
                        history.copy(),
                        round_num
                    ): agent
                    for agent in self.agents
                }

                for future in as_completed(future_to_agent):
                    agent = future_to_agent[future]
                    try:
                        contribution = future.result()
                        history.append({
                            "speaker": agent.name,
                            "content": contribution
                        })
                        logging.info(f"âœ… {agent.name} å®Œæˆç¬¬ {round_num} è½®")
                    except Exception as e:
                        logging.error(f"âŒ {agent.name} æ‰§è¡Œå¤±è´¥: {e}")
                        history.append({
                            "speaker": agent.name,
                            "content": f"[æ‰§è¡Œå¤±è´¥: {str(e)}]"
                        })

            round_elapsed = time.time() - round_start
            round_time_str = tracker.format_time(round_elapsed)
            print(f"\nâ±ï¸  ç¬¬ {round_num} è½®è®¨è®ºå®Œæˆ | è€—æ—¶: {round_time_str}\n")
            logging.info(f"â±ï¸  ç¬¬ {round_num} è½®è®¨è®ºè€—æ—¶: {round_time_str}")

            tracker.checkpoint(f"2ï¸âƒ£ ç¬¬{round_num}è½®è®¨è®º")

            # åæ€ä¸è§„åˆ’
            if self.mode == "intelligent" and self.reflection_planning:
                reflection_start = time.time()

                logging.info(f"\n{'â”€' * 80}")
                logging.info(f"ğŸ¤” Leader Multi-Round Reflection (ç¬¬ {round_num} è½®)")
                logging.info(f"{'â”€' * 80}")

                plan_prompt = (
                    "è¯·ä»¥ JSON æ ¼å¼è§„åˆ’ä¸‹ä¸€è½®çš„é‡ç‚¹æ–¹å‘ã€‚\n"
                    "æ ¼å¼: {\"focus_areas\": [\"æ–¹å‘1\", \"æ–¹å‘2\"], \"expected_improvement\": \"é¢„æœŸæ”¹è¿›\"}"
                )
                plan = self.leader.generate_response(
                    history + [{"speaker": "System", "content": plan_prompt}],
                    round_num,
                    force_non_stream=True
                )
                logging.info(f"ğŸ“‹ Plan: {plan[:200]}...")

                max_reflection_rounds = self.max_reflection_rounds
                final_decision = "continue"
                final_quality = 0
                previous_quality = 0

                for reflection_round in range(1, max_reflection_rounds + 1):
                    logging.info(f"\nğŸ” Reflection Round {reflection_round}/{max_reflection_rounds}")

                    if reflection_round == 1:
                        reflect_prompt = (
                            "è¯·åæ€æœ¬è½®è®¨è®ºç»“æœï¼Œç»™å‡ºè´¨é‡è¯„åˆ†å’Œå†³ç­–ã€‚\n"
                            "è¯„ä¼°æ ‡å‡†ï¼š\n"
                            "- ä¿¡æ¯å®Œæ•´æ€§ï¼ˆæ˜¯å¦è¦†ç›–å…³é”®ç‚¹ï¼‰\n"
                            "- é€»è¾‘ä¸¥å¯†æ€§ï¼ˆæ˜¯å¦æœ‰çŸ›ç›¾æˆ–è·³è·ƒï¼‰\n"
                            "- æ·±åº¦ä¸æ´å¯Ÿï¼ˆæ˜¯å¦æœ‰ç‹¬åˆ°è§è§£ï¼‰\n"
                            "JSON æ ¼å¼: {\"quality_score\": 1-10, \"decision\": \"continue/stop\", "
                            "\"reason\": \"åŸå› \", \"suggestions\": [\"å»ºè®®1\", \"å»ºè®®2\"]}"
                        )
                    elif reflection_round == 2:
                        reflect_prompt = (
                            f"è¿™æ˜¯ç¬¬ {reflection_round} æ¬¡æ·±åº¦åæ€ã€‚\n"
                            f"ä¸Šæ¬¡è¯„åˆ†ï¼š{final_quality}/10\n"
                            f"ä¸Šæ¬¡å»ºè®®ï¼šå·²åœ¨è®¨è®ºä¸­éƒ¨åˆ†ä½“ç°\n\n"
                            "è¯·æ›´æ·±å…¥åœ°åˆ†æï¼š\n"
                            "- æ˜¯å¦è¿˜æœ‰éšè—çš„é€»è¾‘æ¼æ´ï¼Ÿ\n"
                            "- è®ºæ®æ˜¯å¦å……åˆ†æ”¯æ’‘ç»“è®ºï¼Ÿ\n"
                            "- è¡¨è¾¾æ˜¯å¦æ¸…æ™°æ˜“æ‡‚ï¼Ÿ\n"
                            "JSON æ ¼å¼: {\"quality_score\": 1-10, \"decision\": \"continue/stop\", "
                            "\"reason\": \"åŸå› \", \"critical_issues\": [\"å…³é”®é—®é¢˜1\", \"å…³é”®é—®é¢˜2\"]}"
                        )
                    else:
                        reflect_prompt = (
                            f"è¿™æ˜¯ç¬¬ {reflection_round} æ¬¡ï¼ˆæœ€ç»ˆï¼‰åæ€ã€‚\n"
                            f"ä¸Šæ¬¡è¯„åˆ†ï¼š{final_quality}/10\n"
                            f"è´¨é‡æå‡å¹…åº¦ï¼š{final_quality - previous_quality if previous_quality > 0 else 'N/A'}\n\n"
                            "è¯·åšæœ€ç»ˆç»¼åˆåˆ¤æ–­ï¼š\n"
                            "- å½“å‰è´¨é‡æ˜¯å¦è¾¾åˆ°å¯äº¤ä»˜æ ‡å‡†ï¼Ÿ\n"
                            "- ç»§ç»­è®¨è®ºçš„è¾¹é™…æ”¶ç›Šå¦‚ä½•ï¼Ÿ\n"
                            "- æ˜¯å¦å­˜åœ¨è‡´å‘½ç¼ºé™·å¿…é¡»ä¿®å¤ï¼Ÿ\n"
                            "JSON æ ¼å¼: {\"quality_score\": 1-10, \"decision\": \"continue/stop\", "
                            "\"reason\": \"åŸå› \", \"final_verdict\": \"ç»¼åˆè¯„ä»·\"}"
                        )

                    leader_eval = self.leader.generate_response(
                        history + [{"speaker": "System", "content": reflect_prompt}],
                        round_num,
                        force_non_stream=True
                    )

                    logging.info(f"ğŸ’­ Reflection {reflection_round}: {leader_eval[:150]}...")

                    try:
                        eval_json = json.loads(
                            leader_eval.strip()
                            .replace("```json", "")
                            .replace("```", "")
                            .strip()
                        )

                        previous_quality = final_quality
                        final_quality = eval_json.get("quality_score", 0)
                        final_decision = eval_json.get("decision", "").lower()

                        logging.info(f"ğŸ“Š è´¨é‡è¯„åˆ†: {final_quality}/10 | å†³ç­–: {final_decision}")

                        if final_quality >= self.reflection_quality_threshold:
                            logging.info(f"âœ… è´¨é‡è¾¾åˆ° {final_quality}/10ï¼Œæ— éœ€ç»§ç»­åæ€")
                            break

                        if final_decision == "stop" and final_quality >= self.stop_quality_threshold:
                            logging.info(f"âœ… Leader åˆ¤æ–­è´¨é‡ {final_quality}/10 å¯æ¥å—ï¼Œåœæ­¢åæ€")
                            break

                        if reflection_round > 1 and previous_quality > 0:
                            quality_delta = final_quality - previous_quality
                            if abs(quality_delta) < self.quality_convergence_delta:
                                logging.info(f"ğŸ”´ è´¨é‡æå‡åœæ» (Î”={quality_delta:.1f})ï¼Œåœæ­¢åæ€")
                                break

                    except json.JSONDecodeError:
                        logging.warning(f"âš ï¸ åæ€ {reflection_round} JSON è§£æå¤±è´¥")
                        final_quality = max(final_quality, 5)
                        continue
                    except Exception as e:
                        logging.error(f"âŒ åæ€ {reflection_round} å¤„ç†å¤±è´¥: {e}")
                        continue

                reflection_elapsed = time.time() - reflection_start
                reflection_time_str = tracker.format_time(reflection_elapsed)
                print(f"â±ï¸  åæ€é˜¶æ®µå®Œæˆ | è€—æ—¶: {reflection_time_str}\n")
                logging.info(f"â±ï¸  åæ€é˜¶æ®µè€—æ—¶: {reflection_time_str}")

                tracker.checkpoint(f"3ï¸âƒ£ ç¬¬{round_num}è½®åæ€")

                if final_decision == "stop" and final_quality >= self.stop_quality_threshold:
                    logging.info(f"ğŸ¯ ç»è¿‡ {reflection_round} è½®åæ€ï¼Œè´¨é‡è¾¾åˆ° {final_quality}/10ï¼Œåœæ­¢è®¨è®º")
                    break
                else:
                    logging.info(f"ğŸ”„ è´¨é‡ {final_quality}/10ï¼Œç»§ç»­ä¸‹ä¸€è½®è®¨è®ºä¼˜åŒ–")

        # æœ€ç»ˆç»¼åˆ
        final_synthesis_start = time.time()

        logging.info(f"\n{'=' * 80}")
        logging.info("ğŸ¯ Leader æœ€ç»ˆç»¼åˆ")
        logging.info(f"{'=' * 80}")

        history.append({
            "speaker": "System",
            "content": (
                "è¯·ç»¼åˆä»¥ä¸Šå…¨éƒ¨è®¨è®ºï¼Œç»™å‡ºæœ€å‡†ç¡®ã€æœ€å®Œæ•´ã€æœ€é«˜è´¨é‡çš„æœ€ç»ˆç­”æ¡ˆã€‚\n"
                "è¦æ±‚ï¼š\n"
                "1. é€»è¾‘ä¸¥å¯†ï¼Œè®ºè¯å……åˆ†\n"
                "2. ä¿¡æ¯å®Œæ•´ï¼Œç»†èŠ‚ä¸°å¯Œ\n"
                "3. ç»“æ„æ¸…æ™°ï¼Œæ˜“äºç†è§£\n"
                "4. å¦‚æ¶‰åŠä»£ç æˆ–æ–‡ä»¶æ“ä½œï¼Œè¯·ç¡®ä¿å·²æ­£ç¡®æ‰§è¡Œ"
            )
        })

        final_answer = self.leader.generate_response(
            history,
            round_num + 1,
            force_non_stream=False
        )

        tracker.checkpoint("4ï¸âƒ£ æœ€ç»ˆç»¼åˆ")

        # ä¿å­˜è®°å¿†
        if use_memory:
            summary_prompt = (
                "è¯·ç”¨ 500 å­—ä»¥å†…æ€»ç»“æœ¬æ¬¡ä»»åŠ¡çš„ï¼š\n"
                "1. æ ¸å¿ƒç»“è®º\n"
                "2. å…³é”®å‘ç°\n"
                "3. å¯å¤ç”¨ç»éªŒ\n"
                "4. é—ç•™é—®é¢˜ï¼ˆå¦‚æœ‰ï¼‰"
            )
            summary = self.leader.generate_response(
                history + [{"speaker": "System", "content": summary_prompt}],
                round_num + 1,
                force_non_stream=True
            )
            self._save_memory(memory_key, summary)

            # å‘é‡è®°å¿†
            if self.vector_memory:
                self.vector_memory.add(
                    summary,
                    metadata={"task": task[:100], "memory_key": memory_key}
                )

            tracker.checkpoint("5ï¸âƒ£ ä¿å­˜è®°å¿†")

        # è¾“å‡ºæœ€ç»ˆç­”æ¡ˆ
        print("\n" + "=" * 100)
        print("ğŸ¯ ã€æœ€ç»ˆæœ€é«˜è´¨é‡ç­”æ¡ˆã€‘")
        print("=" * 100)
        print(final_answer)
        print("=" * 100)

        print(tracker.summary())
        logging.info(tracker.summary())

        logging.info(f"\n{'=' * 80}")
        logging.info("âœ… ä»»åŠ¡å®Œæˆ")
        logging.info(f"{'=' * 80}\n")

        return final_answer


# ====================== ä¸»å‡½æ•° ======================
if __name__ == "__main__":
    try:
        swarm = MultiAgentSwarm()

        # ç¤ºä¾‹1ï¼šåŸºç¡€ä»»åŠ¡
        # swarm.solve("è¯·å¸®æˆ‘åˆ†æä¸€ä¸‹äººå·¥æ™ºèƒ½çš„å‘å±•è¶‹åŠ¿")

        # ç¤ºä¾‹2ï¼šå¸¦è®°å¿†çš„æ·±åº¦æŠ¥å‘Š
        swarm.solve(
            "è¯·å¸®æˆ‘å†™ä¸€ç¯‡å…³äºäººå·¥æ™ºèƒ½çš„æ·±åº¦åˆ†ææŠ¥å‘Šï¼Œå¹¶ä¿å­˜åˆ° ./reports/ai_report.md",
            use_memory=True,
            memory_key="ai_topic"
        )

        # ç¤ºä¾‹3ï¼šå›¾åƒåˆ†æï¼ˆéœ€è¦æä¾›çœŸå®å›¾ç‰‡è·¯å¾„ï¼‰
        # swarm.solve(
        #     "è¯·åˆ†æè¿™äº›å›¾ç‰‡ä¸­çš„ä»£ç é—®é¢˜",
        #     image_paths=["./screenshot1.png", "./screenshot2.png"]
        # )

    except Exception as e:
        logging.error(f"âŒ ç¨‹åºå¼‚å¸¸: {e}", exc_info=True)