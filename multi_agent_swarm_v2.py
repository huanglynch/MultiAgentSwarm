import yaml
import logging
import os
import glob
import importlib.util
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import List, Dict, Callable
from openai import OpenAI
import json

# ====================== Skill åŠ¨æ€åŠ è½½å™¨ï¼ˆä¿æŒä¸å˜ï¼‰ ======================
def load_skills(skills_dir: str = "skill"):
    tool_registry: Dict[str, Dict] = {}
    shared_knowledge = ""

    if not os.path.exists(skills_dir):
        logging.warning(f"âš ï¸ skill/ ç›®å½•ä¸å­˜åœ¨ï¼Œå°†ä½¿ç”¨ç©ºå·¥å…·é›†")
        return tool_registry, shared_knowledge

    for py_file in glob.glob(os.path.join(skills_dir, "*.py")):
        if "__init__" in py_file: continue
        module_name = os.path.splitext(os.path.basename(py_file))[0]
        try:
            spec = importlib.util.spec_from_file_location(module_name, py_file)
            module = importlib.util.module_from_spec(spec)
            spec.loader.exec_module(module)
            if hasattr(module, "execute") and hasattr(module, "schema"):
                name = getattr(module, "name", module_name)
                tool_registry[name] = {"func": module.execute, "schema": module.schema}
                logging.info(f"âœ… åŠ è½½ Skill (py): {name}")
        except Exception as e:
            logging.error(f"åŠ è½½ Skill {py_file} å¤±è´¥: {e}")

    for md_file in glob.glob(os.path.join(skills_dir, "*.md")):
        try:
            with open(md_file, "r", encoding="utf-8") as f:
                content = f.read().strip()
                shared_knowledge += f"\n\n### æ¥è‡ª {os.path.basename(md_file)} ###\n{content}"
            logging.info(f"âœ… åŠ è½½çŸ¥è¯† (md): {os.path.basename(md_file)}")
        except Exception as e:
            logging.error(f"åŠ è½½çŸ¥è¯† {md_file} å¤±è´¥: {e}")

    return tool_registry, shared_knowledge


# ====================== Agent ç±»ï¼ˆä¿æŒä¸å˜ï¼‰ ======================
class Agent:
    def __init__(self, config: Dict, default_model: str, default_max_tokens: int, tool_registry: Dict, shared_knowledge: str = ""):
        self.name = config["name"]
        self.role = config["role"]
        self.shared_knowledge = shared_knowledge
        self.client = OpenAI(api_key=config.get("api_key"), base_url=config.get("base_url"))
        self.model = config.get("model", default_model)
        self.temperature = config.get("temperature", 0.7)
        self.stream = config.get("stream", False)
        self.max_tokens = config.get("max_tokens", default_max_tokens)
        enabled = config.get("enabled_tools", [])
        self.tools = [tool_registry[name]["schema"] for name in enabled if name in tool_registry]
        self.tool_map: Dict[str, Callable] = {name: tool_registry[name]["func"] for name in enabled if name in tool_registry}

    def _execute_tool(self, tool_call) -> Dict:
        # ï¼ˆä¿æŒä¸å˜ï¼Œçœç•¥ä»¥èŠ‚çœç¯‡å¹…ï¼Œå®é™…ä»£ç ä¸ v2.2 å®Œå…¨ç›¸åŒï¼‰
        func_name = tool_call.function.name
        try:
            args = json.loads(tool_call.function.arguments)
            result = self.tool_map[func_name](**args)
            return {"role": "tool", "tool_call_id": tool_call.id, "name": func_name, "content": str(result)}
        except Exception as e:
            return {"role": "tool", "tool_call_id": tool_call.id, "name": func_name, "content": f"Tool error: {str(e)}"}

    def generate_response(self, history: List[Dict], round_num: int, system_extra: str = "") -> str:
        system_prompt = f"{self.role}\n{self.shared_knowledge}\n{system_extra}\nä½ æ˜¯å¤šæ™ºèƒ½ä½“åä½œå›¢é˜Ÿçš„ä¸€å‘˜ï¼Œè¯·æä¾›æœ‰ä»·å€¼ã€å‡†ç¡®ã€æœ‰æ·±åº¦çš„è´¡çŒ®ã€‚"
        messages = [{"role": "system", "content": system_prompt}]
        for h in history:
            if h["speaker"] == "User":
                messages.append({"role": "user", "content": h["content"]})
            else:
                messages.append({"role": "user", "content": f"[{h['speaker']}] {h.get('content', '')}"})

        try:
            if self.stream and self.name == "Grok":  # åªåœ¨ Leader æµå¼æ˜¾ç¤ºæœ€ç»ˆç»¼åˆæ›´æ¸…æ™°
                print(f"\nã€{self.name}ã€‘æ­£åœ¨æ€è€ƒ... ", end="", flush=True)

            response = self.client.chat.completions.create(
                model=self.model, messages=messages, temperature=self.temperature,
                max_tokens=self.max_tokens, tools=self.tools if self.tools else None,
                tool_choice="auto" if self.tools else None, stream=self.stream,
            )

            full_response = ""
            if self.stream:
                for chunk in response:
                    if chunk.choices[0].delta.content:
                        delta = chunk.choices[0].delta.content
                        print(delta, end="", flush=True)
                        full_response += delta
                print()
            else:
                full_response = response.choices[0].message.content or ""

            # Tool Calling å•è½®ï¼ˆä¿æŒä¸å˜ï¼‰
            message_obj = response.choices[0].message
            if not self.stream and hasattr(message_obj, 'tool_calls') and message_obj.tool_calls:
                messages.append(message_obj.model_dump())
                for tool_call in message_obj.tool_calls:
                    tool_result = self._execute_tool(tool_call)
                    messages.append(tool_result)
                final_resp = self.client.chat.completions.create(model=self.model, messages=messages, temperature=self.temperature, max_tokens=self.max_tokens)
                full_response = final_resp.choices[0].message.content or ""

            return full_response.strip()

        except Exception as e:
            err = f"[Error in {self.name}]: {str(e)}"
            logging.error(err)
            return err


# ====================== ä¸»ç±» v2.3 ======================
class MultiAgentSwarm:
    def __init__(self, config_path: str = "swarm_config.yaml"):
        if not os.path.exists(config_path):
            raise FileNotFoundError(f"é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_path}")

        with open(config_path, "r", encoding="utf-8") as f:
            cfg = yaml.safe_load(f)

        oai = cfg.get("openai", {})
        self.default_model = oai.get("default_model", "gpt-4o-mini")
        self.default_max_tokens = oai.get("default_max_tokens", 4096)

        swarm = cfg.get("swarm", {})
        self.mode = swarm.get("mode", "fixed")                    # æ–°å¢ï¼šfixed / intelligent
        self.max_rounds = swarm.get("max_rounds", 3 if swarm.get("mode", "fixed") == "fixed" else 10)
        self.log_file = swarm.get("log_file", "swarm.log")
        self.skills_dir = swarm.get("skills_dir", "skill")

        # æ—¥å¿—è®¾ç½®ï¼ˆä¸å˜ï¼‰
        logging.basicConfig(filename=self.log_file, level=logging.INFO, format="%(asctime)s | %(levelname)s | %(message)s", encoding="utf-8", force=True)
        logging.getLogger().addHandler(logging.StreamHandler())
        logging.info(f"=== MultiAgentSwarm v2.3 åˆå§‹åŒ–å®Œæˆ | Mode: {self.mode} | Max Rounds: {self.max_rounds} ===")

        self.tool_registry, self.shared_knowledge = load_skills(self.skills_dir)

        self.agents = []
        for a_cfg in cfg.get("agents", [])[:swarm.get("num_agents", 4)]:
            agent = Agent(a_cfg, self.default_model, self.default_max_tokens, self.tool_registry, self.shared_knowledge)
            self.agents.append(agent)
            logging.info(f"âœ… Agent åŠ è½½: {agent.name} | Model: {agent.model}")

        self.leader = self.agents[0]

    def solve(self, task: str) -> str:
        logging.info(f"ã€æ–°ä»»åŠ¡å¯åŠ¨ã€‘{task} | æ¨¡å¼: {self.mode}")
        history: List[Dict] = [{"speaker": "User", "content": task}]
        round_num = 0

        while True:
            round_num += 1
            if round_num > self.max_rounds:
                logging.warning(f"è¾¾åˆ°æœ€å¤§è½®æ¬¡ä¸Šé™ {self.max_rounds}ï¼Œå¼ºåˆ¶ç»“æŸ")
                break

            logging.info(f"--- ç¬¬ {round_num} è½®å¹¶è¡Œè®¨è®ºå¼€å§‹ ---")
            with ThreadPoolExecutor(max_workers=len(self.agents)) as executor:
                future_to_agent = {executor.submit(agent.generate_response, history.copy(), round_num): agent for agent in self.agents}
                for future in as_completed(future_to_agent):
                    agent = future_to_agent[future]
                    contribution = future.result()
                    history.append({"speaker": agent.name, "content": contribution})

            # ==================== æ™ºèƒ½æ¨¡å¼æ ¸å¿ƒï¼šLeader æ™ºèƒ½è¯„ä»· ====================
            if self.mode == "intelligent":
                logging.info(f"--- Leader æ™ºèƒ½è¯„ä»·ç¬¬ {round_num} è½®è´¨é‡ ---")
                eval_prompt = (
                    "ä½ ç°åœ¨æ˜¯å›¢é˜Ÿè´¨é‡æ§åˆ¶å®˜ã€‚è¯·ä¸¥æ ¼è¯„ä¼°å½“å‰æ‰€æœ‰è®¨è®ºçš„è´¨é‡ã€‚\n"
                    "è¾“å‡ºå¿…é¡»æ˜¯åˆæ³• JSON å¯¹è±¡ï¼Œæ ¼å¼å¦‚ä¸‹ï¼š\n"
                    "{\n"
                    '  "quality_score": æ•´æ•°(1-10),\n'
                    '  "decision": "continue" æˆ– "stop",\n'
                    '  "reason": "ç®€çŸ­ç†ç”±",\n'
                    '  "suggestions": "å¦‚æœç»§ç»­ï¼Œä¸‹ä¸€æ­¥æ”¹è¿›ç‚¹"\n'
                    "}\n"
                    "è´¨é‡ >=8 ä¸”ä½ è®¤ä¸ºå·²è¶³å¤Ÿå®Œç¾æ—¶ decision=stopï¼Œå¹¶ç›´æ¥ç»™å‡ºæœ€ç»ˆé«˜è´¨é‡ç­”æ¡ˆã€‚"
                )
                eval_history = history + [{"speaker": "System", "content": eval_prompt}]
                leader_eval = self.leader.generate_response(eval_history, round_num, "è¯·ä»¥ JSON æ ¼å¼å›å¤ã€‚")

                try:
                    eval_json = json.loads(leader_eval.strip().strip("```json").strip("```"))
                    score = eval_json.get("quality_score", 5)
                    decision = eval_json.get("decision", "continue")
                    logging.info(f"Leader è¯„ä»·: åˆ†æ•°={score} | å†³ç­–={decision} | ç†ç”±={eval_json.get('reason')}")
                    if decision.lower() == "stop":
                        logging.info("âœ… Leader åˆ¤æ–­å·²è¾¾æœ€é«˜è´¨é‡ï¼Œåœæ­¢è®¨è®º")
                        # æœ€ç»ˆç­”æ¡ˆå·²åœ¨ eval_json æˆ–ç›´æ¥ç”¨ leader_eval åçš„å†…å®¹
                        final_answer = eval_json.get("final_answer") or leader_eval
                        break
                except:
                    logging.warning("JSON è§£æå¤±è´¥ï¼Œç»§ç»­ä¸‹ä¸€è½®")
                    continue

            # å›ºå®šæ¨¡å¼æˆ–æœªåœæ­¢æ—¶ç»§ç»­
            if self.mode == "fixed" and round_num >= self.max_rounds:
                break

        # Leader æœ€ç»ˆç»¼åˆï¼ˆæ™ºèƒ½æ¨¡å¼ä¸‹å¯èƒ½å·²åœ¨ eval ä¸­å®Œæˆï¼‰
        if self.mode == "fixed" or "final_answer" not in locals():
            logging.info("--- Leader æœ€ç»ˆç»¼åˆ ---")
            history.append({"speaker": "System", "content": "è¯·ç»¼åˆä»¥ä¸Šå…¨éƒ¨è®¨è®ºï¼Œç»™å‡ºæœ€å‡†ç¡®ã€æœ€å®Œæ•´ã€æœ€é«˜è´¨é‡çš„æœ€ç»ˆç­”æ¡ˆã€‚"})
            final_answer = self.leader.generate_response(history, round_num + 1)

        print("\n" + "="*100)
        print("ğŸ¯ ã€æœ€ç»ˆæœ€é«˜è´¨é‡ç­”æ¡ˆã€‘")
        print(final_answer)
        print("="*100)
        logging.info("âœ… ä»»åŠ¡å®Œæˆ")
        return final_answer


if __name__ == "__main__":
    swarm = MultiAgentSwarm()
    swarm.solve("è¯·å¸®æˆ‘å†™ä¸€ç¯‡å…³äºã€2026 å¹´ä¸œäº¬äººå·¥æ™ºèƒ½äº§ä¸šè¶‹åŠ¿ã€çš„æ·±åº¦æŠ¥å‘Šï¼Œå¹¶ä¿å­˜åˆ° ./reports/tokyo_ai_2026.md")
