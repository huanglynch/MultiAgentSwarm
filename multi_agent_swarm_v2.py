import yaml
import logging
import os
import glob
import importlib.util
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import List, Dict, Callable
from openai import OpenAI
import json

# ====================== Skill åŠ¨æ€åŠ è½½å™¨ ======================
def load_skills(skills_dir: str = "skill"):
    tool_registry: Dict[str, Dict] = {}
    shared_knowledge = ""

    if not os.path.exists(skills_dir):
        logging.warning(f"âš ï¸ skill/ ç›®å½•ä¸å­˜åœ¨ï¼ˆ{skills_dir}ï¼‰ï¼Œå°†ä½¿ç”¨ç©ºå·¥å…·é›†")
        return tool_registry, shared_knowledge

    # 1. åŠ è½½ .py æ–‡ä»¶ â†’ å¯æ‰§è¡Œ Tool
    for py_file in glob.glob(os.path.join(skills_dir, "*.py")):
        if "__init__" in py_file:
            continue
        module_name = os.path.splitext(os.path.basename(py_file))[0]
        try:
            spec = importlib.util.spec_from_file_location(module_name, py_file)
            module = importlib.util.module_from_spec(spec)
            spec.loader.exec_module(module)

            if hasattr(module, "execute") and hasattr(module, "schema"):
                name = getattr(module, "name", module_name)
                tool_registry[name] = {
                    "func": module.execute,
                    "schema": module.schema
                }
                logging.info(f"âœ… åŠ è½½ Skill (py): {name}")
        except Exception as e:
            logging.error(f"åŠ è½½ Skill {py_file} å¤±è´¥: {e}")

    # 2. åŠ è½½ .md æ–‡ä»¶ â†’ å…±äº«çŸ¥è¯†
    for md_file in glob.glob(os.path.join(skills_dir, "*.md")):
        try:
            with open(md_file, "r", encoding="utf-8") as f:
                content = f.read().strip()
                shared_knowledge += f"\n\n### æ¥è‡ª {os.path.basename(md_file)} ###\n{content}"
            logging.info(f"âœ… åŠ è½½çŸ¥è¯† (md): {os.path.basename(md_file)}")
        except Exception as e:
            logging.error(f"åŠ è½½çŸ¥è¯† {md_file} å¤±è´¥: {e}")

    return tool_registry, shared_knowledge


# ====================== Agent ç±» ======================
class Agent:
    def __init__(self, config: Dict, default_model: str, default_max_tokens: int, tool_registry: Dict, shared_knowledge: str = ""):
        self.name = config["name"]
        self.role = config["role"]
        self.shared_knowledge = shared_knowledge

        self.client = OpenAI(
            api_key=config.get("api_key"),
            base_url=config.get("base_url")
        )
        self.model = config.get("model", default_model)
        self.temperature = config.get("temperature", 0.7)
        self.stream = config.get("stream", False)
        self.max_tokens = config.get("max_tokens", default_max_tokens)

        # å·¥å…·ï¼ˆåªå¯ç”¨ yaml ä¸­å£°æ˜çš„ï¼‰
        enabled = config.get("enabled_tools", [])
        self.tools = [tool_registry[name]["schema"] for name in enabled if name in tool_registry]
        self.tool_map: Dict[str, Callable] = {name: tool_registry[name]["func"] for name in enabled if name in tool_registry}

    def _execute_tool(self, tool_call) -> Dict:
        func_name = tool_call.function.name
        try:
            args = json.loads(tool_call.function.arguments)
            result = self.tool_map[func_name](**args)
            return {"role": "tool", "tool_call_id": tool_call.id, "name": func_name, "content": str(result)}
        except Exception as e:
            return {"role": "tool", "tool_call_id": tool_call.id, "name": func_name, "content": f"Tool error: {str(e)}"}

    def generate_response(self, history: List[Dict], round_num: int) -> str:
        system_prompt = f"{self.role}\n{self.shared_knowledge}\nä½ æ˜¯å¤šæ™ºèƒ½ä½“åä½œå›¢é˜Ÿçš„ä¸€å‘˜ï¼Œè¯·æä¾›æœ‰ä»·å€¼ã€å‡†ç¡®ã€æœ‰æ·±åº¦çš„è´¡çŒ®ã€‚"
        messages = [{"role": "system", "content": system_prompt}]
        for h in history:
            if h["speaker"] == "User":
                messages.append({"role": "user", "content": h["content"]})
            else:
                messages.append({"role": "user", "content": f"[{h['speaker']}] {h.get('content', '')}"})

        try:
            if self.stream:
                print(f"\nã€{self.name}ã€‘æ­£åœ¨æ€è€ƒ... ", end="", flush=True)

            response = self.client.chat.completions.create(
                model=self.model,
                messages=messages,
                temperature=self.temperature,
                max_tokens=self.max_tokens,
                tools=self.tools if self.tools else None,
                tool_choice="auto" if self.tools else None,
                stream=self.stream,
            )

            full_response = ""
            if self.stream:
                for chunk in response:
                    if chunk.choices[0].delta.content:
                        delta = chunk.choices[0].delta.content
                        print(delta, end="", flush=True)
                        full_response += delta
                print()
            else:
                full_response = response.choices[0].message.content or ""

            # Tool Callingï¼ˆå•è½®ï¼‰
            message_obj = response.choices[0].message
            if not self.stream and hasattr(message_obj, 'tool_calls') and message_obj.tool_calls:
                messages.append(message_obj.model_dump())
                for tool_call in message_obj.tool_calls:
                    tool_result = self._execute_tool(tool_call)
                    messages.append(tool_result)
                    logging.info(f"[{self.name}] æ‰§è¡Œå·¥å…·: {tool_call.function.name}")

                final_resp = self.client.chat.completions.create(
                    model=self.model,
                    messages=messages,
                    temperature=self.temperature,
                    max_tokens=self.max_tokens,
                )
                full_response = final_resp.choices[0].message.content or ""

            logging.info(f"[Round {round_num}] {self.name} å®Œæˆ")
            return full_response.strip()

        except Exception as e:
            err = f"[Error in {self.name}]: {str(e)}"
            logging.error(err)
            return err


# ====================== ä¸»ç±» ======================
class MultiAgentSwarm:
    def __init__(self, config_path: str = "swarm_config.yaml"):
        if not os.path.exists(config_path):
            raise FileNotFoundError(f"é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_path}")

        with open(config_path, "r", encoding="utf-8") as f:
            cfg = yaml.safe_load(f)

        oai = cfg.get("openai", {})
        self.default_model = oai.get("default_model", "gpt-4o-mini")
        self.default_max_tokens = oai.get("default_max_tokens", 4096)

        swarm = cfg.get("swarm", {})
        self.num_agents = swarm.get("num_agents", 4)
        self.max_rounds = swarm.get("max_rounds", 3)
        self.log_file = swarm.get("log_file", "swarm.log")
        self.skills_dir = swarm.get("skills_dir", "skill")   # â† æ–°å¢

        # æ—¥å¿—
        logging.basicConfig(filename=self.log_file, level=logging.INFO,
                            format="%(asctime)s | %(levelname)s | %(message)s",
                            encoding="utf-8", force=True)
        logging.getLogger().addHandler(logging.StreamHandler())

        logging.info("=== MultiAgentSwarm v2.2 (Skill ç‹¬ç«‹ç›®å½•) åˆå§‹åŒ– ===")

        # åŠ¨æ€åŠ è½½ Skill
        self.tool_registry, self.shared_knowledge = load_skills(self.skills_dir)
        logging.info(f"å…±åŠ è½½ {len(self.tool_registry)} ä¸ªå¯æ‰§è¡Œ Skillï¼ŒçŸ¥è¯†åº“é•¿åº¦ {len(self.shared_knowledge)} å­—ç¬¦")

        # åŠ è½½ Agent
        self.agents: List[Agent] = []
        for a_cfg in cfg.get("agents", [])[:self.num_agents]:
            agent = Agent(a_cfg, self.default_model, self.default_max_tokens,
                          self.tool_registry, self.shared_knowledge)
            self.agents.append(agent)
            logging.info(f"âœ… Agent åŠ è½½: {agent.name} | Model: {agent.model} | max_tokens: {agent.max_tokens}")

        self.leader = self.agents[0]

    def solve(self, task: str) -> str:
        logging.info(f"ã€æ–°ä»»åŠ¡ã€‘{task}")
        history: List[Dict] = [{"speaker": "User", "content": task}]

        for r in range(1, self.max_rounds + 1):
            logging.info(f"--- ç¬¬ {r} è½®å¹¶è¡Œè®¨è®ºå¼€å§‹ ---")
            with ThreadPoolExecutor(max_workers=len(self.agents)) as executor:
                future_to_agent = {executor.submit(agent.generate_response, history.copy(), r): agent for agent in self.agents}
                for future in as_completed(future_to_agent):
                    agent = future_to_agent[future]
                    try:
                        contribution = future.result()
                        history.append({"speaker": agent.name, "content": contribution})
                    except Exception as e:
                        logging.error(f"{agent.name} å¼‚å¸¸: {e}")

        logging.info("--- Leader æœ€ç»ˆç»¼åˆ ---")
        history.append({"speaker": "System", "content": "è¯·ç»¼åˆä»¥ä¸Šå…¨éƒ¨è®¨è®ºï¼Œç»™å‡ºæœ€å‡†ç¡®ã€æœ€å®Œæ•´ã€æœ€ä¼˜çš„æœ€ç»ˆç­”æ¡ˆã€‚"})
        final_answer = self.leader.generate_response(history, self.max_rounds + 1)

        print("\n" + "="*90)
        print("ğŸ¯ ã€æœ€ç»ˆç­”æ¡ˆã€‘")
        print(final_answer)
        print("="*90)
        return final_answer


if __name__ == "__main__":
    swarm = MultiAgentSwarm()
    swarm.solve("è¯·è¯»å– skill/knowledge.md ä¸­çš„å†…å®¹ï¼Œç„¶åå¸®æˆ‘å†™ä¸€ç¯‡å…³äºäººå·¥æ™ºèƒ½çš„çŸ­æ–‡ï¼Œå¹¶ä¿å­˜åˆ° ./output/ai_essay.md")
