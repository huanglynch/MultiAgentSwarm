import yaml
import logging
import os
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import List, Dict, Callable
from openai import OpenAI
import json

# ====================== é»˜è®¤å†…ç½® Skill ======================
def read_file(path: str) -> str:
    """è¯»å–æœ¬åœ°æ–‡ä»¶å†…å®¹"""
    try:
        with open(path, 'r', encoding='utf-8') as f:
            return f.read()
    except Exception as e:
        return f"è¯»å–å¤±è´¥: {str(e)}"

def write_file(path: str, content: str) -> str:
    """å†™å…¥å†…å®¹åˆ°æœ¬åœ°æ–‡ä»¶ï¼ˆè‡ªåŠ¨åˆ›å»ºç›®å½•ï¼‰"""
    try:
        os.makedirs(os.path.dirname(path) or '.', exist_ok=True)
        with open(path, 'w', encoding='utf-8') as f:
            f.write(content)
        return f"æ–‡ä»¶å·²æˆåŠŸå†™å…¥: {path}"
    except Exception as e:
        return f"å†™å…¥å¤±è´¥: {str(e)}"

def list_dir(path: str = ".") -> str:
    """åˆ—å‡ºç›®å½•ä¸‹çš„æ–‡ä»¶å’Œæ–‡ä»¶å¤¹"""
    try:
        items = os.listdir(path)
        return "\n".join([f"ğŸ“„ {item}" if os.path.isfile(os.path.join(path, item)) else f"ğŸ“ {item}/" for item in items])
    except Exception as e:
        return f"åˆ—ç›®å½•å¤±è´¥: {str(e)}"

DEFAULT_TOOLS = {
    "read_file": {
        "func": read_file,
        "schema": {
            "type": "function",
            "function": {
                "name": "read_file",
                "description": "è¯»å–æŒ‡å®šè·¯å¾„çš„æœ¬åœ°æ–‡ä»¶å†…å®¹",
                "parameters": {
                    "type": "object",
                    "properties": {"path": {"type": "string", "description": "æ–‡ä»¶è·¯å¾„"}},
                    "required": ["path"]
                }
            }
        }
    },
    "write_file": {
        "func": write_file,
        "schema": {
            "type": "function",
            "function": {
                "name": "write_file",
                "description": "å°†å†…å®¹å†™å…¥æŒ‡å®šè·¯å¾„çš„æ–‡ä»¶ï¼ˆè‡ªåŠ¨åˆ›å»ºç›®å½•ï¼‰",
                "parameters": {
                    "type": "object",
                    "properties": {
                        "path": {"type": "string", "description": "æ–‡ä»¶è·¯å¾„"},
                        "content": {"type": "string", "description": "è¦å†™å…¥çš„å†…å®¹"}
                    },
                    "required": ["path", "content"]
                }
            }
        }
    },
    "list_dir": {
        "func": list_dir,
        "schema": {
            "type": "function",
            "function": {
                "name": "list_dir",
                "description": "åˆ—å‡ºæŒ‡å®šç›®å½•ä¸‹çš„æ–‡ä»¶å’Œæ–‡ä»¶å¤¹",
                "parameters": {
                    "type": "object",
                    "properties": {"path": {"type": "string", "description": "ç›®å½•è·¯å¾„ï¼Œé»˜è®¤ä¸ºå½“å‰ç›®å½•"}},
                    "required": []
                }
            }
        }
    }
}

# ====================== Agent ç±» ======================
class Agent:
    def __init__(self, config: Dict, default_model: str, default_max_tokens: int):
        self.name = config["name"]
        self.role = config["role"]
        
        self.client = OpenAI(
            api_key=config.get("api_key"),
            base_url=config.get("base_url")
        )
        self.model = config.get("model", default_model)
        self.temperature = config.get("temperature", 0.7)
        self.stream = config.get("stream", False)
        self.max_tokens = config.get("max_tokens", default_max_tokens)
        
        enabled = config.get("enabled_tools", [])
        self.tools = [DEFAULT_TOOLS[name]["schema"] for name in enabled if name in DEFAULT_TOOLS]
        self.tool_map: Dict[str, Callable] = {name: DEFAULT_TOOLS[name]["func"] for name in enabled if name in DEFAULT_TOOLS}

    def _execute_tool(self, tool_call) -> Dict:
        func_name = tool_call.function.name
        try:
            args = json.loads(tool_call.function.arguments)
            func = self.tool_map.get(func_name)
            if func:
                result = func(**args)
                return {
                    "role": "tool",
                    "tool_call_id": tool_call.id,
                    "name": func_name,
                    "content": str(result)
                }
        except Exception as e:
            return {"role": "tool", "tool_call_id": tool_call.id, "name": func_name, "content": f"Tool error: {str(e)}"}
        return {"role": "tool", "content": "Tool not found"}

    def generate_response(self, history: List[Dict], round_num: int) -> str:
        messages = [{"role": "system", "content": f"{self.role}\nä½ æ˜¯å¤šæ™ºèƒ½ä½“åä½œå›¢é˜Ÿçš„ä¸€å‘˜ï¼Œè¯·æä¾›æœ‰ä»·å€¼ã€å‡†ç¡®ã€æœ‰æ·±åº¦çš„è´¡çŒ®ã€‚"}]
        for h in history:
            if h["speaker"] == "User":
                messages.append({"role": "user", "content": h["content"]})
            else:
                messages.append({"role": "user", "content": f"[{h['speaker']}] {h.get('content', '')}"})

        try:
            if self.stream:
                print(f"\nã€{self.name}ã€‘æ­£åœ¨æ€è€ƒ... ", end="", flush=True)

            response = self.client.chat.completions.create(
                model=self.model,
                messages=messages,
                temperature=self.temperature,
                max_tokens=self.max_tokens,
                tools=self.tools if self.tools else None,
                tool_choice="auto" if self.tools else None,
                stream=self.stream,
            )

            full_response = ""
            if self.stream:
                for chunk in response:
                    if chunk.choices[0].delta.content:
                        delta = chunk.choices[0].delta.content
                        print(delta, end="", flush=True)
                        full_response += delta
                print()
            else:
                full_response = response.choices[0].message.content or ""

            # Tool Callingï¼ˆå•è½®ï¼Œæœ€å¯é ï¼‰
            message_obj = response.choices[0].message
            if not self.stream and hasattr(message_obj, 'tool_calls') and message_obj.tool_calls:
                messages.append(message_obj.model_dump())
                for tool_call in message_obj.tool_calls:
                    tool_result = self._execute_tool(tool_call)
                    messages.append(tool_result)
                    logging.info(f"[{self.name}] æ‰§è¡Œå·¥å…·: {tool_call.function.name}")
                
                # Tool æ‰§è¡Œåå†æ¬¡è°ƒç”¨å¾—åˆ°æœ€ç»ˆå›ç­”
                final_resp = self.client.chat.completions.create(
                    model=self.model,
                    messages=messages,
                    temperature=self.temperature,
                    max_tokens=self.max_tokens,
                )
                full_response = final_resp.choices[0].message.content or ""

            logging.info(f"[Round {round_num}] {self.name} å®Œæˆè´¡çŒ®")
            return full_response.strip()

        except Exception as e:
            err_msg = f"[Error in {self.name}]: {str(e)}"
            logging.error(err_msg)
            return err_msg

# ====================== ä¸»ç±» ======================
class MultiAgentSwarm:
    def __init__(self, config_path: str = "swarm_config.yaml"):
        if not os.path.exists(config_path):
            raise FileNotFoundError(f"é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_path}ï¼ˆè¯·æ ¹æ®ä¸‹æ–¹ç¤ºä¾‹åˆ›å»ºï¼‰")

        with open(config_path, "r", encoding="utf-8") as f:
            cfg = yaml.safe_load(f)

        oai = cfg.get("openai", {})
        self.default_model = oai.get("default_model", "gpt-4o-mini")
        self.default_max_tokens = oai.get("default_max_tokens", 4096)

        swarm = cfg.get("swarm", {})
        self.num_agents = swarm.get("num_agents", 4)
        self.max_rounds = swarm.get("max_rounds", 3)
        self.log_file = swarm.get("log_file", "swarm.log")

        # æ—¥å¿—
        logging.basicConfig(
            filename=self.log_file,
            level=logging.INFO,
            format="%(asctime)s | %(levelname)s | %(message)s",
            encoding="utf-8",
            force=True
        )
        console = logging.StreamHandler()
        console.setLevel(logging.INFO)
        logging.getLogger().addHandler(console)

        logging.info("=== MultiAgentSwarm v2.1 åˆå§‹åŒ–å®Œæˆ ===")

        self.agents: List[Agent] = []
        for a_cfg in cfg.get("agents", [])[:self.num_agents]:
            agent = Agent(a_cfg, self.default_model, self.default_max_tokens)
            self.agents.append(agent)
            logging.info(f"âœ… Agent åŠ è½½: {agent.name} | Model: {agent.model} | max_tokens: {agent.max_tokens} | Stream: {agent.stream}")

        if not self.agents:
            raise ValueError("è‡³å°‘éœ€è¦é…ç½® 1 ä¸ª Agent")
        self.leader = self.agents[0]

    def solve(self, task: str) -> str:
        logging.info(f"ã€æ–°ä»»åŠ¡å¯åŠ¨ã€‘{task}")
        history: List[Dict] = [{"speaker": "User", "content": task}]

        for r in range(1, self.max_rounds + 1):
            logging.info(f"--- ç¬¬ {r} è½®å¹¶è¡Œè®¨è®ºå¼€å§‹ ---")
            with ThreadPoolExecutor(max_workers=len(self.agents)) as executor:
                future_to_agent = {executor.submit(agent.generate_response, history.copy(), r): agent for agent in self.agents}
                for future in as_completed(future_to_agent):
                    agent = future_to_agent[future]
                    try:
                        contribution = future.result()
                        history.append({"speaker": agent.name, "content": contribution})
                    except Exception as e:
                        logging.error(f"{agent.name} æ‰§è¡Œå¼‚å¸¸: {e}")

        # Leader æœ€ç»ˆç»¼åˆ
        logging.info("--- Leader æœ€ç»ˆç»¼åˆ ---")
        history.append({"speaker": "System", "content": "è¯·ç»¼åˆä»¥ä¸Šæ‰€æœ‰æ™ºèƒ½ä½“çš„è®¨è®ºï¼Œç»™å‡ºæœ€å‡†ç¡®ã€æœ€å®Œæ•´ã€æœ€ä¼˜çš„æœ€ç»ˆç­”æ¡ˆã€‚"})
        
        final_answer = self.leader.generate_response(history, self.max_rounds + 1)

        print("\n" + "="*90)
        print("ğŸ¯ ã€æœ€ç»ˆç­”æ¡ˆã€‘")
        print(final_answer)
        print("="*90)

        logging.info("âœ… ä»»åŠ¡å®Œæˆ")
        return final_answer


if __name__ == "__main__":
    swarm = MultiAgentSwarm()
    swarm.solve("è¯·å¸®æˆ‘å†™ä¸€ç¯‡å…³äºã€Œäººå·¥æ™ºèƒ½å¦‚ä½•æ”¹å˜è½¯ä»¶å¼€å‘ã€çš„æ·±åº¦åˆ†ææŠ¥å‘Šï¼Œå¹¶ä¿å­˜åˆ° ./reports/ai_impact_report.md")
