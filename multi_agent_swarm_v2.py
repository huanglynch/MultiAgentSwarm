import yaml
import logging
import os
import glob
import importlib.util
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import List, Dict, Callable
from openai import OpenAI
import json
from datetime import datetime

# ====================== Skill åŠ¨æ€åŠ è½½å™¨ ======================
def load_skills(skills_dir: str = "skills"):
    tool_registry: Dict[str, Dict] = {}
    shared_knowledge = ""

    if not os.path.exists(skills_dir):
        logging.warning(f"âš ï¸ skills/ ç›®å½•ä¸å­˜åœ¨ï¼Œå°†ä½¿ç”¨ç©ºå·¥å…·é›†")
        return tool_registry, shared_knowledge

    for py_file in glob.glob(os.path.join(skills_dir, "*.py")):
        if "__init__" in py_file: continue
        module_name = os.path.splitext(os.path.basename(py_file))[0]
        try:
            spec = importlib.util.spec_from_file_location(module_name, py_file)
            module = importlib.util.module_from_spec(spec)
            spec.loader.exec_module(module)
            if hasattr(module, "execute") and hasattr(module, "schema"):
                name = getattr(module, "name", module_name)
                tool_registry[name] = {"func": module.execute, "schema": module.schema}
                logging.info(f"âœ… åŠ è½½ Skill (py): {name}")
        except Exception as e:
            logging.error(f"åŠ è½½ Skill {py_file} å¤±è´¥: {e}")

    for md_file in glob.glob(os.path.join(skills_dir, "*.md")):
        try:
            with open(md_file, "r", encoding="utf-8") as f:
                content = f.read().strip()
                shared_knowledge += f"\n\n### æ¥è‡ª {os.path.basename(md_file)} ###\n{content}"
            logging.info(f"âœ… åŠ è½½çŸ¥è¯† (md): {os.path.basename(md_file)}")
        except Exception as e:
            logging.error(f"åŠ è½½çŸ¥è¯† {md_file} å¤±è´¥: {e}")

    return tool_registry, shared_knowledge


# ====================== Agent ç±» ======================
class Agent:
    def __init__(self, config: Dict, default_model: str, default_max_tokens: int, tool_registry: Dict, shared_knowledge: str = ""):
        self.name = config["name"]
        self.role = config["role"]
        self.shared_knowledge = shared_knowledge

        self.client = OpenAI(
            api_key=config.get("api_key"),
            base_url=config.get("base_url")
        )
        self.model = config.get("model", default_model)
        self.temperature = config.get("temperature", 0.7)
        self.stream = config.get("stream", False)
        self.max_tokens = config.get("max_tokens", default_max_tokens)

        enabled = config.get("enabled_tools", [])
        self.tools = [tool_registry[name]["schema"] for name in enabled if name in tool_registry]
        self.tool_map: Dict[str, Callable] = {name: tool_registry[name]["func"] for name in enabled if name in tool_registry}

    def _execute_tool(self, tool_call) -> Dict:
        func_name = tool_call.function.name
        try:
            args = json.loads(tool_call.function.arguments)
            result = self.tool_map[func_name](**args)
            return {"role": "tool", "tool_call_id": tool_call.id, "name": func_name, "content": str(result)}
        except Exception as e:
            return {"role": "tool", "tool_call_id": tool_call.id, "name": func_name, "content": f"Tool error: {str(e)}"}

    def generate_response(self, history: List[Dict], round_num: int, system_extra: str = "") -> str:
        system_prompt = f"{self.role}\n{self.shared_knowledge}\n{system_extra}\nä½ æ˜¯å¤šæ™ºèƒ½ä½“åä½œå›¢é˜Ÿçš„ä¸€å‘˜ï¼Œè¯·æä¾›æœ‰ä»·å€¼ã€å‡†ç¡®ã€æœ‰æ·±åº¦çš„è´¡çŒ®ã€‚"
        messages = [{"role": "system", "content": system_prompt}]
        for h in history:
            if h["speaker"] == "User":
                messages.append({"role": "user", "content": h["content"]})
            else:
                messages.append({"role": "user", "content": f"[{h['speaker']}] {h.get('content', '')}"})

        try:
            if self.stream:
                print(f"\nã€{self.name}ã€‘æ­£åœ¨æ€è€ƒ... ", end="", flush=True)

            response = self.client.chat.completions.create(
                model=self.model,
                messages=messages,
                temperature=self.temperature,
                max_tokens=self.max_tokens,
                tools=self.tools if self.tools else None,
                tool_choice="auto" if self.tools else None,
                stream=self.stream,
            )

            full_response = ""
            if self.stream:
                for chunk in response:
                    if chunk.choices[0].delta.content:
                        delta = chunk.choices[0].delta.content
                        print(delta, end="", flush=True)
                        full_response += delta
                print()
            else:
                full_response = response.choices[0].message.content or ""

            # Tool Calling å•è½®
            message_obj = response.choices[0].message
            if not self.stream and hasattr(message_obj, 'tool_calls') and message_obj.tool_calls:
                messages.append(message_obj.model_dump())
                for tool_call in message_obj.tool_calls:
                    tool_result = self._execute_tool(tool_call)
                    messages.append(tool_result)
                final_resp = self.client.chat.completions.create(
                    model=self.model,
                    messages=messages,
                    temperature=self.temperature,
                    max_tokens=self.max_tokens,
                )
                full_response = final_resp.choices[0].message.content or ""

            return full_response.strip()

        except Exception as e:
            err = f"[Error in {self.name}]: {str(e)}"
            logging.error(err)
            return err


# ====================== ä¸»ç±» v2.4 ======================
class MultiAgentSwarm:
    def __init__(self, config_path: str = "swarm_config.yaml"):
        if not os.path.exists(config_path):
            raise FileNotFoundError(f"é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_path}")

        with open(config_path, "r", encoding="utf-8") as f:
            cfg = yaml.safe_load(f)

        oai = cfg.get("openai", {})
        self.default_model = oai.get("default_model", "gpt-4o-mini")
        self.default_max_tokens = oai.get("default_max_tokens", 4096)

        swarm = cfg.get("swarm", {})
        self.mode = swarm.get("mode", "fixed")
        self.max_rounds = swarm.get("max_rounds", 3 if swarm.get("mode", "fixed") == "fixed" else 10)
        self.log_file = swarm.get("log_file", "swarm.log")
        self.skills_dir = swarm.get("skills_dir", "skills")
        self.memory_file = swarm.get("memory_file", "memory.json")      # æ–°å¢
        self.max_memory_items = swarm.get("max_memory_items", 50)       # æ–°å¢

        # æ—¥å¿—
        logging.basicConfig(filename=self.log_file, level=logging.INFO,
                            format="%(asctime)s | %(levelname)s | %(message)s",
                            encoding="utf-8", force=True)
        logging.getLogger().addHandler(logging.StreamHandler())

        logging.info(f"=== MultiAgentSwarm v2.4 åˆå§‹åŒ–å®Œæˆ | Mode: {self.mode} ===")

        # åŠ è½½ skills
        self.tool_registry, self.shared_knowledge = load_skills(self.skills_dir)

        # åŠ è½½è®°å¿†
        self.memory = self._load_memory()

        # åŠ è½½ Agent
        self.agents = []
        for a_cfg in cfg.get("agents", [])[:swarm.get("num_agents", 4)]:
            agent = Agent(a_cfg, self.default_model, self.default_max_tokens, self.tool_registry, self.shared_knowledge)
            self.agents.append(agent)
            logging.info(f"âœ… Agent åŠ è½½: {agent.name}")

        self.leader = self.agents[0]

    def _load_memory(self) -> Dict:
        if os.path.exists(self.memory_file):
            try:
                with open(self.memory_file, "r", encoding="utf-8") as f:
                    return json.load(f)
            except:
                return {}
        return {}

    def _save_memory(self, key: str, summary: str):
        if key not in self.memory:
            self.memory[key] = []
        self.memory[key].append({
            "timestamp": datetime.now().isoformat(),
            "summary": summary[:3000]
        })
        if len(self.memory[key]) > self.max_memory_items:
            self.memory[key] = self.memory[key][-self.max_memory_items:]
        with open(self.memory_file, "w", encoding="utf-8") as f:
            json.dump(self.memory, f, ensure_ascii=False, indent=2)

    def solve(self, task: str, use_memory: bool = False, memory_key: str = "default") -> str:
        logging.info(f"ã€æ–°ä»»åŠ¡ã€‘{task} | è®°å¿†æ¨¡å¼: {use_memory} | key: {memory_key}")
        history: List[Dict] = [{"speaker": "User", "content": task}]

        # åŠ è½½è®°å¿†
        if use_memory and memory_key in self.memory:
            memory_text = "\n".join([item["summary"] for item in self.memory[memory_key][-5:]])
            history.insert(0, {"speaker": "System", "content": f"å†å²è®°å¿†ï¼ˆ{memory_key}ï¼‰ï¼š\n{memory_text}"})

        round_num = 0
        while True:
            round_num += 1
            if round_num > self.max_rounds:
                logging.warning(f"è¾¾åˆ°æœ€å¤§è½®æ¬¡ {self.max_rounds}ï¼Œå¼ºåˆ¶ç»“æŸ")
                break

            logging.info(f"--- ç¬¬ {round_num} è½®å¹¶è¡Œè®¨è®ºå¼€å§‹ ---")
            with ThreadPoolExecutor(max_workers=len(self.agents)) as executor:
                future_to_agent = {executor.submit(agent.generate_response, history.copy(), round_num): agent for agent in self.agents}
                for future in as_completed(future_to_agent):
                    agent = future_to_agent[future]
                    contribution = future.result()
                    history.append({"speaker": agent.name, "content": contribution})

            # æ™ºèƒ½æ¨¡å¼è¯„ä»·
            if self.mode == "intelligent":
                logging.info(f"--- Leader æ™ºèƒ½è¯„ä»·ç¬¬ {round_num} è½® ---")
                eval_prompt = "è¯·è¯„ä¼°å½“å‰è®¨è®ºè´¨é‡ï¼Œè¾“å‡ºåˆæ³• JSONï¼š{\"quality_score\": æ•´æ•°(1-10), \"decision\": \"continue\"æˆ–\"stop\", \"reason\": \"ç®€çŸ­ç†ç”±\"}"
                eval_history = history + [{"speaker": "System", "content": eval_prompt}]
                leader_eval = self.leader.generate_response(eval_history, round_num)

                try:
                    eval_json = json.loads(leader_eval.strip().strip("```json").strip("```"))
                    if eval_json.get("decision", "").lower() == "stop":
                        logging.info("âœ… Leader åˆ¤æ–­å·²è¾¾æœ€é«˜è´¨é‡ï¼Œåœæ­¢è®¨è®º")
                        break
                except:
                    pass

        # Leader æœ€ç»ˆç»¼åˆ
        logging.info("--- Leader æœ€ç»ˆç»¼åˆ ---")
        history.append({"speaker": "System", "content": "è¯·ç»¼åˆä»¥ä¸Šå…¨éƒ¨è®¨è®ºï¼Œç»™å‡ºæœ€å‡†ç¡®ã€æœ€å®Œæ•´ã€æœ€é«˜è´¨é‡çš„æœ€ç»ˆç­”æ¡ˆã€‚"})
        final_answer = self.leader.generate_response(history, round_num + 1)

        # ä¿å­˜è®°å¿†
        if use_memory:
            summary_prompt = "è¯·ç”¨ 500 å­—ä»¥å†…æ€»ç»“æœ¬æ¬¡ä»»åŠ¡çš„æ ¸å¿ƒç»“è®ºã€å…³é”®å‘ç°å’Œå¯å¤ç”¨ç»éªŒã€‚"
            summary = self.leader.generate_response(history + [{"speaker": "System", "content": summary_prompt}], round_num + 1)
            self._save_memory(memory_key, summary)

        print("\n" + "="*100)
        print("ğŸ¯ ã€æœ€ç»ˆæœ€é«˜è´¨é‡ç­”æ¡ˆã€‘")
        print(final_answer)
        print("="*100)
        return final_answer


if __name__ == "__main__":
    swarm = MultiAgentSwarm()
    swarm.solve("è¯·å¸®æˆ‘å†™ä¸€ç¯‡å…³äºäººå·¥æ™ºèƒ½çš„æ·±åº¦åˆ†ææŠ¥å‘Šï¼Œå¹¶ä¿å­˜åˆ° ./reports/ai_report.md", use_memory=True, memory_key="ai_topic")
