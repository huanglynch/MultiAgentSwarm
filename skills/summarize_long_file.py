"""
è¶…é•¿æ–‡ä»¶æ™ºèƒ½æ€»ç»“å·¥å…·
åŠŸèƒ½ï¼šè§£å†³ token é™åˆ¶ï¼Œè‡ªåŠ¨æ€»ç»“ 10k+ è¡Œä»£ç /æ–‡æ¡£
ä¼˜åŒ–ç‚¹ï¼š
- ä½¿ç”¨å’Œ read_file å®Œå…¨ä¸€è‡´çš„å®‰å…¨æ£€æŸ¥ï¼ˆæ›´ä¸¥æ ¼ï¼‰
- è¿”å›ç»“æ„æ›´æ¸…æ™°ï¼Œé€‚åˆ Agent åç»­é’ˆå¯¹æ€§åˆ†æ
"""
from pathlib import Path
import logging

def tool_function(file_path: str, max_tokens: int = 8000, chunk_size: int = 3000):
    try:
        # è·¯å¾„è§„èŒƒåŒ–
        file_path = file_path.replace('\\', '/')
        if not file_path.startswith('uploads/'):
            file_path = 'uploads/' + file_path.split('uploads/')[-1]

        logging.info(f"ğŸ“ summarize_long_file å°è¯•è¯»å–: {file_path}")

        SCRIPT_ROOT = Path(__file__).parent.parent.parent.absolute()
        target_path = Path(file_path)
        if not target_path.is_absolute():
            target_path = SCRIPT_ROOT / target_path
        target_path = target_path.resolve()

        # ===== ä¸¥æ ¼å®‰å…¨æ£€æŸ¥ï¼ˆä¸ read_file ä¸€è‡´ï¼‰=====
        try:
            relative_path = target_path.relative_to(SCRIPT_ROOT)
        except ValueError:
            return {
                "success": False,
                "error": "å®‰å…¨é”™è¯¯ï¼šä¸å…è®¸è¯»å–è„šæœ¬ç›®å½•å¤–çš„æ–‡ä»¶",
                "attempted_path": str(target_path)
            }

        if not target_path.exists():
            return {"success": False, "error": f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}"}

        with open(target_path, "r", encoding="utf-8", errors="ignore") as f:
            content = f.read()

        original_length = len(content)
        if original_length <= max_tokens * 4:  # ç²—ç•¥ token ä¼°è®¡
            return {
                "success": True,
                "file": str(relative_path),
                "original_length": original_length,
                "summary": content,
                "note": "æ–‡ä»¶è¾ƒçŸ­ï¼Œç›´æ¥è¿”å›å…¨æ–‡"
            }

        # åˆ†å—æ€»ç»“
        chunks = [content[i:i+chunk_size] for i in range(0, len(content), chunk_size)]
        summary_parts = [
            f"ã€Chunk {i+1}/{len(chunks)}ã€‘\n{chunk[:800]}..."
            for i, chunk in enumerate(chunks[:6])  # æœ€å¤šé¢„è§ˆ6å—
        ]

        return {
            "success": True,
            "file": str(relative_path),
            "original_length": original_length,
            "total_chunks": len(chunks),
            "summary_preview": "\n\n".join(summary_parts),
            "note": f"æ–‡ä»¶è¿‡é•¿ï¼ˆ{original_length:,} å­—ç¬¦ï¼‰ï¼Œå·²åˆ†å—é¢„è§ˆå‰ 6 å—ã€‚å»ºè®®åç»­ä½¿ç”¨ read_file + å…·ä½“è¡Œå· æˆ– search_files æ·±å…¥åˆ†æç‰¹å®šéƒ¨åˆ†ã€‚"
        }
    except Exception as e:
        return {"success": False, "error": str(e)}


tool_schema = {
    "type": "function",
    "function": {
        "name": "summarize_long_file",
        "description": "è‡ªåŠ¨æ€»ç»“è¶…é•¿æ–‡ä»¶ï¼ˆä»£ç ã€æ—¥å¿—ã€æŠ¥å‘Šã€PDFè½¬æ–‡æœ¬åç­‰ï¼‰ï¼Œè§£å†³ token é™åˆ¶ã€‚è¿”å›åˆ†å—é¢„è§ˆï¼ŒAgent å¯ç»§ç»­æ·±å…¥ã€‚",
        "parameters": {
            "type": "object",
            "properties": {
                "file_path": {"type": "string", "description": "æ–‡ä»¶è·¯å¾„ï¼ˆç›¸å¯¹äºé¡¹ç›®æ ¹ç›®å½•ï¼‰"},
                "max_tokens": {"type": "integer", "description": "ç›®æ ‡ token ä¸Šé™ï¼ˆç²—ç•¥ä¼°è®¡ï¼‰", "default": 8000},
                "chunk_size": {"type": "integer", "description": "æ¯å—å­—ç¬¦æ•°", "default": 3000}
            },
            "required": ["file_path"]
        }
    }

}
